{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729580198569\n",
      "0.957427107756\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "a1 = [1, 1, 1, 1]\n",
    "a2 = [2, 54, 13, 15]\n",
    "a3 = [2, 2, 4, 3]\n",
    "print 1 - spatial.distance.cosine(a1, a2)\n",
    "print 1 - spatial.distance.cosine(a1, a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "200 OK\n",
      "{u'url': u'https://api.hh.ru/areas/1', u'id': u'1', u'name': u'\\u041c\\u043e\\u0441\\u043a\\u0432\\u0430'}\n",
      "[]\n",
      "[u'1.221', u'1.295', u'1.89']\n",
      "{u'currency': u'RUR', u'amount': 135000}\n",
      "{u'id': u'full', u'name': u'\\u041f\\u043e\\u043b\\u043d\\u0430\\u044f \\u0437\\u0430\\u043d\\u044f\\u0442\\u043e\\u0441\\u0442\\u044c'}\n",
      "[{u'id': u'fullDay', u'name': u'\\u041f\\u043e\\u043b\\u043d\\u044b\\u0439 \\u0434\\u0435\\u043d\\u044c'}, {u'id': u'remote', u'name': u'\\u0423\\u0434\\u0430\\u043b\\u0435\\u043d\\u043d\\u0430\\u044f \\u0440\\u0430\\u0431\\u043e\\u0442\\u0430'}]\n",
      "{u'months': 116}\n",
      "Java Developer\n",
      "Опытный разработчик программного обеспечения. Имею широкий спектр навыков, знаний и технологий. Разрабатываю быстрые программы, работающие с большими объемами данных в многопотоковой среде. Уделяю большое внимание качеству кода и удобству дальнейшей поддержки. Аналитически подхожу к решению задач. Имею хорошие знания алгоритмов и структур данных.\n",
      "\n",
      "Хобби, увлечения\n",
      "Интернет-технологии, путешествия, фотография, кулинария. \n",
      "Организовал и веду в местной школе кружок по программированию для детей.\n",
      "В последнее время интересуюсь машинным обучением и OpenCV\n",
      "\n",
      "Профиль на Habrahabr: http://habrahabr.ru/users/shurik2533/\n",
      "[u'Java', u'Java EE', u'Oracle Pl/SQL', u'Python', u'Data Analysis', u'Big Data', u'recommender systems', u'machine learning']\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import httplib\n",
    "import json\n",
    "headers = {\"Authorization\": \"Bearer PHUAM0L3PU56VNT041CJM3MUTGSABUCDTMBVHEAMF5CGCDEEIC7VFFT4VLOP0GQP\", \n",
    "           \"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"/resumes/mine\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "print r1.status, r1.reason\n",
    "data = r1.read()\n",
    "for i in range(len(json.loads(data)['items'])):\n",
    "    resume_id = json.loads(data)['items'][i]['id']\n",
    "    conn.request(\"GET\", \"/resumes/{0}\".format(resume_id), headers=headers)\n",
    "    r2 = conn.getresponse()\n",
    "    print r2.status, r2.reason\n",
    "    resume_data = r2.read()\n",
    "    features = [1,2,3]\n",
    "    print json.loads(resume_data)['area'] #по этому отсекаем сразу\n",
    "    print json.loads(resume_data)['relocation']['area'] #этот туда же\n",
    "    \n",
    "    \n",
    "    print [d['id'] for d in json.loads(resume_data)['specialization']] #для этого надо взять список всех специализаций. потом взять специализации по всем резюме и на их основе составить веса\n",
    "    print json.loads(resume_data)['salary']\n",
    "    print json.loads(resume_data)['employment']\n",
    "    print json.loads(resume_data)['schedules']\n",
    "    print json.loads(resume_data)['total_experience']\n",
    "    print json.loads(resume_data)['skill_set']\n",
    "    print json.loads(resume_data)['title'] #из этого достаем слова\n",
    "    print json.loads(resume_data)['skills']\n",
    "    \n",
    "    print features\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16823578\n",
      "found\n",
      "16823572\n",
      "found\n",
      "16823571\n",
      "found\n",
      "16823570\n",
      "found\n",
      "16823568\n",
      "found\n",
      "16823564\n",
      "found\n",
      "16823563\n",
      "found\n",
      "16823562\n",
      "found\n",
      "16823561\n",
      "found\n",
      "16823560\n",
      "found\n",
      "16823558\n",
      "found\n",
      "16823557\n",
      "found\n",
      "16823556\n",
      "found\n",
      "16823554\n",
      "found\n",
      "16823551\n",
      "found\n",
      "16823549\n",
      "found\n",
      "16823547\n",
      "found\n",
      "16823544\n",
      "found\n",
      "16823543\n",
      "found\n",
      "16823542\n",
      "found\n",
      "16823540\n",
      "found\n",
      "16823539\n",
      "found\n",
      "16823538\n",
      "found\n",
      "16823536\n",
      "found\n",
      "16823534\n",
      "found\n",
      "16823530\n",
      "found\n",
      "16823519\n",
      "found\n",
      "16823516\n",
      "found\n",
      "16823506\n",
      "found\n",
      "16823503\n",
      "found\n",
      "16823500\n",
      "found\n",
      "16823499\n",
      "found\n",
      "16823498\n",
      "found\n",
      "16823497\n",
      "found\n",
      "16823494\n",
      "found\n",
      "16823492\n",
      "found\n",
      "16823491\n",
      "found\n",
      "16823474\n",
      "found\n",
      "16823469\n",
      "found\n",
      "16823455\n",
      "found\n",
      "16823450\n",
      "found\n",
      "16823444\n",
      "found\n",
      "16823442\n",
      "found\n",
      "16823439\n",
      "found\n",
      "16823416\n",
      "found\n",
      "16823414\n",
      "found\n",
      "16823410\n",
      "found\n",
      "16823408\n",
      "found\n",
      "16823407\n",
      "found\n",
      "16823406\n",
      "found\n",
      "16823404\n",
      "found\n",
      "16823402\n",
      "found\n",
      "16823401\n",
      "found\n",
      "16823392\n",
      "found\n",
      "16823390\n",
      "found\n",
      "16823387\n",
      "found\n",
      "16823384\n",
      "found\n",
      "16823383\n",
      "found\n",
      "16823379\n",
      "found\n",
      "16823378\n",
      "found\n",
      "16823377\n",
      "found\n",
      "16823374\n",
      "found\n",
      "16823370\n",
      "found\n",
      "16823368\n",
      "found\n",
      "16823366\n",
      "found\n",
      "16823364\n",
      "found\n",
      "16823361\n",
      "found\n",
      "16823356\n",
      "found\n",
      "16823355\n",
      "found\n",
      "16823352\n",
      "found\n",
      "16823349\n",
      "found\n",
      "16823346\n",
      "found\n",
      "16823344\n",
      "found\n",
      "16823343\n",
      "found\n",
      "16823342\n",
      "found\n",
      "16823339\n",
      "found\n",
      "16823338\n",
      "found\n",
      "16823337\n",
      "found\n",
      "16823335\n",
      "found\n",
      "16823334\n",
      "found\n",
      "16823333\n",
      "found\n",
      "16823332\n",
      "found\n",
      "16823330\n",
      "found\n",
      "16823329\n",
      "found\n",
      "16823328\n",
      "found\n",
      "16823327\n",
      "found\n",
      "16823325\n",
      "found\n",
      "16823319\n",
      "found\n",
      "16823315\n",
      "found\n",
      "16823311\n",
      "found\n",
      "16823308\n",
      "found\n",
      "16823307\n",
      "found\n",
      "16823306\n",
      "found\n",
      "16823305\n",
      "found\n",
      "16823303\n",
      "found\n",
      "16823300\n",
      "found\n",
      "16823299\n",
      "found\n",
      "16823297\n",
      "found\n",
      "16823294\n",
      "found\n",
      "16823293\n",
      "found\n",
      "16823278\n",
      "found\n",
      "16823276\n",
      "found\n",
      "16823275\n",
      "found\n",
      "16823266\n",
      "found\n",
      "16823264\n",
      "found\n",
      "16823263\n",
      "found\n",
      "16823262\n",
      "found\n",
      "16823261\n",
      "found\n",
      "16823259\n",
      "found\n",
      "16823258\n",
      "found\n",
      "16823257\n",
      "found\n",
      "16823256\n",
      "found\n",
      "16823255\n",
      "found\n",
      "16823254\n",
      "found\n",
      "16823253\n",
      "found\n",
      "16823252\n",
      "found\n",
      "16823250\n",
      "found\n",
      "16823249\n",
      "found\n",
      "16823248\n",
      "found\n",
      "16823244\n",
      "found\n",
      "16823240\n",
      "found\n",
      "16823235\n",
      "found\n",
      "16823233\n",
      "found\n",
      "16823232\n",
      "found\n",
      "16823231\n",
      "found\n",
      "16823229\n",
      "found\n",
      "16823227\n",
      "found\n",
      "16823226\n",
      "found\n",
      "16823225\n",
      "found\n",
      "16823223\n",
      "found\n",
      "16823221\n",
      "found\n",
      "16823218\n",
      "found\n",
      "16823216\n",
      "found\n",
      "16823213\n",
      "found\n",
      "16823212\n",
      "found\n",
      "16823209\n",
      "found\n",
      "16823208\n",
      "found\n",
      "16823206\n",
      "found\n",
      "16823202\n",
      "found\n",
      "16823201\n",
      "found\n",
      "16823199\n",
      "found\n",
      "16823198\n",
      "found\n",
      "16823196\n",
      "found\n",
      "16823194\n",
      "found\n",
      "16823192\n",
      "found\n",
      "16823190\n",
      "found\n",
      "16823188\n",
      "found\n",
      "16823187\n",
      "found\n",
      "16823186\n",
      "found\n",
      "16823181\n",
      "found\n",
      "16823179\n",
      "found\n",
      "16823177\n",
      "found\n",
      "16823175\n",
      "found\n",
      "16823174\n",
      "found\n",
      "16823171\n",
      "found\n",
      "16823166\n",
      "found\n",
      "16823164\n",
      "found\n",
      "16823162\n",
      "found\n",
      "16823161\n",
      "found\n",
      "16823160\n",
      "found\n",
      "16823159\n",
      "found\n",
      "16823158\n",
      "found\n",
      "16823157\n",
      "found\n",
      "16823156\n",
      "found\n",
      "16823155\n",
      "found\n",
      "16823154\n",
      "found\n",
      "16823153\n",
      "found\n",
      "16823152\n",
      "found\n",
      "16823151\n",
      "found\n",
      "16823149\n",
      "found\n",
      "16823148\n",
      "found\n",
      "16823147\n",
      "found\n",
      "16823146\n",
      "found\n",
      "16823145\n",
      "found\n",
      "16823144\n",
      "found\n",
      "16823143\n",
      "found\n",
      "16823142\n",
      "found\n",
      "16823141\n",
      "found\n",
      "16823139\n",
      "found\n",
      "16823138\n",
      "found\n",
      "16823137\n",
      "found\n",
      "16823136\n",
      "found\n",
      "16823134\n",
      "found\n",
      "16823133\n",
      "found\n",
      "16823132\n",
      "found\n",
      "16823131\n",
      "found\n",
      "16823130\n",
      "found\n",
      "16823129\n",
      "found\n",
      "16823128\n",
      "found\n",
      "16823126\n",
      "found\n",
      "16823125\n",
      "found\n",
      "16823124\n",
      "found\n",
      "16823123\n",
      "found\n",
      "16823121\n",
      "found\n",
      "16823119\n",
      "found\n",
      "16823117\n",
      "found\n",
      "16823116\n",
      "found\n",
      "16823115\n",
      "found\n",
      "16823114\n",
      "found\n",
      "16823113\n",
      "found\n",
      "16823112\n",
      "found\n",
      "16823111\n",
      "found\n",
      "16823110\n",
      "found\n",
      "16823109\n",
      "found\n",
      "16823108\n",
      "found\n",
      "16823107\n",
      "found\n",
      "16823106\n",
      "found\n",
      "16823104\n",
      "found\n",
      "16823103\n",
      "found\n",
      "16823101\n",
      "found\n",
      "16823100\n",
      "found\n",
      "16823099\n",
      "found\n",
      "16823098\n",
      "found\n",
      "16823096\n",
      "found\n",
      "16823095\n",
      "found\n",
      "16823094\n",
      "found\n",
      "16823093\n",
      "found\n",
      "16823092\n",
      "found\n",
      "16823091\n",
      "found\n",
      "16823090\n",
      "found\n",
      "16823089\n",
      "found\n",
      "16823088\n",
      "found\n",
      "16823087\n",
      "found\n",
      "16823085\n",
      "found\n",
      "16823084\n",
      "found\n",
      "16823083\n",
      "found\n",
      "16823081\n",
      "found\n",
      "16823080\n",
      "found\n",
      "16823074\n",
      "found\n",
      "16823069\n",
      "found\n",
      "16823067\n",
      "found\n",
      "16823065\n",
      "found\n",
      "16823063\n",
      "found\n",
      "16823062\n",
      "found\n",
      "16823060\n",
      "found\n",
      "16823058\n",
      "found\n",
      "16823057\n",
      "found\n",
      "16823056\n",
      "found\n",
      "16823055\n",
      "found\n",
      "16823052\n",
      "found\n",
      "16823051\n",
      "found\n",
      "16823050\n",
      "found\n",
      "16823048\n",
      "found\n",
      "16823046\n",
      "found\n",
      "16823045\n",
      "found\n",
      "16823044\n",
      "found\n",
      "16823042\n",
      "found\n",
      "16823040\n",
      "found\n",
      "16823039\n",
      "found\n",
      "16823036\n",
      "found\n",
      "16823035\n",
      "found\n",
      "16823034\n",
      "found\n",
      "16823032\n",
      "found\n",
      "16823030\n",
      "found\n",
      "16823028\n",
      "found\n",
      "16823026\n",
      "found\n",
      "16823024\n",
      "found\n",
      "16823023\n",
      "found\n",
      "16823015\n",
      "found\n",
      "16823014\n",
      "found\n",
      "16823009\n",
      "found\n",
      "16823008\n",
      "found\n",
      "16823003\n",
      "found\n",
      "16823000\n"
     ]
    }
   ],
   "source": [
    "import httplib\n",
    "import json\n",
    "import pickle\n",
    "from tinydb import TinyDB\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "db = TinyDB('/home/shurik2533/vacancies_test.json')\n",
    "k = 0\n",
    "for i in range(16823582, 0, -1): #18822744\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"/vacancies/{0}\".format(i), headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "    k = k+1\n",
    "    if r1.status==200:\n",
    "        vacancy_data = r1.read()\n",
    "        #print json.loads(vacancy_data)['archived']\n",
    "        if json.loads(vacancy_data)['archived'] == False:\n",
    "            db.insert(json.loads(vacancy_data))\n",
    "            print json.loads(vacancy_data)['id']\n",
    "            k=0\n",
    "    if i%1000 == 0:\n",
    "        print i\n",
    "        break;\n",
    "    if k == 100:\n",
    "        print \"break\"\n",
    "        break;\n",
    "    \n",
    "#print db.all()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'7.14': u'\\u0410\\u0432\\u0442\\u043e\\u0437\\u0430\\u043f\\u0447\\u0430\\u0441\\u0442\\u0438', u'16.658': u'\\u041c\\u0443\\u043d\\u0438\\u0446\\u0438\\u043f\\u0430\\u043b\\u0438\\u0442\\u0435\\u0442', u'21.292': u'\\u0422\\u0430\\u043c\\u043e\\u0436\\u0435\\u043d\\u043d\\u043e\\u0435 \\u043e\\u0444\\u043e\\u0440\\u043c\\u043b\\u0435\\u043d\\u0438\\u0435', u'23.362': u'\\u042e\\u0440\\u0438\\u0441\\u043a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442', u'6.336': u'\\u0423\\u0447\\u0435\\u0442 \\u043a\\u0430\\u0434\\u0440\\u043e\\u0432', u'23.165': u'\\u041d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'5.61': u'\\u0414\\u0435\\u043d\\u0435\\u0436\\u043d\\u044b\\u0439 \\u0440\\u044b\\u043d\\u043e\\u043a (money market)', u'17.623': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0435 \\u0443\\u0441\\u043b\\u0443\\u0433\\u0438', u'1.10': u'Web \\u043c\\u0430\\u0441\\u0442\\u0435\\u0440', u'3.166': u'\\u041d\\u0430\\u0440\\u0443\\u0436\\u043d\\u0430\\u044f \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'5.367': u'\\u041c\\u0435\\u0442\\u043e\\u0434\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f, \\u0411\\u0430\\u043d\\u043a\\u043e\\u0432\\u0441\\u043a\\u0438\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'5.366': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043f\\u0440\\u043e\\u0447\\u0438\\u0435', u'5.365': u'\\u041e\\u0446\\u0435\\u043d\\u043a\\u0430 \\u0437\\u0430\\u043b\\u043e\\u0433\\u0430, \\u0421\\u0442\\u043e\\u0438\\u043c\\u043e\\u0441\\u0442\\u0438 \\u0438\\u043c\\u0443\\u0449\\u0435\\u0441\\u0442\\u0432\\u0430', u'5.369': u'\\u041e\\u0442\\u0447\\u0435\\u0442\\u043d\\u043e\\u0441\\u0442\\u044c', u'5.368': u'\\u0420\\u0430\\u0437\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430 \\u043d\\u043e\\u0432\\u044b\\u0445 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u043e\\u0432, \\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'13.268': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'22.518': u'\\u0421\\u043e\\u043c\\u0435\\u043b\\u044c\\u0435', u'22.104': u'\\u041a\\u0435\\u0439\\u0442\\u0435\\u0440\\u0438\\u043d\\u0433', u'29.560': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043c\\u043e\\u043d\\u0442\\u0435\\u0440, \\u041a\\u0430\\u0431\\u0435\\u043b\\u044c\\u0449\\u0438\\u043a', u'29.561': u'\\u042e\\u0432\\u0435\\u043b\\u0438\\u0440', u'11.436': u'\\u041a\\u0438\\u043d\\u043e', u'18.57': u'\\u0413\\u043b\\u0430\\u0432\\u043d\\u044b\\u0439 \\u0438\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'18.56': u'\\u0413\\u043b\\u0430\\u0432\\u043d\\u044b\\u0439 \\u0430\\u0433\\u0440\\u043e\\u043d\\u043e\\u043c', u'22.193': u'\\u041e\\u0444\\u0438\\u0446\\u0438\\u0430\\u043d\\u0442, \\u0411\\u0430\\u0440\\u043c\\u0435\\u043d', u'18.174': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'22.198': u'\\u041e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f \\u0432\\u0441\\u0442\\u0440\\u0435\\u0447, \\u041a\\u043e\\u043d\\u0444\\u0435\\u0440\\u0435\\u043d\\u0446\\u0438\\u0439', u'22.199': u'\\u041e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f \\u0442\\u0443\\u0440\\u0438\\u0441\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u043e\\u0432', u'7.455': u'\\u0428\\u0438\\u043d\\u044b, \\u0414\\u0438\\u0441\\u043a\\u0438', u'22.204': u'\\u041f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b \\u043a\\u0443\\u0445\\u043d\\u0438', u'15.308': u'\\u0422\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u0440\\u0442, \\u041b\\u043e\\u0433\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'14.60': u'\\u0413\\u0443\\u043c\\u0430\\u043d\\u0438\\u0442\\u0430\\u0440\\u043d\\u044b\\u0435 \\u043d\\u0430\\u0443\\u043a\\u0438', u'24.493': u'\\u041f\\u0430\\u0440\\u0438\\u043a\\u043c\\u0430\\u0445\\u0435\\u0440', u'24.492': u'\\u041c\\u0430\\u0441\\u0441\\u0430\\u0436\\u0438\\u0441\\u0442', u'9.22': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'19.421': u'\\u0410\\u043a\\u0442\\u0443\\u0430\\u0440\\u0438\\u0439', u'3.236': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u044b', u'3.230': u'\\u041f\\u0440\\u043e\\u0434\\u0432\\u0438\\u0436\\u0435\\u043d\\u0438\\u0435, \\u0421\\u043f\\u0435\\u0446\\u0438\\u0430\\u043b\\u044c\\u043d\\u044b\\u0435 \\u043c\\u0435\\u0440\\u043e\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u044f', u'2.523': u'CIPA', u'12.326': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'12.322': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u0430\\u043a\\u0442\\u0438\\u043a\\u043e\\u0439', u'16.435': u'\\u041d\\u0418\\u0418', u'11.62': u'\\u0414\\u0438\\u0437\\u0430\\u0439\\u043d, \\u0433\\u0440\\u0430\\u0444\\u0438\\u043a\\u0430, \\u0436\\u0438\\u0432\\u043e\\u043f\\u0438\\u0441\\u044c', u'17.269': u'\\u0422\\u0435\\u043b\\u0435\\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438, \\u0421\\u0435\\u0442\\u0435\\u0432\\u044b\\u0435 \\u0440\\u0435\\u0448\\u0435\\u043d\\u0438\\u044f', u'1.232': u'\\u041f\\u0440\\u043e\\u0434\\u044e\\u0441\\u0435\\u0440', u'5.121': u'\\u041a\\u043e\\u0440\\u043f\\u043e\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u043e\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'5.123': u'\\u041a\\u043e\\u0440\\u0440\\u0435\\u0441\\u043f\\u043e\\u043d\\u0434\\u0435\\u043d\\u0442\\u0441\\u043a\\u0438\\u0435, \\u041c\\u0435\\u0436\\u0434\\u0443\\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u044b\\u0435 \\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u044f', u'5.124': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043d\\u044b\\u0435', u'5.126': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u044b', u'2.454': u'\\u041c\\u0421\\u0424\\u041e, IFRS', u'4.181': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'18.263': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u043f\\u0440\\u0435\\u0434\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u0435\\u043c', u'2.351': u'\\u0426\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0431\\u0443\\u043c\\u0430\\u0433\\u0438', u'24.380': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'23.36': u'\\u0411\\u0430\\u043d\\u043a\\u043e\\u0432\\u0441\\u043a\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'3.305': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u044b\\u0439 \\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433(Trade marketing)', u'26.416': u'FMCG, \\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0433\\u043e \\u043f\\u043e\\u0442\\u0440\\u0435\\u0431\\u043b\\u0435\\u043d\\u0438\\u044f', u'26.415': u'\\u0425\\u0438\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0446\\u0438\\u044f', u'26.414': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u0438\\u043a\\u0430, \\u0444\\u043e\\u0442\\u043e, \\u0432\\u0438\\u0434\\u0435\\u043e', u'26.413': u'\\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u0434\\u043b\\u044f \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'26.412': u'\\u041f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u044b \\u043f\\u0438\\u0442\\u0430\\u043d\\u0438\\u044f', u'26.411': u'\\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'26.410': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u0430\\u044f \\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'9.312': u'\\u0422\\u0443\\u0440\\u0438\\u0437\\u043c, \\u0413\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u044b, \\u0420\\u0435\\u0441\\u0442\\u043e\\u0440\\u0430\\u043d\\u044b', u'9.317': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043c\\u0430\\u043b\\u044b\\u043c \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u043e\\u043c', u'26.419': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'5.234': u'\\u041f\\u0440\\u043e\\u0435\\u043a\\u0442\\u043d\\u043e\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.420': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u043e\\u0440 \\u0431\\u0430\\u0437 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445', u'3.114': u'\\u041a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0430\\u043d\\u0442', u'23.539': u'\\u0410\\u043d\\u0442\\u0438\\u043c\\u043e\\u043d\\u043e\\u043f\\u043e\\u043b\\u044c\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'18.190': u'\\u041d\\u0435\\u0444\\u0442\\u0435\\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'14.37': u'\\u0411\\u0438\\u043e\\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'15.389': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'15.388': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b', u'23.120': u'\\u041a\\u043e\\u0440\\u043f\\u043e\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'12.331': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u043a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'11.134': u'\\u041b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u043d\\u0430\\u044f, \\u0420\\u0435\\u0434\\u0430\\u043a\\u0442\\u043e\\u0440\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'5.241': u'\\u041f\\u0440\\u044f\\u043c\\u044b\\u0435 \\u0438\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'5.534': u'\\u0424\\u0430\\u043a\\u0442\\u043e\\u0440\\u0438\\u043d\\u0433', u'18.299': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0438 \\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430 \\u0437\\u0435\\u0440\\u043d\\u043e\\u0432\\u044b\\u0445', u'18.290': u'\\u0421\\u0442\\u0440\\u043e\\u0439\\u043c\\u0430\\u0442\\u0435\\u0440\\u0438\\u0430\\u043b\\u044b', u'14.178': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'18.297': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433', u'18.13': u'\\u0410\\u0432\\u0438\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'13.438': u'\\u041e\\u043f\\u0442\\u0438\\u043a\\u0430', u'18.16': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u044c\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'13.433': u'\\u0420\\u0435\\u0433\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u0443\\u0440\\u0430', u'13.432': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'6.247': u'\\u0420\\u0430\\u0437\\u0432\\u0438\\u0442\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u0430', u'17.441': u'\\u0424\\u0440\\u0430\\u043d\\u0447\\u0430\\u0439\\u0437\\u0438\\u043d\\u0433', u'17.443': u'\\u041c\\u0435\\u0431\\u0435\\u043b\\u044c', u'17.446': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u0438', u'1.161': u'\\u041c\\u0443\\u043b\\u044c\\u0442\\u0438\\u043c\\u0435\\u0434\\u0438\\u0430', u'18.129': u'\\u041b\\u0435\\u0433\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'21.310': u'\\u0422\\u0440\\u0443\\u0431\\u043e\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u044b', u'22.329': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0440\\u0435\\u0441\\u0442\\u043e\\u0440\\u0430\\u043d\\u0430\\u043c\\u0438, \\u0411\\u0430\\u0440\\u0430\\u043c\\u0438', u'15.93': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438, \\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442, \\u041c\\u0443\\u043b\\u044c\\u0442\\u0438\\u043c\\u0435\\u0434\\u0438\\u0430', u'22.175': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'2.179': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.88': u'\\u0418\\u043d\\u0442\\u0435\\u043b\\u043b\\u0435\\u043a\\u0442\\u0443\\u0430\\u043b\\u044c\\u043d\\u0430\\u044f \\u0441\\u043e\\u0431\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'20.396': u'\\u042d\\u043a\\u0441\\u043f\\u043b\\u0443\\u0430\\u0442\\u0430\\u0446\\u0438\\u044f', u'13.489': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u0438\\u0439 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c', u'6.107': u'\\u041a\\u043e\\u043c\\u043f\\u0435\\u043d\\u0441\\u0430\\u0446\\u0438\\u0438 \\u0438 \\u043b\\u044c\\u0433\\u043e\\u0442\\u044b', u'9.532': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0437\\u0430\\u043a\\u0443\\u043f\\u043a\\u0430\\u043c\\u0438', u'5.4': u'Forex', u'18.81': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'18.86': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0441\\u0430\\u0445\\u0430\\u0440\\u0430', u'7.566': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0439\\u0449\\u0438\\u043a', u'7.565': u'\\u0410\\u0432\\u0442\\u043e\\u0436\\u0435\\u0441\\u0442\\u044f\\u043d\\u0449\\u0438\\u043a', u'18.85': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0438 \\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430 \\u0437\\u0435\\u0440\\u043d\\u043e\\u0432\\u044b\\u0445', u'8.575': u'\\u041f\\u043e\\u0436\\u0430\\u0440\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'27.533': u'\\u041f\\u043e\\u043c\\u043e\\u0449\\u043d\\u0438\\u043a \\u043f\\u043e \\u0445\\u043e\\u0437\\u044f\\u0439\\u0441\\u0442\\u0432\\u0443, \\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u044f\\u044e\\u0449\\u0438\\u0439', u'10.472': u'\\u0413\\u0430\\u0437', u'3.40': u'\\u0411\\u0440\\u0435\\u043d\\u0434-\\u043c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442', u'26.427': u'\\u041c\\u0435\\u0442\\u0430\\u043b\\u043b\\u043e\\u043f\\u0440\\u043e\\u043a\\u0430\\u0442', u'5.399': u'\\u0418\\u043f\\u043e\\u0442\\u0435\\u043a\\u0430, \\u0418\\u043f\\u043e\\u0442\\u0435\\u0447\\u043d\\u043e\\u0435 \\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'3.48': u'\\u0412\\u0435\\u0440\\u0441\\u0442\\u0430\\u043b\\u044c\\u0449\\u0438\\u043a', u'12.376': u'\\u041d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u044c', u'18.339': u'\\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'15.167': u'\\u041d\\u0430\\u0443\\u043a\\u0430, \\u041e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'13.155': u'\\u041c\\u043b\\u0430\\u0434\\u0448\\u0438\\u0439 \\u0438 \\u0441\\u0440\\u0435\\u0434\\u043d\\u0438\\u0439 \\u043c\\u0435\\u0434\\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b', u'1.89': u'\\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442', u'17.59': u'\\u0413\\u0421\\u041c, \\u043d\\u0435\\u0444\\u0442\\u044c, \\u0431\\u0435\\u043d\\u0437\\u0438\\u043d', u'12.97': u'\\u0418\\u0441\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u0440\\u044b\\u043d\\u043a\\u0430', u'5.192': u'\\u041e\\u0431\\u043c\\u0435\\u043d\\u043d\\u044b\\u0435 \\u043f\\u0443\\u043d\\u043a\\u0442\\u044b, \\u0411\\u0430\\u043d\\u043a\\u043e\\u043c\\u0430\\u0442\\u044b', u'23.72': u'\\u0417\\u0430\\u043a\\u043e\\u043d\\u043e\\u0442\\u0432\\u043e\\u0440\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e', u'5.195': u'\\u041e\\u041f\\u0415\\u0420\\u0423', u'19.18': u'\\u0410\\u0432\\u0442\\u043e\\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'5.691': u'Private Banking', u'17.538': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438 \\u043f\\u043e \\u0442\\u0435\\u043b\\u0435\\u0444\\u043e\\u043d\\u0443, \\u0422\\u0435\\u043b\\u0435\\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'5.177': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.476': u'\\u0417\\u0435\\u043c\\u0435\\u043b\\u044c\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'23.477': u'\\u0414\\u043e\\u0433\\u043e\\u0432\\u043e\\u0440\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'23.478': u'\\u0420\\u0435\\u0433\\u0438\\u0441\\u0442\\u0440\\u0430\\u0446\\u0438\\u044f \\u044e\\u0440\\u0438\\u0434\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043b\\u0438\\u0446', u'23.479': u'\\u0412\\u0437\\u044b\\u0441\\u043a\\u0430\\u043d\\u0438\\u0435 \\u0437\\u0430\\u0434\\u043e\\u043b\\u0436\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438, \\u041a\\u043e\\u043b\\u043b\\u0435\\u043a\\u0442\\u043e\\u0440\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'14.141': u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u043a\\u0430', u'17.535': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u044b\\u0435 \\u0441\\u0435\\u0442\\u0438', u'11.173': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'24.377': u'\\u041a\\u043e\\u0441\\u043c\\u0435\\u0442\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f', u'12.7': u'PR Consulting', u'9.105': u'\\u041a\\u043e\\u043c\\u043c\\u0435\\u0440\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0411\\u0430\\u043d\\u043a', u'12.5': u'Internet, E-Commerce', u'4.32': u'\\u0410\\u0440\\u0445\\u0438\\u0432\\u0438\\u0441\\u0442', u'24.378': u'\\u0422\\u0440\\u0435\\u043d\\u0435\\u0440\\u0441\\u043a\\u0438\\u0439 \\u0441\\u043e\\u0441\\u0442\\u0430\\u0432', u'24.379': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0446\\u0438\\u044f', u'20.58': u'\\u0413\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u044b, \\u041c\\u0430\\u0433\\u0430\\u0437\\u0438\\u043d\\u044b', u'21.506': u'\\u042d\\u043a\\u0441\\u043f\\u0435\\u0434\\u0438\\u0442\\u043e\\u0440', u'21.482': u'\\u0412\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c', u'22.55': u'\\u0413\\u0438\\u0434, \\u042d\\u043a\\u0441\\u043a\\u0443\\u0440\\u0441\\u043e\\u0432\\u043e\\u0434', u'21.481': u'\\u0414\\u0438\\u0441\\u043f\\u0435\\u0442\\u0447\\u0435\\u0440', u'19.284': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u0438', u'9.448': u'\\u042e\\u0440\\u0438\\u0441\\u043f\\u0440\\u0443\\u0434\\u0435\\u043d\\u0446\\u0438\\u044f', u'5.579': u'\\u0420\\u0430\\u0431\\u043e\\u0442\\u0430 \\u0441 \\u043f\\u0440\\u043e\\u0431\\u043b\\u0435\\u043c\\u043d\\u044b\\u043c\\u0438 \\u0437\\u0430\\u0435\\u043c\\u0449\\u0438\\u043a\\u0430\\u043c\\u0438', u'20.233': u'\\u041f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0410\\u0440\\u0445\\u0438\\u0442\\u0435\\u043a\\u0442\\u0443\\u0440\\u0430', u'5.133': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043b\\u0438\\u0437\\u0438\\u043d\\u0433\\u043e\\u0432\\u044b\\u0435', u'10.315': u'\\u0423\\u0433\\u043e\\u043b\\u044c', u'10.258': u'\\u0420\\u0443\\u0434\\u0430', u'4.264': u'\\u0421\\u0435\\u043a\\u0440\\u0435\\u0442\\u0430\\u0440\\u044c', u'13.578': u'\\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442', u'5.304': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u043e\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'9.67': u'\\u0414\\u043e\\u0431\\u044b\\u0447\\u0430 c\\u044b\\u0440\\u044c\\u044f', u'17.401': u'\\u0425\\u0438\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0446\\u0438\\u044f', u'5.51': u'\\u0412\\u043d\\u0443\\u0442\\u0440\\u0435\\u043d\\u043d\\u0438\\u0435 \\u043e\\u043f\\u0435\\u0440\\u0430\\u0446\\u0438\\u0438 (Back Office)', u'22.353': u'\\u0428\\u0435\\u0444-\\u043f\\u043e\\u0432\\u0430\\u0440', u'3.171': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c/\\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'12.180': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.276': u'\\u0421\\u043b\\u0438\\u044f\\u043d\\u0438\\u044f \\u0438 \\u043f\\u043e\\u0433\\u043b\\u043e\\u0449\\u0435\\u043d\\u0438\\u044f', u'22.491': u'\\u041f\\u043e\\u0432\\u0430\\u0440', u'18.212': u'\\u041f\\u043e\\u043b\\u0438\\u0433\\u0440\\u0430\\u0444\\u0438\\u044f', u'19.259': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c \\u043d\\u0430\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u044f', u'21.564': u'\\u0420\\u0430\\u0431\\u043e\\u0447\\u0438\\u0439 \\u0441\\u043a\\u043b\\u0430\\u0434\\u0430', u'1.395': u'\\u0411\\u0430\\u043d\\u043a\\u043e\\u0432\\u0441\\u043a\\u043e\\u0435 \\u041f\\u041e', u'5.370': u'\\u041a\\u0430\\u0437\\u043d\\u0430\\u0447\\u0435\\u0439\\u0441\\u0442\\u0432\\u043e, \\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043b\\u0438\\u043a\\u0432\\u0438\\u0434\\u043d\\u043e\\u0441\\u0442\\u044c\\u044e', u'23.286': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'5.372': u'\\u0411\\u044e\\u0434\\u0436\\u0435\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'29.495': u'\\u0413\\u0440\\u0443\\u0437\\u0447\\u0438\\u043a', u'16.216': u'\\u041f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e', u'22.505': u'\\u0428\\u0432\\u0435\\u0439\\u0446\\u0430\\u0440', u'21.136': u'\\u041b\\u043e\\u0433\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'1.137': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'9.145': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0430, \\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'8.77': u'\\u0418\\u043c\\u0443\\u0449\\u0435\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'29.513': u'\\u0421\\u0442\\u043e\\u043b\\u044f\\u0440', u'29.512': u'\\u0424\\u0430\\u0441\\u043e\\u0432\\u0449\\u0438\\u043a', u'2.33': u'\\u0410\\u0443\\u0434\\u0438\\u0442', u'29.515': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u0438\\u043a', u'8.202': u'\\u041e\\u0445\\u0440\\u0430\\u043d\\u043d\\u0438\\u043a', u'16.38': u'\\u0411\\u043b\\u0430\\u0433\\u043e\\u0442\\u0432\\u043e\\u0440\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'13.587': u'\\u0412\\u0440\\u0430\\u0447-\\u044d\\u043a\\u0441\\u043f\\u0435\\u0440\\u0442', u'29.516': u'\\u0428\\u0432\\u0435\\u044f', u'15.363': u'\\u042e\\u0440\\u0438\\u0441\\u0442\\u044b', u'21.74': u'\\u0417\\u0430\\u043a\\u0443\\u043f\\u043a\\u0438, \\u0421\\u043d\\u0430\\u0431\\u0436\\u0435\\u043d\\u0438\\u0435', u'19.430': u'\\u0423\\u0440\\u0435\\u0433\\u0443\\u043b\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0443\\u0431\\u044b\\u0442\\u043a\\u043e\\u0432', u'24.624': u'\\u041d\\u043e\\u0433\\u0442\\u0435\\u0432\\u043e\\u0439 \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441', u'2.125': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043d\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'3.90': u'\\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442-\\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'7.239': u'\\u041f\\u0440\\u043e\\u043a\\u0430\\u0442, \\u043b\\u0438\\u0437\\u0438\\u043d\\u0433', u'9.321': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u043e\\u043c, \\u0422\\u0440\\u0435\\u043d\\u0438\\u043d\\u0433\\u0438', u'3.98': u'\\u0418\\u0441\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u0440\\u044b\\u043d\\u043a\\u0430', u'14.349': u'\\u0425\\u0438\\u043c\\u0438\\u044f', u'7.235': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'11.71': u'\\u0416\\u0443\\u0440\\u043d\\u0430\\u043b\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'11.76': u'\\u0418\\u0437\\u0434\\u0430\\u0442\\u0435\\u043b\\u044c\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'19.19': u'\\u0410\\u0433\\u0435\\u043d\\u0442', u'14.340': u'\\u0424\\u0438\\u0437\\u0438\\u043a\\u0430', u'12.197': u'\\u041e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u043e\\u0435 \\u043a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.221': u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0420\\u0430\\u0437\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'1.225': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'5.132': u'\\u041b\\u0438\\u0437\\u0438\\u043d\\u0433', u'20.573': u'\\u0422\\u0435\\u043d\\u0434\\u0435\\u0440\\u044b', u'15.730': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440\\u0438\\u044f', u'19.282': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'19.283': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0436\\u0438\\u0437\\u043d\\u0438', u'2.463': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440-\\u043a\\u0430\\u043b\\u044c\\u043a\\u0443\\u043b\\u044f\\u0442\\u043e\\u0440', u'22.11': u'\\u0410\\u0432\\u0438\\u0430\\u0431\\u0438\\u043b\\u0435\\u0442\\u044b', u'2.467': u'ACCA', u'2.465': u'GAAP', u'2.464': u'\\u041e\\u0441\\u043d\\u043e\\u0432\\u043d\\u044b\\u0435 \\u0441\\u0440\\u0435\\u0434\\u0441\\u0442\\u0432\\u0430', u'2.342': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u0430\\u043d\\u0430\\u043b\\u0438\\u0437', u'2.343': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'2.469': u'\\u041f\\u0435\\u0440\\u0432\\u0438\\u0447\\u043d\\u0430\\u044f \\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u0430\\u0446\\u0438\\u044f', u'2.468': u'\\u0422\\u041c\\u0426', u'2.344': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u043c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442', u'11.99': u'\\u041a\\u0430\\u0437\\u0438\\u043d\\u043e \\u0438 \\u0438\\u0433\\u043e\\u0440\\u043d\\u044b\\u0439 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441', u'3.318': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433\\u043e\\u043c', u'3.253': u'\\u0420\\u0435\\u043a\\u043b\\u0430\\u043c\\u043d\\u044b\\u0439 \\u0430\\u0433\\u0435\\u043d\\u0442', u'23.21': u'\\u0410\\u0434\\u0432\\u043e\\u043a\\u0430\\u0442', u'23.29': u'\\u0410\\u0440\\u0431\\u0438\\u0442\\u0440\\u0430\\u0436', u'20.83': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'20.375': u'\\u0414\\u0435\\u0432\\u0435\\u043b\\u043e\\u043f\\u0435\\u0440', u'17.303': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u043b\\u044f \\u0431\\u0438\\u0440\\u0436\\u0435\\u0432\\u044b\\u043c\\u0438 \\u0442\\u043e\\u0432\\u0430\\u0440\\u0430\\u043c\\u0438', u'17.302': u'FMCG, \\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0433\\u043e \\u043f\\u043e\\u0442\\u0440\\u0435\\u0431\\u043b\\u0435\\u043d\\u0438\\u044f', u'17.301': u'\\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u0434\\u043b\\u044f \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'17.306': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u044b\\u0439 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c', u'5.224': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0445 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u043e\\u0432', u'5.341': u'\\u0424\\u0438\\u043b\\u0438\\u0430\\u043b\\u044b', u'14.87': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440\\u043d\\u044b\\u0435 \\u043d\\u0430\\u0443\\u043a\\u0438', u'17.111': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u0430\\u044f \\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'1.296': u'\\u0422\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u043f\\u0438\\u0441\\u0430\\u0442\\u0435\\u043b\\u044c', u'1.295': u'\\u0422\\u0435\\u043b\\u0435\\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438', u'4.47': u'\\u0412\\u0432\\u043e\\u0434 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445', u'1.359': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u043d\\u0430\\u044f \\u043a\\u043e\\u043c\\u043c\\u0435\\u0440\\u0446\\u0438\\u044f', u'15.146': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0430, \\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'4.127': u'\\u041a\\u0443\\u0440\\u044c\\u0435\\u0440', u'18.451': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'18.453': u'\\u042e\\u0432\\u0435\\u043b\\u0438\\u0440\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'10.323': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u0435\\u0434\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u0435\\u043c', u'17.572': u'\\u0422\\u0435\\u043d\\u0434\\u0435\\u0440\\u044b', u'1.3': u'CTO, CIO, \\u0414\\u0438\\u0440\\u0435\\u043a\\u0442\\u043e\\u0440 \\u043f\\u043e IT', u'1.9': u'Web \\u0438\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'12.92': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'6.254': u'\\u0420\\u0435\\u043a\\u0440\\u0443\\u0442\\u043c\\u0435\\u043d\\u0442', u'1.172': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'6.319': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u043e\\u043c', u'15.320': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u043e\\u043c', u'3.148': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0440\\u0430\\u0431\\u043e\\u0442\\u0435 \\u0441 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442\\u0430\\u043c\\u0438', u'19.109': u'\\u041a\\u043e\\u043c\\u043f\\u043b\\u0435\\u043a\\u0441\\u043d\\u043e\\u0435 \\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u044e\\u0440\\u0438\\u0434\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043b\\u0438\\u0446', u'19.108': u'\\u041a\\u043e\\u043c\\u043f\\u043b\\u0435\\u043a\\u0441\\u043d\\u043e\\u0435 \\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0444\\u0438\\u0437\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043b\\u0438\\u0446', u'15.281': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'17.112': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u044b\\u0435 \\u043f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u044b', u'1.30': u'\\u0410\\u0440\\u0442-\\u0434\\u0438\\u0440\\u0435\\u043a\\u0442\\u043e\\u0440', u'15.731': u'\\u0417\\u0430\\u043a\\u0443\\u043f\\u043a\\u0438', u'22.316': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0433\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u0430\\u043c\\u0438', u'13.537': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u0438\\u0439 \\u0441\\u043e\\u0432\\u0435\\u0442\\u043d\\u0438\\u043a', u'2.164': u'\\u041d\\u0430\\u043b\\u043e\\u0433\\u0438', u'29.546': u'\\u041a\\u0440\\u0430\\u0441\\u043d\\u043e\\u0434\\u0435\\u0440\\u0435\\u0432\\u0449\\u0438\\u043a', u'29.547': u'\\u041a\\u0443\\u0437\\u043d\\u0435\\u0446', u'2.249': u'\\u0420\\u0430\\u0441\\u0447\\u0435\\u0442 \\u0441\\u0435\\u0431\\u0435\\u0441\\u0442\\u043e\\u0438\\u043c\\u043e\\u0441\\u0442\\u0438', u'29.545': u'\\u041a\\u043e\\u043c\\u043f\\u043b\\u0435\\u043a\\u0442\\u043e\\u0432\\u0449\\u0438\\u043a, \\u0423\\u043a\\u043b\\u0430\\u0434\\u0447\\u0438\\u043a-\\u0443\\u043f\\u0430\\u043a\\u043e\\u0432\\u0449\\u0438\\u043a', u'29.542': u'\\u0414\\u043e\\u0440\\u043e\\u0436\\u043d\\u044b\\u0435 \\u0440\\u0430\\u0431\\u043e\\u0447\\u0438\\u0435', u'29.543': u'\\u0416\\u0435\\u0441\\u0442\\u044f\\u043d\\u0449\\u0438\\u043a', u'29.540': u'\\u0413\\u0430\\u0440\\u0434\\u0435\\u0440\\u043e\\u0431\\u0449\\u0438\\u043a', u'29.541': u'\\u0414\\u0432\\u043e\\u0440\\u043d\\u0438\\u043a, \\u0423\\u0431\\u043e\\u0440\\u0449\\u0438\\u043a', u'29.548': u'\\u041b\\u0438\\u0444\\u0442\\u0435\\u0440', u'29.549': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u0438\\u0441\\u0442 \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u0430', u'27.500': u'\\u041f\\u043e\\u0432\\u0430\\u0440', u'27.501': u'\\u0421\\u0438\\u0434\\u0435\\u043b\\u043a\\u0430', u'7.503': u'\\u0410\\u0432\\u0442\\u043e\\u0441\\u043b\\u0435\\u0441\\u0430\\u0440\\u044c', u'19.285': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438', u'10.80': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'18.142': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u043e\\u0441\\u0442\\u0440\\u043e\\u0435\\u043d\\u0438\\u0435', u'21.176': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'2.425': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0441\\u0442', u'26.439': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u043c\\u0430\\u0442\\u0435\\u0440\\u0438\\u0430\\u043b\\u044b', u'26.437': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0437\\u0430\\u043a\\u0443\\u043f\\u043a\\u0430\\u043c\\u0438', u'10.393': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'13.398': u'\\u041a\\u043b\\u0438\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435 \\u0438\\u0441\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f', u'3.213': u'\\u041f\\u043e\\u043b\\u0438\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 PR', u'20.525': u'\\u0417\\u0435\\u043c\\u043b\\u0435\\u0443\\u0441\\u0442\\u0440\\u043e\\u0439\\u0441\\u0442\\u0432\\u043e', u'20.527': u'\\u041e\\u0442\\u043e\\u043f\\u043b\\u0435\\u043d\\u0438\\u0435, \\u0432\\u0435\\u043d\\u0442\\u0438\\u043b\\u044f\\u0446\\u0438\\u044f \\u0438 \\u043a\\u043e\\u043d\\u0434\\u0438\\u0446\\u0438\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'20.526': u'\\u041e\\u0446\\u0435\\u043d\\u043a\\u0430', u'5.262': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0431\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440\\u0438\\u0435\\u0439', u'17.242': u'\\u041f\\u0440\\u044f\\u043c\\u044b\\u0435 \\u043f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'20.528': u'\\u0412\\u043e\\u0434\\u043e\\u0441\\u043d\\u0430\\u0431\\u0436\\u0435\\u043d\\u0438\\u0435 \\u0438 \\u043a\\u0430\\u043d\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f', u'20.445': u'\\u0416\\u041a\\u0425', u'20.63': u'\\u0414\\u0438\\u0437\\u0430\\u0439\\u043d/\\u041e\\u0444\\u043e\\u0440\\u043c\\u043b\\u0435\\u043d\\u0438\\u0435', u'11.160': u'\\u041c\\u0443\\u0437\\u044b\\u043a\\u0430', u'18.360': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u044d\\u043d\\u0435\\u0440\\u0433\\u0435\\u0442\\u0438\\u043a\\u0430', u'18.361': u'\\u042d\\u043d\\u0435\\u0440\\u0433\\u0435\\u0442\\u0438\\u043a \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u0430', u'2.337': u'\\u0423\\u0447\\u0435\\u0442 \\u0441\\u0447\\u0435\\u0442\\u043e\\u0432 \\u0438 \\u043f\\u043b\\u0430\\u0442\\u0435\\u0436\\u0435\\u0439', u'2.335': u'\\u0423\\u0447\\u0435\\u0442 \\u0437\\u0430\\u0440\\u0430\\u0431\\u043e\\u0442\\u043d\\u043e\\u0439 \\u043f\\u043b\\u0430\\u0442\\u044b', u'1.82': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'4.271': u'\\u0421\\u0438\\u043d\\u0445\\u0440\\u043e\\u043d\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0434', u'13.49': u'\\u0412\\u0435\\u0442\\u0435\\u0440\\u0438\\u043d\\u0430\\u0440\\u0438\\u044f', u'9.289': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e, \\u041d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u044c', u'11.240': u'\\u041f\\u0440\\u043e\\u0447\\u0435\\u0435', u'11.243': u'\\u0420\\u0430\\u0434\\u0438\\u043e', u'4.278': u'\\u0421\\u043e\\u0442\\u0440\\u0443\\u0434\\u043d\\u0438\\u043a call-\\u0446\\u0435\\u043d\\u0442\\u0440\\u0430', u'3.328': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'17.333': u'\\u0423\\u0441\\u043b\\u0443\\u0433\\u0438 \\u0434\\u043b\\u044f \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'29.556': u'\\u0420\\u0430\\u0437\\u043d\\u043e\\u0440\\u0430\\u0431\\u043e\\u0447\\u0438\\u0439', u'17.334': u'\\u0423\\u0441\\u043b\\u0443\\u0433\\u0438 \\u0434\\u043b\\u044f \\u043d\\u0430\\u0441\\u0435\\u043b\\u0435\\u043d\\u0438\\u044f', u'4.374': u'\\u0410\\u0425\\u041e', u'17.231': u'\\u041f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u044b \\u043f\\u0438\\u0442\\u0430\\u043d\\u0438\\u044f', u'5.460': u'\\u0410\\u0432\\u0442\\u043e\\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'12.6': u'Knowledge management', u'5.41': u'\\u0411\\u0443\\u043c\\u0430\\u0433\\u0438 \\u0441 \\u0444\\u0438\\u043a\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043d\\u043e\\u0439 \\u0434\\u043e\\u0445\\u043e\\u0434\\u043d\\u043e\\u0441\\u0442\\u044c\\u044e (fixed Income)', u'17.144': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0430, \\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'5.42': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440', u'5.45': u'\\u0412\\u0430\\u043b\\u044e\\u0442\\u043d\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'4.456': u'\\u0412\\u0435\\u0447\\u0435\\u0440\\u043d\\u0438\\u0439 \\u0441\\u0435\\u043a\\u0440\\u0435\\u0442\\u0430\\u0440\\u044c', u'17.418': u'\\u0421\\u0435\\u043b\\u044c\\u0441\\u043a\\u043e\\u0435 \\u0445\\u043e\\u0437\\u044f\\u0439\\u0441\\u0442\\u0432\\u043e', u'17.149': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0440\\u0430\\u0431\\u043e\\u0442\\u0435 \\u0441 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442\\u0430\\u043c\\u0438', u'29.162': u'\\u041d\\u0430\\u043b\\u0430\\u0434\\u0447\\u0438\\u043a', u'19.147': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u043e\\u0435 \\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.400': u'\\u041e\\u043f\\u0442\\u0438\\u043c\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f \\u0441\\u0430\\u0439\\u0442\\u0430 (SEO)', u'23.266': u'\\u0421\\u0435\\u043c\\u0435\\u0439\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'17.417': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u043c\\u0430\\u0442\\u0435\\u0440\\u0438\\u0430\\u043b\\u044b', u'18.404': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'18.568': u'\\u041a\\u043e\\u043d\\u0441\\u0442\\u0440\\u0443\\u043a\\u0442\\u043e\\u0440', u'25.381': u'\\u0421\\u0435\\u0440\\u0432\\u0438\\u0441\\u043d\\u044b\\u0439 \\u0438\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'25.382': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u043d\\u043e\\u0433\\u043e \\u0446\\u0435\\u043d\\u0442\\u0440\\u0430', u'25.383': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u0443 - \\u0441\\u0435\\u0442\\u0435\\u0432\\u044b\\u0435 \\u0438 \\u0442\\u0435\\u043b\\u0435\\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'25.384': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u0443 - \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'25.385': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u0443 - \\u0442\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u0440\\u0442', u'25.386': u'\\u0418\\u043d\\u0441\\u0442\\u0430\\u043b\\u043b\\u044f\\u0446\\u0438\\u044f \\u0438 \\u043d\\u0430\\u0441\\u0442\\u0440\\u043e\\u0439\\u043a\\u0430 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f', u'17.520': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0432\\u0435\\u0446 \\u0432 \\u043c\\u0430\\u0433\\u0430\\u0437\\u0438\\u043d\\u0435', u'17.486': u'\\u0421\\u0430\\u043d\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'17.487': u'\\u0411\\u044b\\u0442\\u043e\\u0432\\u0430\\u044f \\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'18.118': u'\\u041a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c \\u043a\\u0430\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430', u'18.354': u'\\u042d\\u043a\\u043e\\u043b\\u043e\\u0433', u'18.585': u'\\u0410\\u0442\\u043e\\u043c\\u043d\\u0430\\u044f \\u044d\\u043d\\u0435\\u0440\\u0433\\u0435\\u0442\\u0438\\u043a\\u0430', u'22.530': u'\\u041e\\u0444\\u043e\\u0440\\u043c\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0432\\u0438\\u0437', u'22.531': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0442\\u0443\\u0440\\u0438\\u0441\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u043c \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u043e\\u043c', u'2.200': u'\\u041e\\u0444\\u0444\\u0448\\u043e\\u0440\\u044b', u'5.34': u'\\u0410\\u0443\\u0434\\u0438\\u0442, \\u0412\\u043d\\u0443\\u0442\\u0440\\u0435\\u043d\\u043d\\u0438\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'23.314': u'\\u0423\\u0433\\u043e\\u043b\\u043e\\u0432\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'23.311': u'\\u0422\\u0440\\u0443\\u0434\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'17.15': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u0438, \\u0417\\u0430\\u043f\\u0447\\u0430\\u0441\\u0442\\u0438', u'18.73': u'\\u0417\\u0430\\u043a\\u0443\\u043f\\u043a\\u0438 \\u0438 \\u0441\\u043d\\u0430\\u0431\\u0436\\u0435\\u043d\\u0438\\u0435', u'18.75': u'\\u0417\\u043e\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a', u'2.43': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440', u'2.44': u'\\u0411\\u044e\\u0434\\u0436\\u0435\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0438 \\u043f\\u043b\\u0430\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'2.46': u'\\u0412\\u0430\\u043b\\u044e\\u0442\\u043d\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'1.536': u'CRM \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b', u'18.265': u'\\u0421\\u0435\\u043b\\u044c\\u0445\\u043e\\u0437\\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'13.567': u'\\u0414\\u0435\\u0444\\u0435\\u043a\\u0442\\u043e\\u043b\\u043e\\u0433, \\u041b\\u043e\\u0433\\u043e\\u043f\\u0435\\u0434', u'17.279': u'\\u0421\\u0442\\u0430\\u043d\\u043a\\u0438, \\u0442\\u044f\\u0436\\u0435\\u043b\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'19.483': u'\\u041f\\u0435\\u0440\\u0435\\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'15.313': u'\\u0422\\u0443\\u0440\\u0438\\u0437\\u043c, \\u0413\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u044b, \\u0420\\u0435\\u0441\\u0442\\u043e\\u0440\\u0430\\u043d\\u044b', u'13.447': u'\\u041f\\u0441\\u0438\\u0445\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f', u'8.135': u'\\u041b\\u0438\\u0447\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'22.223': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430 \\u0442\\u0443\\u0440\\u0438\\u0441\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u0443\\u0441\\u043b\\u0443\\u0433', u'16.569': u'\\u0410\\u0440\\u0445\\u0438\\u0432\\u0430\\u0440\\u0438\\u0443\\u0441', u'10.191': u'\\u041d\\u0435\\u0444\\u0442\\u044c', u'21.69': u'\\u0416\\u0435\\u043b\\u0435\\u0437\\u043d\\u043e\\u0434\\u043e\\u0440\\u043e\\u0436\\u043d\\u044b\\u0435 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'26.574': u'\\u0422\\u0435\\u043d\\u0434\\u0435\\u0440\\u044b', u'7.222': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430', u'17.65': u'\\u0414\\u0438\\u043b\\u0435\\u0440\\u0441\\u043a\\u0438\\u0435 \\u0441\\u0435\\u0442\\u0438', u'17.66': u'\\u0414\\u0438\\u0441\\u0442\\u0440\\u0438\\u0431\\u0443\\u0446\\u0438\\u044f', u'11.347': u'\\u0424\\u043e\\u0442\\u043e\\u0433\\u0440\\u0430\\u0444\\u0438\\u044f', u'27.496': u'\\u0412\\u043e\\u0441\\u043f\\u0438\\u0442\\u0430\\u0442\\u0435\\u043b\\u044c, \\u0413\\u0443\\u0432\\u0435\\u0440\\u043d\\u0430\\u043d\\u0442\\u043a\\u0430, \\u041d\\u044f\\u043d\\u044f', u'27.497': u'\\u0434\\u043e\\u043c\\u0440\\u0430\\u0431\\u043e\\u0442\\u043d\\u0438\\u0446\\u0430/\\u0434\\u043e\\u043c\\u0440\\u0430\\u0431\\u043e\\u0442\\u043d\\u0438\\u043a, \\u0413\\u043e\\u0440\\u043d\\u0438\\u0447\\u043d\\u0430\\u044f', u'20.186': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c/\\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'20.189': u'\\u041d\\u0435\\u0436\\u0438\\u043b\\u044b\\u0435 \\u043f\\u043e\\u043c\\u0435\\u0449\\u0435\\u043d\\u0438\\u044f', u'19.28': u'\\u0410\\u043d\\u0434\\u0435\\u0440\\u0440\\u0430\\u0439\\u0442\\u0435\\u0440', u'27.498': u'\\u041f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c', u'27.499': u'\\u0421\\u0430\\u0434\\u043e\\u0432\\u043d\\u0438\\u043a', u'19.357': u'\\u042d\\u043a\\u0441\\u043f\\u0435\\u0440\\u0442-\\u043e\\u0446\\u0435\\u043d\\u0449\\u0438\\u043a', u'20.20': u'\\u0410\\u0433\\u0435\\u043d\\u0442', u'14.355': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430, \\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442', u'1.211': u'\\u041f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u043a\\u0430, Helpdesk', u'18.201': u'\\u041e\\u0445\\u0440\\u0430\\u043d\\u0430 \\u0442\\u0440\\u0443\\u0434\\u0430', u'13.131': u'\\u041b\\u0435\\u0447\\u0430\\u0449\\u0438\\u0439 \\u0432\\u0440\\u0430\\u0447', u'21.17': u'\\u0410\\u0432\\u0442\\u043e\\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'21.12': u'\\u0410\\u0432\\u0438\\u0430\\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'13.138': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'18.208': u'\\u041f\\u0438\\u0449\\u0435\\u0432\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'3.244': u'\\u0420\\u0430\\u0434\\u0438\\u043e \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'4.332': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u044f\\u044e\\u0449\\u0438\\u0439 \\u043e\\u0444\\u0438\\u0441\\u043e\\u043c(\\u041effice manager)', u'5.424': u'\\u041f\\u0430\\u0435\\u0432\\u044b\\u0435 \\u0444\\u043e\\u043d\\u0434\\u044b', u'5.426': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0441\\u0442', u'4.338': u'\\u0423\\u0447\\u0435\\u0442 \\u0442\\u043e\\u0432\\u0430\\u0440\\u043e\\u043e\\u0431\\u043e\\u0440\\u043e\\u0442\\u0430', u'9.452': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'9.238': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e, \\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f', u'26.473': u'\\u0421\\u0442\\u0430\\u043d\\u043a\\u0438, \\u0422\\u044f\\u0436\\u0435\\u043b\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'3.31': u'\\u0410\\u0440\\u0442 \\u0434\\u0438\\u0440\\u0435\\u043a\\u0442\\u043e\\u0440', u'9.78': u'\\u0418\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'17.156': u'\\u041c\\u043d\\u043e\\u0433\\u043e\\u0443\\u0440\\u043e\\u0432\\u043d\\u0435\\u0432\\u044b\\u0439 \\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'5.210': u'\\u041f\\u043b\\u0430\\u0441\\u0442\\u0438\\u043a\\u043e\\u0432\\u044b\\u0435 \\u043a\\u0430\\u0440\\u0442\\u044b', u'5.215': u'\\u041f\\u043e\\u0440\\u0442\\u0444\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u0438\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'17.405': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'5.219': u'\\u041f\\u0440\\u0438\\u0432\\u043b\\u0435\\u0447\\u0435\\u043d\\u0438\\u0435 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442\\u043e\\u0432', u'17.152': u'\\u041c\\u0435\\u0442\\u0430\\u043b\\u043b\\u043e\\u043f\\u0440\\u043e\\u043a\\u0430\\u0442', u'4.52': u'\\u0412\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c', u'11.293': u'\\u0422\\u0435\\u043b\\u0435\\u0432\\u0438\\u0434\\u0435\\u043d\\u0438\\u0435', u'17.196': u'\\u041e\\u043f\\u0442\\u043e\\u0432\\u0430\\u044f \\u0442\\u043e\\u0440\\u0433\\u043e\\u0432\\u043b\\u044f', u'18.488': u'\\u041c\\u0435\\u0442\\u0440\\u043e\\u043b\\u043e\\u0433', u'18.330': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0446\\u0435\\u0445\\u043e\\u043c', u'9.345': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u044b', u'23.422': u'\\u0410\\u0432\\u0442\\u043e\\u0440\\u0441\\u043a\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'5.457': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043e\\u043f\\u0435\\u0440\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435', u'21.480': u'\\u041a\\u043e\\u043d\\u0442\\u0435\\u0439\\u043d\\u0435\\u0440\\u043d\\u044b\\u0435 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'5.103': u'\\u041a\\u0430\\u0441\\u0441\\u043e\\u0432\\u043e\\u0435 \\u043e\\u0431\\u0441\\u043b\\u0443\\u0436\\u0438\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0438\\u043d\\u043a\\u0430\\u0441\\u0441\\u0430\\u0446\\u0438\\u044f', u'5.450': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043c\\u0430\\u043b\\u043e\\u0433\\u043e \\u0438 \\u0441\\u0440\\u0435\\u0434\\u043d\\u0435\\u0433\\u043e \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'5.101': u'\\u0422\\u0440\\u0435\\u0439\\u0434\\u0438\\u043d\\u0433, \\u0414\\u0438\\u043b\\u0438\\u043d\\u0433', u'20.484': u'\\u0413\\u0435\\u043e\\u0434\\u0435\\u0437\\u0438\\u044f \\u0438 \\u043a\\u0430\\u0440\\u0442\\u043e\\u0433\\u0440\\u0430\\u0444\\u0438\\u044f', u'21.563': u'\\u041a\\u043b\\u0430\\u0434\\u043e\\u0432\\u0449\\u0438\\u043a', u'5.459': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u0440\\u044b\\u043d\\u043e\\u0447\\u043d\\u044b\\u0435', u'5.458': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0435', u'15.288': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e, \\u0410\\u0440\\u0445\\u0438\\u0442\\u0435\\u043a\\u0442\\u0443\\u0440\\u0430', u'5.79': u'\\u0418\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u043a\\u043e\\u043c\\u043f\\u0430\\u043d\\u0438\\u044f', u'18.130': u'\\u0414\\u0435\\u0440\\u0435\\u0432\\u043e\\u043e\\u0431\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430, \\u041b\\u0435\\u0441\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'23.214': u'\\u041f\\u043e\\u043c\\u043e\\u0449\\u043d\\u0438\\u043a', u'17.580': u'\\u041a\\u043b\\u0438\\u043d\\u0438\\u043d\\u0433\\u043e\\u0432\\u044b\\u0435 \\u0443\\u0441\\u043b\\u0443\\u0433\\u0438', u'27.584': u'\\u0420\\u0435\\u043f\\u0435\\u0442\\u0438\\u0442\\u043e\\u0440', u'23.352': u'\\u0426\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0431\\u0443\\u043c\\u0430\\u0433\\u0438, \\u0420\\u044b\\u043d\\u043a\\u0438 \\u043a\\u0430\\u043f\\u0438\\u0442\\u0430\\u043b\\u0430', u'6.309': u'\\u0422\\u0440\\u0435\\u043d\\u0438\\u043d\\u0433\\u0438', u'18.348': u'\\u0425\\u0438\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'19.170': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'3.150': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u0430 (Product manager)', u'3.151': u'\\u041c\\u0435\\u0440\\u0447\\u0435\\u043d\\u0434\\u0430\\u0439\\u0437\\u0438\\u043d\\u0433', u'1.25': u'\\u0410\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a', u'18.431': u'\\u0413\\u043b\\u0430\\u0432\\u043d\\u044b\\u0439 \\u043c\\u0435\\u0445\\u0430\\u043d\\u0438\\u043a', u'3.294': u'\\u0422\\u0435\\u043b\\u0435\\u0432\\u0438\\u0437\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'29.555': u'\\u041f\\u0435\\u0440\\u0435\\u043c\\u043e\\u0442\\u0447\\u0438\\u043a', u'29.554': u'\\u041f\\u0430\\u0441\\u0442\\u0443\\u0445, \\u0427\\u0430\\u0431\\u0430\\u043d', u'29.557': u'\\u0421\\u0430\\u043d\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a', u'7.502': u'\\u0422\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0449\\u0438\\u043a', u'29.551': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u0438\\u0441\\u0442 \\u044d\\u043a\\u0441\\u043a\\u0430\\u0432\\u0430\\u0442\\u043e\\u0440\\u0430', u'29.550': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u0438\\u0441\\u0442 \\u0441\\u0446\\u0435\\u043d\\u044b', u'29.553': u'\\u041e\\u043f\\u0435\\u0440\\u0430\\u0442\\u043e\\u0440 \\u0441\\u0442\\u0430\\u043d\\u043a\\u043e\\u0432', u'29.552': u'\\u041c\\u0435\\u0445\\u0430\\u043d\\u0438\\u043a', u'9.168': u'\\u041d\\u0430\\u0443\\u043a\\u0430, \\u041e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'15.68': u'\\u0414\\u043e\\u0431\\u044b\\u0447\\u0430 \\u0441\\u044b\\u0440\\u044c\\u044f', u'29.559': u'\\u0428\\u0442\\u0443\\u043a\\u0430\\u0442\\u0443\\u0440', u'29.558': u'\\u0428\\u043b\\u0438\\u0444\\u043e\\u0432\\u0449\\u0438\\u043a', u'7.267': u'\\u0421\\u0435\\u0440\\u0432\\u0438\\u0441\\u043d\\u043e\\u0435 \\u043e\\u0431\\u0441\\u043b\\u0443\\u0436\\u0438\\u0432\\u0430\\u043d\\u0438\\u0435', u'17.24': u'\\u0410\\u043b\\u043a\\u043e\\u0433\\u043e\\u043b\\u044c', u'8.519': u'\\u0418\\u043d\\u043a\\u0430\\u0441\\u0441\\u0430\\u0442\\u043e\\u0440', u'13.185': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'15.346': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u044b, \\u0411\\u0430\\u043d\\u043a\\u0438, \\u0418\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'18.153': u'\\u041c\\u0435\\u0442\\u0430\\u043b\\u043b\\u0443\\u0440\\u0433\\u0438\\u044f', u'26.449': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435/\\u0441\\u0432\\u0435\\u0442\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'26.408': u'\\u0410\\u043b\\u043a\\u043e\\u0433\\u043e\\u043b\\u044c', u'26.409': u'\\u0413\\u0421\\u041c, \\u043d\\u0435\\u0444\\u0442\\u044c, \\u0431\\u0435\\u043d\\u0437\\u0438\\u043d', u'2.434': u'\\u041f\\u043b\\u0430\\u043d\\u043e\\u0432\\u043e-\\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u0443\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435', u'3.64': u'\\u0414\\u0438\\u0437\\u0430\\u0439\\u043d\\u0435\\u0440', u'8.260': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c \\u0421\\u0411', u'26.406': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'26.407': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u0438, \\u0417\\u0430\\u043f\\u0447\\u0430\\u0441\\u0442\\u0438', u'10.54': u'\\u0413\\u0435\\u043e\\u043b\\u043e\\u0433\\u043e\\u0440\\u0430\\u0437\\u0432\\u0435\\u0434\\u043a\\u0430', u'21.53': u'\\u0412\\u042d\\u0414', u'3.209': u'\\u041f\\u043b\\u0430\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0420\\u0430\\u0437\\u043c\\u0435\\u0449\\u0435\\u043d\\u0438\\u0435 \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u044b', u'20.325': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'12.251': u'\\u0420\\u0435\\u0438\\u043d\\u0436\\u0438\\u043d\\u0438\\u0440\\u0438\\u043d\\u0433 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441 \\u043f\\u0440\\u043e\\u0446\\u0435\\u0441\\u0441\\u043e\\u0432', u'12.252': u'\\u0420\\u0435\\u0438\\u043d\\u0436\\u0438\\u043d\\u0438\\u0440\\u0438\\u043d\\u0433, \\u0410\\u0443\\u0442\\u0441\\u043e\\u0440\\u0441\\u0438\\u043d\\u0433 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u043e\\u0439 \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438', u'3.206': u'\\u041f\\u0435\\u0447\\u0430\\u0442\\u043d\\u0430\\u044f \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'20.287': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e', u'8.356': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u0438 \\u0438\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'9.562': u'\\u0410\\u043d\\u0442\\u0438\\u043a\\u0440\\u0438\\u0437\\u0438\\u0441\\u043d\\u043e\\u0435 \\u0443\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435', u'1.246': u'\\u0420\\u0430\\u0437\\u0432\\u0438\\u0442\\u0438\\u0435 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'17.397': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0421\\u0432\\u0435\\u0442\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'1.327': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'18.245': u'\\u0420\\u0430\\u0434\\u0438\\u043e\\u044d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'10.470': u'\\u0411\\u0443\\u0440\\u0435\\u043d\\u0438\\u0435', u'10.471': u'\\u041c\\u0430\\u0440\\u043a\\u0448\\u0435\\u0439\\u0434\\u0435\\u0440', u'18.373': u'\\u0421\\u0443\\u0434\\u043e\\u0441\\u0442\\u0440\\u043e\\u0435\\u043d\\u0438\\u0435', u'3.1': u'Below The Line (BTL)', u'11.157': u'\\u041c\\u043e\\u0434\\u0430', u'20.70': u'\\u0416\\u0438\\u043b\\u044c\\u0435', u'3.8': u'PR, \\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433\\u043e\\u0432\\u044b\\u0435 \\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438', u'22.39': u'\\u0411\\u0440\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'4.205': u'\\u041f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0430\\u0441\\u0441\\u0438\\u0441\\u0442\\u0435\\u043d\\u0442', u'4.207': u'\\u041f\\u0438\\u0441\\u044c\\u043c\\u0435\\u043d\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0434', u'5.371': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u044b: \\u0440\\u043e\\u0437\\u043d\\u0438\\u0447\\u043d\\u044b\\u0435', u'22.35': u'\\u0411\\u0430\\u043d\\u043a\\u0435\\u0442\\u044b', u'3.507': u'\\u0422\\u0430\\u0439\\u043d\\u044b\\u0439 \\u043f\\u043e\\u043a\\u0443\\u043f\\u0430\\u0442\\u0435\\u043b\\u044c', u'17.324': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430\\u043c\\u0438', u'3.509': u'\\u041f\\u0440\\u043e\\u043c\\u043e\\u0443\\u0442\\u0435\\u0440', u'3.508': u'\\u041f\\u0440\\u043e\\u0432\\u0435\\u0434\\u0435\\u043d\\u0438\\u0435 \\u043e\\u043f\\u0440\\u043e\\u0441\\u043e\\u0432, \\u0418\\u043d\\u0442\\u0435\\u0440\\u0432\\u044c\\u044e\\u0435\\u0440', u'9.307': u'\\u0422\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u0440\\u0442, \\u041b\\u043e\\u0433\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'4.429': u'\\u0414\\u0435\\u043b\\u043e\\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'4.428': u'\\u041f\\u043e\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0434', u'15.390': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441', u'15.391': u'\\u041a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'23.159': u'\\u041c\\u043e\\u0440\\u0441\\u043a\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'3.119': u'\\u041a\\u043e\\u043f\\u0438\\u0440\\u0430\\u0439\\u0442\\u0435\\u0440', u'9.115': u'\\u041a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'29.544': u'\\u0417\\u0430\\u043f\\u0440\\u0430\\u0432\\u0449\\u0438\\u043a \\u043a\\u0430\\u0440\\u0442\\u0440\\u0438\\u0434\\u0436\\u0435\\u0439', u'5.257': u'\\u0420\\u043e\\u0437\\u043d\\u0438\\u0447\\u043d\\u044b\\u0439 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441', u'5.106': u'\\u041a\\u043e\\u043c\\u043c\\u0435\\u0440\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0431\\u0430\\u043d\\u043a', u'15.140': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433, \\u0420\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430, PR', u'23.2': u'Compliance', u'5.250': u'\\u0420\\u0430\\u0441\\u0447\\u0435\\u0442\\u044b', u'5.394': u'\\u042d\\u043c\\u0438\\u0441\\u0441\\u0438\\u0438', u'14.169': u'\\u041d\\u0430\\u0443\\u043a\\u0438 \\u043e \\u0417\\u0435\\u043c\\u043b\\u0435', u'17.625': u'\\u0420\\u0435\\u0448\\u0435\\u043d\\u0438\\u044f \\u043f\\u043e \\u0430\\u0432\\u0442\\u043e\\u043c\\u0430\\u0442\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438 \\u043f\\u0440\\u043e\\u0446\\u0435\\u0441\\u0441\\u043e\\u0432', u'21.158': u'\\u041c\\u043e\\u0440\\u0441\\u043a\\u0438\\u0435/\\u0420\\u0435\\u0447\\u043d\\u044b\\u0435 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'20.387': u'\\u041f\\u0440\\u043e\\u0440\\u0430\\u0431', u'22.248': u'\\u0420\\u0430\\u0437\\u043c\\u0435\\u0449\\u0435\\u043d\\u0438\\u0435, \\u041e\\u0431\\u0441\\u043b\\u0443\\u0436\\u0438\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0433\\u043e\\u0441\\u0442\\u0435\\u0439', u'14.364': u'\\u042f\\u0437\\u044b\\u043a\\u0438', u'5.27': u'\\u0410\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a', u'9.94': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438, \\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442, \\u041c\\u0443\\u043b\\u044c\\u0442\\u0438\\u043c\\u0435\\u0434\\u0438\\u0430', u'9.95': u'\\u0418\\u0441\\u043a\\u0443\\u0441\\u0441\\u0442\\u0432\\u043e, \\u0420\\u0430\\u0437\\u0432\\u043b\\u0435\\u0447\\u0435\\u043d\\u0438\\u044f, \\u041c\\u0430\\u0441\\u0441-\\u043c\\u0435\\u0434\\u0438\\u0430', u'5.23': u'\\u0410\\u043a\\u0446\\u0438\\u0438, \\u0426\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0431\\u0443\\u043c\\u0430\\u0433\\u0438', u'29.511': u'\\u0422\\u043e\\u043a\\u0430\\u0440\\u044c, \\u0424\\u0440\\u0435\\u0437\\u0435\\u0440\\u043e\\u0432\\u0449\\u0438\\u043a', u'29.510': u'\\u0421\\u043b\\u0435\\u0441\\u0430\\u0440\\u044c', u'1.116': u'\\u041a\\u043e\\u043d\\u0442\\u0435\\u043d\\u0442', u'1.117': u'\\u0422\\u0435\\u0441\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.110': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'29.514': u'\\u0421\\u0432\\u0430\\u0440\\u0449\\u0438\\u043a', u'29.517': u'\\u0421\\u0431\\u043e\\u0440\\u0449\\u0438\\u043a', u'1.113': u'\\u041a\\u043e\\u043d\\u0441\\u0430\\u043b\\u0442\\u0438\\u043d\\u0433, \\u0410\\u0443\\u0442\\u0441\\u043e\\u0440\\u0441\\u0438\\u043d\\u0433', u'18.298': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433, \\u041c\\u044f\\u0441\\u043e- \\u0438 \\u043f\\u0442\\u0438\\u0446\\u0435\\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'29.582': u'\\u041f\\u0440\\u043e\\u0432\\u043e\\u0434\\u043d\\u0438\\u043a', u'29.583': u'\\u041c\\u0430\\u043b\\u044f\\u0440', u'1.50': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0443\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u044f \\u043f\\u0440\\u0435\\u0434\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u0435\\u043c (ERP)', u'29.581': u'\\u041c\\u043e\\u043d\\u0442\\u0430\\u0436\\u043d\\u0438\\u043a', u'15.96': u'\\u0418\\u0441\\u043a\\u0443\\u0441\\u0441\\u0442\\u0432\\u043e, \\u0420\\u0430\\u0437\\u0432\\u043b\\u0435\\u0447\\u0435\\u043d\\u0438\\u044f, \\u041c\\u0430\\u0441\\u0441-\\u043c\\u0435\\u0434\\u0438\\u0430', u'22.504': u'\\u0425\\u043e\\u0441\\u0442\\u0435\\u0441', u'29.588': u'\\u0414\\u0440\\u0443\\u0433\\u043e\\u0435', u'17.256': u'\\u0420\\u043e\\u0437\\u043d\\u0438\\u0447\\u043d\\u0430\\u044f \\u0442\\u043e\\u0440\\u0433\\u043e\\u0432\\u043b\\u044f', u'2.100': u'\\u041a\\u0430\\u0437\\u043d\\u0430\\u0447\\u0435\\u0439\\u0441\\u0442\\u0432\\u043e', u'2.102': u'\\u041a\\u0430\\u0441\\u0441\\u0438\\u0440, \\u0418\\u043d\\u043a\\u0430\\u0441\\u0441\\u0430\\u0442\\u043e\\u0440', u'16.571': u'\\u0411\\u0438\\u0431\\u043b\\u0438\\u043e\\u0442\\u0435\\u043a\\u0430\\u0440\\u044c', u'16.570': u'\\u0410\\u0442\\u0442\\u0430\\u0448\\u0435', u'13.228': u'\\u041b\\u0435\\u043a\\u0430\\u0440\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0435 \\u043f\\u0440\\u0435\\u043f\\u0430\\u0440\\u0430\\u0442\\u044b', u'13.229': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'13.220': u'\\u041f\\u0440\\u043e\\u0432\\u0438\\u0437\\u043e\\u0440', u'12.122': u'\\u041a\\u043e\\u0440\\u043f\\u043e\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u044b\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u044b', u'7.392': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c /\\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430 ', u'2.261': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0431\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440\\u0438\\u0435\\u0439', u'13.227': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'9.521': u'\\u0421\\u043f\\u043e\\u0440\\u0442\\u0438\\u0432\\u043d\\u044b\\u0435 \\u043a\\u043b\\u0443\\u0431\\u044b, \\u0424\\u0438\\u0442\\u043d\\u0435\\u0441, \\u0421\\u0430\\u043b\\u043e\\u043d\\u044b \\u043a\\u0440\\u0430\\u0441\\u043e\\u0442\\u044b', u'18.291': u'\\u0422\\u0430\\u0431\\u0430\\u0447\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'9.139': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433, \\u0420\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430, PR', u'15.237': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e, \\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'8.461': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0432\\u0438\\u0434\\u0435\\u043e\\u043d\\u0430\\u0431\\u043b\\u044e\\u0434\\u0435\\u043d\\u0438\\u044f', u'8.462': u'\\u0412\\u0437\\u044b\\u0441\\u043a\\u0430\\u043d\\u0438\\u0435 \\u0437\\u0430\\u0434\\u043e\\u043b\\u0436\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438, \\u041a\\u043e\\u043b\\u043b\\u0435\\u043a\\u0442\\u043e\\u0440\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'22.529': u'\\u0410\\u043d\\u0438\\u043c\\u0430\\u0446\\u0438\\u044f', u'16.194': u'\\u041e\\u0431\\u0449\\u0435\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0435 \\u043e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438', u'17.358': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u0438\\u043a\\u0430, \\u0444\\u043e\\u0442\\u043e, \\u0432\\u0438\\u0434\\u0435\\u043e', u'14.91': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0442\\u0438\\u043a\\u0430, \\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b', u'17.350': u'\\u0426\\u0432\\u0435\\u0442\\u043d\\u044b\\u0435 \\u043c\\u0435\\u0442\\u0430\\u043b\\u043b\\u044b', u'1.203': u'\\u041f\\u0435\\u0440\\u0435\\u0434\\u0430\\u0447\\u0430 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445 \\u0438 \\u0434\\u043e\\u0441\\u0442\\u0443\\u043f \\u0432 \\u0438\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442', u'11.218': u'\\u041f\\u0440\\u0435\\u0441\\u0441\\u0430', u'19.586': u'\\u0412\\u0440\\u0430\\u0447-\\u044d\\u043a\\u0441\\u043f\\u0435\\u0440\\u0442', u'13.128': u'\\u041b\\u0430\\u0431\\u043e\\u0440\\u0430\\u043d\\u0442', u'12.280': u'\\u0421\\u0442\\u0440\\u0430\\u0442\\u0435\\u0433\\u0438\\u044f', u'6.184': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'21.403': u'\\u0411\\u0438\\u0437\\u043d\\u0435\\u0441-\\u0430\\u0432\\u0438\\u0430\\u0446\\u0438\\u044f', u'21.402': u'\\u0413\\u0440\\u0430\\u0436\\u0434\\u0430\\u043d\\u0441\\u043a\\u0430\\u044f \\u0430\\u0432\\u0438\\u0430\\u0446\\u0438\\u044f', u'3.423': u'\\u0410\\u0441\\u0441\\u0438\\u0441\\u0442\\u0435\\u043d\\u0442', u'3.26': u'\\u0410\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a', u'14.217': u'\\u041f\\u0440\\u0435\\u043f\\u043e\\u0434\\u0430\\u0432\\u0430\\u043d\\u0438\\u0435', u'9.226': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'18.143': u'\\u041c\\u0435\\u0431\\u0435\\u043b\\u044c\\u043d\\u043e\\u0435 \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'5.163': u'\\u041d\\u0430\\u043b\\u043e\\u0433\\u0438', u'23.442': u'\\u041c\\u0435\\u0436\\u0434\\u0443\\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'1.475': u'\\u0418\\u0433\\u0440\\u043e\\u0432\\u043e\\u0435 \\u041f\\u041e', u'1.474': u'\\u0421\\u0442\\u0430\\u0440\\u0442\\u0430\\u043f\\u044b', u'1.274': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0430\\u0432\\u0442\\u043e\\u043c\\u0430\\u0442\\u0438\\u0437\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043d\\u043e\\u0433\\u043e \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f', u'1.277': u'\\u0421\\u043e\\u0442\\u043e\\u0432\\u044b\\u0435, \\u0411\\u0435\\u0441\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'1.270': u'\\u0421\\u0435\\u0442\\u0435\\u0432\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'1.273': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u043d\\u044b\\u0439 \\u0430\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u043e\\u0440', u'1.272': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u043d\\u0430\\u044f \\u0438\\u043d\\u0442\\u0435\\u0433\\u0440\\u0430\\u0446\\u0438\\u044f', u'17.440': u'\\u0422\\u0435\\u043a\\u0441\\u0442\\u0438\\u043b\\u044c, \\u041e\\u0434\\u0435\\u0436\\u0434\\u0430, \\u041e\\u0431\\u0443\\u0432\\u044c', u'17.183': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.187': u'\\u041d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u044c', u'18.300': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0441\\u0430\\u0445\\u0430\\u0440\\u0430', u'21.275': u'\\u0421\\u043a\\u043b\\u0430\\u0434\\u0441\\u043a\\u043e\\u0435 \\u0445\\u043e\\u0437\\u044f\\u0439\\u0441\\u0442\\u0432\\u043e', u'4.494': u'\\u0423\\u0431\\u043e\\u0440\\u0449\\u0438\\u0446\\u0430', u'5.444': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u043c\\u043e\\u043d\\u0438\\u0442\\u043e\\u0440\\u0438\\u043d\\u0433', u'23.188': u'\\u041d\\u0435\\u0434\\u0440\\u043e\\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'18.84': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440, \\u041c\\u044f\\u0441\\u043e- \\u0438 \\u043f\\u0442\\u0438\\u0446\\u0435\\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'4.255': u'\\u0420\\u0435\\u0441\\u0435\\u043f\\u0448\\u0435\\u043d', u'20.490': u'\\u0420\\u0430\\u0431\\u043e\\u0447\\u0438\\u0435 \\u0441\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0445 \\u0441\\u043f\\u0435\\u0446\\u0438\\u0430\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u0435\\u0439', u'23.182': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430'}\n",
      "0.13013013013\n"
     ]
    }
   ],
   "source": [
    "###специализации\n",
    "import httplib\n",
    "import json\n",
    "headers = {\"Authorization\": \"Bearer PHUAM0L3PU56VNT041CJM3MUTGSABUCDTMBVHEAMF5CGCDEEIC7VFFT4VLOP0GQP\", \n",
    "           \"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"/specializations\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "\n",
    "#print json.loads(r1.read())\n",
    "specs = {}\n",
    "for k in [d['specializations'] for d in json.loads(r1.read())]:\n",
    "    for n in k:\n",
    "        specs[n['id']] = n['name']\n",
    "print specs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.3\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "###ЗП\n",
    "real = 100000.0\n",
    "expected = 100000.0\n",
    "print 1-(abs(expected-real)/max(real,expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "##опыт работы\n",
    "real = 0\n",
    "expected = 0\n",
    "print (3.0 - abs(real-expected))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##навыки\n",
    "import httplib\n",
    "import json\n",
    "import pickle\n",
    "headers = {\"Authorization\": \"Bearer UGE7369P4FJVPH027PQII1S422JHFJ2NQADKREFO8KTUP1BCIJQ2T6BUQ7CLQMGF\", \n",
    "           \"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"/specializations\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "\n",
    "# print json.loads(r1.read())\n",
    "spec_ids = []\n",
    "spec_names = []\n",
    "for k in [d['specializations'] for d in json.loads(r1.read())]:\n",
    "    for n in k:\n",
    "        spec_ids.append(n['id'])\n",
    "        spec_names.append(n['name'])\n",
    "pickle.dump( spec_ids, open( \"spec_ids.p\", \"wb\" ) )\n",
    "pickle.dump( spec_names, open( \"spec_names.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199376\n",
      "6286\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "##вытаскиваем из вакансий ключевые навыки\n",
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "db = TinyDB('/home/shurik2533/vacancies.json')\n",
    "db2 = TinyDB('/home/shurik2533/vacancies2.json')\n",
    "vacancies = db.all()\n",
    "vacancies2 = db2.all()\n",
    "all_key_skills = []\n",
    "key_skills_set = set()\n",
    "key_skills_dict = dict()\n",
    "for vacancy in vacancies:\n",
    "#     print vacancy['name']\n",
    "    for skill in vacancy['key_skills']:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            all_key_skills.append(word)\n",
    "            key_skills_set.add(word)\n",
    "            if word in key_skills_dict:\n",
    "                key_skills_dict[word] = key_skills_dict[word]+1;\n",
    "            else:\n",
    "                key_skills_dict[word] = 1;\n",
    "#     print vacancy['description']\n",
    "\n",
    "for vacancy in vacancies2:\n",
    "#     print vacancy['name']\n",
    "    for skill in vacancy['key_skills']:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            all_key_skills.append(word)\n",
    "            key_skills_set.add(word)\n",
    "            if word in key_skills_dict:\n",
    "                key_skills_dict[word] = key_skills_dict[word]+1;\n",
    "            else:\n",
    "                key_skills_dict[word] = 1;\n",
    "#     print vacancy['description']\n",
    "\n",
    "\n",
    "print len(all_key_skills)\n",
    "print len(key_skills_set)\n",
    "for k in key_skills_dict.keys():\n",
    "    if key_skills_dict[k] <= 7:\n",
    "        del key_skills_dict[k]\n",
    "\n",
    "res = list(key_skills_dict.keys())\n",
    "print len(res)\n",
    "pickle.dump( res, open( \"key_skills.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "print len(key_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44308\n",
      "11107\n",
      "1968\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "\n",
    "db = TinyDB('/home/shurik2533/vacancies.json')\n",
    "db2 = TinyDB('/home/shurik2533/vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "titles_s = set()\n",
    "for vac in vacancies:\n",
    "    titles_s.add(vac['name'])\n",
    "print len(titles_s)\n",
    "title_words = dict()\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "for title in titles_s:\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', title.lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        if word in title_words:\n",
    "            title_words[word] = title_words[word]+1;\n",
    "        else:\n",
    "            title_words[word] = 1;\n",
    "print len(title_words)\n",
    "for k in title_words.keys():\n",
    "    if title_words[k] <= 8 or len(k.strip()) <= 2:\n",
    "        del title_words[k]\n",
    "print len(title_words)\n",
    "res = list(title_words.keys())\n",
    "pickle.dump( res, open( \"title_words.p\", \"wb\" ) )\n",
    "# vacancies.append(vacancies2)\n",
    "# print len(vacancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113289\n",
      "290361\n",
      "137670\n",
      " юрист ведущ по претензион исков и договорн работ в област аренд недвижим в тр комплекс новомосковск в город московск ул хабаров м саларьев в администрац всег комплекс занима аренд коммерческ нежил помещен офис и торгов площади. обязанности: переговор с арендаторами. договорн работа. претензион - исков работ (иски, претенз и т. д.) представительств в арбитражн суд и административн процессе. требования: высш юридическ образование. хорош знан договорн работ в област аренд !!! оп работ с арбитражн суд в област «аренд недвижимости» ! условия: . гр/р: 5/2 , с 9.00 ч. – 18.00 ч. з/плата: пок 55 000 руб. - 60 000 руб. ( на рук ), в зависим от ваш опыта! карьерн рост : начальник юридическ отдела, с з/п до 100 000 руб. оплат телефон . оформлен по тк рф социальн пакет по тк рф в ближайш 3 - 4 месяц будет сво столов – бесплатн обеды! проезд: м. саларьево, 420 автобус, (10-12 минут), , остановк по требован «гостиничн комплекс.\n",
      "<p>Юрист (ведущий) по претензионно - исковой и договорной работе в области «Аренда недвижимости» в ТР Комплекс «НовоМосковский», в город Московский, ул. Хабарова (м. Саларьево). В администрацию всего Комплекса, занимающейся арендой коммерческих, нежилых помещений – офисы и торговые площади. </p> <p><strong>Обязанности: </strong></p> <ul> <li>Переговоры с Арендаторами.</li> <li>Договорная работа.</li> <li>Претензионно - исковая работа (иски, претензии и т. д.)</li> <li>Представительство в Арбитражном суде и административном процессе. </li> </ul> <p> <strong>Требования: </strong></p> <ul> <li>Высшее Юридическое образование. </li> <li>Хорошее знание Договорной работы в области Аренды !!!</li> <li>Опыт работы с Арбитражном судом в области «Аренда недвижимости» !</li> </ul> <p><strong>Условия: </strong>.</p> <ul> <li>Гр/р: 5/2 , с 9.00 ч. – 18.00 ч.</li> <li>З/плата: пока 55 000 руб. - 60 000 руб. ( на руки ), в зависимости от Вашего опыта!</li> <li>Карьерный рост : начальник юридического отдела, с з/п до 100 000 руб.</li> <li>Оплата телефона .</li> <li>Оформление по ТК РФ</li> <li>Социальный пакет по ТК РФ</li> <li>В ближайшие 3 - 4 месяца будет своя столовая – бесплатные обеды!</li> </ul> Проезд: М. Саларьево, 420 автобус, (10-12 минут), , остановка по требованию «Гостиничный Комплекс&quot;.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "\n",
    "db = TinyDB('/home/shurik2533/vacancies.json')\n",
    "db2 = TinyDB('/home/shurik2533/vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "docs = []\n",
    "for vac in vacancies:\n",
    "    docs.append(vac['description'])\n",
    "print len(docs)\n",
    "    \n",
    "    \n",
    "doc_words = dict()\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "p_docs = []\n",
    "for doc in docs:\n",
    "    p_doc = \"\"\n",
    "    doc = re.sub('<[^>]*>', '', doc.lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        p_doc = p_doc + \" \" + word\n",
    "        if word in doc_words:\n",
    "            doc_words[word] = doc_words[word]+1;\n",
    "        else:\n",
    "            doc_words[word] = 1;\n",
    "    p_docs.append(p_doc)\n",
    "print len(doc_words)\n",
    "for k in doc_words.keys():\n",
    "    if doc_words[k] == 1 or len(k.strip()) <= 1:\n",
    "        del doc_words[k]\n",
    "print len(doc_words)\n",
    "# pickle.dump( doc_words, open( \"doc_words.p\", \"wb\" ) )\n",
    "pickle.dump( p_docs, open( \"processed_docs.p\", \"wb\" ) )\n",
    "print p_docs[0]\n",
    "print docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Юрист (ведущий) по претензионно - исковой и договорной работе в области «Аренда недвижимости» в ТР Комплекс «НовоМосковский», в город Московский, ул. Хабарова (м. Саларьево). В администрацию всего Комплекса, занимающейся арендой коммерческих, нежилых помещений – офисы и торговые площади.  Обязанности:   Переговоры с Арендаторами. Договорная работа. Претензионно - исковая работа (иски, претензии и т. д.) Представительство в Арбитражном суде и административном процессе.    Требования:   Высшее Юридическое образование.  Хорошее знание Договорной работы в области Аренды !!! Опыт работы с Арбитражном судом в области «Аренда недвижимости» !  Условия: .  Гр/р: 5/2 , с 9.00 ч. – 18.00 ч. З/плата: пока 55 000 руб. - 60 000 руб. ( на руки ), в зависимости от Вашего опыта! Карьерный рост : начальник юридического отдела, с з/п до 100 000 руб. Оплата телефона . Оформление по ТК РФ Социальный пакет по ТК РФ В ближайшие 3 - 4 месяца будет своя столовая – бесплатные обеды!  Проезд: М. Саларьево, 420 автобус, (10-12 минут), , остановка по требованию «Гостиничный Комплекс&quot;.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "doc = \"<p>Юрист (ведущий) по претензионно - исковой и договорной работе в области «Аренда недвижимости» в ТР Комплекс «НовоМосковский», в город Московский, ул. Хабарова (м. Саларьево). В администрацию всего Комплекса, занимающейся арендой коммерческих, нежилых помещений – офисы и торговые площади. </p> <p><strong>Обязанности: </strong></p> <ul> <li>Переговоры с Арендаторами.</li> <li>Договорная работа.</li> <li>Претензионно - исковая работа (иски, претензии и т. д.)</li> <li>Представительство в Арбитражном суде и административном процессе. </li> </ul> <p> <strong>Требования: </strong></p> <ul> <li>Высшее Юридическое образование. </li> <li>Хорошее знание Договорной работы в области Аренды !!!</li> <li>Опыт работы с Арбитражном судом в области «Аренда недвижимости» !</li> </ul> <p><strong>Условия: </strong>.</p> <ul> <li>Гр/р: 5/2 , с 9.00 ч. – 18.00 ч.</li> <li>З/плата: пока 55 000 руб. - 60 000 руб. ( на руки ), в зависимости от Вашего опыта!</li> <li>Карьерный рост : начальник юридического отдела, с з/п до 100 000 руб.</li> <li>Оплата телефона .</li> <li>Оформление по ТК РФ</li> <li>Социальный пакет по ТК РФ</li> <li>В ближайшие 3 - 4 месяца будет своя столовая – бесплатные обеды!</li> </ul> Проезд: М. Саларьево, 420 автобус, (10-12 минут), , остановка по требованию «Гостиничный Комплекс&quot;.\"\n",
    "doc = re.sub('<[^>]*>', '', doc)\n",
    "print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113289, 120498)\n",
      "(113289, 3443)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "doc_words = pickle.load( open( \"processed_docs.p\", \"rb\" ) )\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(doc_words)\n",
    "# T = vectorizer.transform(['java разработчик программист'])\n",
    "\n",
    "print X.shape\n",
    "\n",
    "sel = VarianceThreshold(0.004)\n",
    "X2 = sel.fit_transform(X)\n",
    "# T2 = sel.transform(T)\n",
    "# print T2.toarray()\n",
    "\n",
    "print X2.shape\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "X2_tfidf = transformer.fit_transform(X2)\n",
    "\n",
    "pickle.dump( vectorizer, open( \"doc_vectorizer.p\", \"wb\" ) )\n",
    "pickle.dump( sel, open( \"count_vectorizer.p\", \"wb\" ) )\n",
    "pickle.dump( transformer, open( \"tfidf_transformer.p\", \"wb\" ) )\n",
    "\n",
    "# T2_tfidf = transformer.transform(T2)\n",
    "# result = cosine_similarity(T2_tfidf, X2_tfidf)\n",
    "\n",
    "# res = heapq.nlargest(10, range(len(result[0])), result[0].take)\n",
    "#print doc_words[result.argmax(axis=1)]\n",
    "# for i in res:\n",
    "#     print doc_words[i]\n",
    "   \n",
    "# from scipy import spatial\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# i = 0\n",
    "# for x_row in X2_tfidf:\n",
    "#     result = cosine_similarity(T2_tfidf, x_row)\n",
    "#     #result = spatial.distance.cosine(, )\n",
    "#     print result\n",
    "#     i = i+1\n",
    "#     if i > 10:\n",
    "#         break\n",
    "#result = spatial.distance.cosine(T2_tfidf, X2_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vacancies?per_page=200&date_from=2016-05-21T13:09:46&date_to=2016-05-21T13:14:46&page=0\n",
      "39\n",
      "starting t1\n",
      "21 sec\n"
     ]
    }
   ],
   "source": [
    "import httplib\n",
    "import json\n",
    "import datetime\n",
    "import MySQLdb\n",
    "import ConfigParser\n",
    "import time\n",
    "import threading\n",
    "from dateutil.parser import parse\n",
    "\n",
    "current_time = lambda: int(round(time.time()))\n",
    "start = current_time()\n",
    "    \n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "cursor.close()\n",
    "\n",
    "def get_vacancy_ids():\n",
    "    vacancy_ids = []\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    per_page = 200\n",
    "    page = 0\n",
    "    count = per_page\n",
    "    date_from = (datetime.datetime.now() - datetime.timedelta(minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    date_to = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    while count == per_page:\n",
    "        path = (\"/vacancies?per_page={}&date_from={}&date_to={}&page={}\"\n",
    "                .format(per_page, date_from, date_to, page))\n",
    "        print path\n",
    "\n",
    "        conn.request(\"GET\", path, headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        vacancies = r1.read()\n",
    "\n",
    "        count = len(json.loads(vacancies)['items'])\n",
    "        page = page+1\n",
    "        for item in json.loads(vacancies)['items']:\n",
    "            vacancy_ids.append(item['id'])\n",
    "    return vacancy_ids\n",
    "        \n",
    "\n",
    "def process_vacancies(vacancy_ids):\n",
    "    headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.readfp(open('my.cfg'))\n",
    "\n",
    "    db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                         port=config.getint('mysqld', 'port'), \n",
    "                         user=config.get('mysqld', 'user'), \n",
    "                         passwd=config.get('mysqld', 'password'), \n",
    "                         db=config.get('mysqld', 'database') )\n",
    "    db.set_character_set('utf8')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute('SET NAMES utf8;')\n",
    "    cursor.execute('SET CHARACTER SET utf8;')\n",
    "    cursor.execute('SET character_set_connection=utf8;')\n",
    "    cursor.close()\n",
    "\n",
    "    for vac_id in vacancy_ids:\n",
    "        conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        vacancy = r1.read()\n",
    "        vacancy_json = json.loads(vacancy)\n",
    "        cursor = db.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "          INSERT INTO vacancies (id, updated, item) \n",
    "          VALUES (%s, %s, %s)\n",
    "          ON DUPLICATE KEY UPDATE \n",
    "            updated  = VALUES(updated), \n",
    "            item   = VALUES(item)\"\"\", (vac_id, \n",
    "                                       parse(vacancy_json['published_at']).strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "                                       vacancy))\n",
    "        db.commit() \n",
    "        cursor.close()\n",
    "    db.close()\n",
    "\n",
    "ids = get_vacancy_ids()\n",
    "print len(ids)\n",
    "vac_id_chunks=[ids[x:x+100] for x in xrange(0, len(ids), 100)]\n",
    "t_num = 1;\n",
    "threads = []\n",
    "for vac_id_chunk in vac_id_chunks:\n",
    "    print 'starting t{}'.format(t_num)\n",
    "    t_num = t_num + 1\n",
    "    t = threading.Thread(target=process_vacancies, kwargs={'vacancy_ids': vac_id_chunk})\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "\n",
    "db.close()\n",
    "print \"{} sec\".format(current_time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#vacancy vectorizer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"\"\"SELECT item FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT 10\"\"\")\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "features_by_type = {}\n",
    "for employment_type in dictionaries_json['employment']:\n",
    "    features_by_type[employment_type['id']] = []\n",
    "\n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "vectorizer = pickle.load( open( \"doc_vectorizer.p\", \"rb\" ) )\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "# # специацизации | уровни зп| навыки (собираем из вакансий) | \n",
    "# # список слов заголовка (собираем из вакансий) | текст (собираем из вакансий)\n",
    "for item in cursor:\n",
    "    feature = []\n",
    "    vacancy = json.loads(item[0])\n",
    "    \n",
    "    #specializations\n",
    "    vac_specializations = vacancy['specializations']\n",
    "    spec_features = [0] * len(spec_ids)\n",
    "    for vac_spec in vac_specializations:\n",
    "        spec_features[spec_ids.index(vac_spec['id'])] = 1        \n",
    "    feature = feature + spec_features\n",
    "    \n",
    "    #salary\n",
    "    salary = 0\n",
    "    if vacancy['salary'] != None:\n",
    "        if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "            salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "        elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "            salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "        elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "            salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "    max_salary = 500000.0\n",
    "    if salary >= max_salary:\n",
    "        salary = max_salary\n",
    "    salary = salary/max_salary\n",
    "    feature.append(salary)\n",
    "    \n",
    "    #keyskills\n",
    "    skill_features = [0] * len(key_skills)\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if word in key_skills:\n",
    "                skill_features[key_skills.index(word)] = 1 \n",
    "    feature = feature + skill_features\n",
    "    \n",
    "    #title\n",
    "    title_features = [0] * len(title_words)\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if title_word in title_words:\n",
    "            title_features[title_words.index(title_word)] = 1\n",
    "    feature = feature + title_features\n",
    "    \n",
    "    #description\n",
    "    p_doc = \"\"\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        p_doc = p_doc + \" \" + word\n",
    "    p_doc_vec = vectorizer.transform([p_doc])\n",
    "    p_doc_vec2 = count_vectorizer.transform(p_doc_vec)\n",
    "    p_doc_vec_tfidf = tfidf_transformer.transform(p_doc_vec2)\n",
    "    feature = feature + list(p_doc_vec_tfidf.toarray()[0])\n",
    "    features_by_type[vacancy['employment']['id']].append(feature)\n",
    "\n",
    "pickle.dump( features_by_type, open( \"features_by_type.p\", \"wb\" ) )\n",
    "cursor.close()\n",
    "\n",
    "db.close()\n",
    "\n",
    "print('done')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      "Java Developer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ea316ec2e81d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mp_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_doc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mp_doc_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0mp_doc_vec2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_doc_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[0mp_doc_vec_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_doc_vec2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_doc_vec_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 238\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    506\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "#resume vectorizer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "headers = {\"User-Agent\": \"hh-recommender\", \"Authorization\" : \"Bearer T5MIT6GVV85LSVR75CB7U768TR3PFGS990I3QJNFV6A4CBQJF6M30G0MOT8U2V8I\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/mine\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "me = r1.read()\n",
    "me_json = json.loads(me)\n",
    "print me_json['items'][0]['id']\n",
    "\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(me_json['items'][0]['id']), headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "resume = r1.read()\n",
    "resume_json = json.loads(resume)\n",
    "\n",
    "feature = []\n",
    "resume_type = 'full'\n",
    "\n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "vectorizer = pickle.load( open( \"doc_vectorizer.p\", \"rb\" ) )\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "# # специацизации | уровни зп| навыки (собираем из вакансий) | \n",
    "# # список слов заголовка (собираем из вакансий) | текст (собираем из вакансий)\n",
    "\n",
    "resume_type = resume_json['employment']['id']\n",
    "\n",
    "#specializations\n",
    "res_specializations = resume_json['specialization']\n",
    "spec_features = [0] * len(spec_ids)\n",
    "for res_spec in res_specializations:\n",
    "    spec_features[spec_ids.index(res_spec['id'])] = 1        \n",
    "feature = feature + spec_features\n",
    "\n",
    "#salary\n",
    "salary = 0\n",
    "if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "    salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "max_salary = 500000.0\n",
    "if salary >= max_salary:\n",
    "    salary = max_salary\n",
    "salary = salary/max_salary\n",
    "feature.append(salary)\n",
    "\n",
    "#keyskills\n",
    "skill_features = [0] * len(key_skills)\n",
    "res_skills = resume_json['skill_set']\n",
    "for skill in res_skills:\n",
    "    words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        if word in key_skills:\n",
    "            skill_features[key_skills.index(word)] = 1 \n",
    "feature = feature + skill_features\n",
    "\n",
    "#title\n",
    "title_features = [0] * len(title_words)\n",
    "title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', title.strip())\n",
    "for title_word in words:\n",
    "    title_word = stemmer.stemWord(title_word)\n",
    "    if title_word in title_words:\n",
    "        title_features[title_words.index(title_word)] = 1\n",
    "feature = feature + title_features\n",
    "\n",
    "#description\n",
    "doc = resume_json['skills']\n",
    "if resume_json['experience'] != None and len(resume_json['experience']) > 0:\n",
    "    doc =  doc + \" \" + resume_json['experience'][0]['description']\n",
    "p_doc = \"\"\n",
    "doc = re.sub('<[^>]*>', '', doc.lower())\n",
    "doc = re.sub('&quot;', '', doc)\n",
    "doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', doc.strip())\n",
    "for word in words:\n",
    "    word = stemmer.stemWord(word)\n",
    "    p_doc = p_doc + \" \" + word\n",
    "p_doc_vec = vectorizer.transform([p_doc])\n",
    "p_doc_vec2 = count_vectorizer.transform(p_doc_vec)\n",
    "p_doc_vec_tfidf = tfidf_transformer.transform(p_doc_vec2)\n",
    "feature = feature + list(p_doc_vec_tfidf.toarray()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacancies done\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      " опытн разработчик программн обеспечен им широк спектр навык знан технолог разрабатыва быстр программ работа больш объем дан многопотоков сред уделя больш вниман качеств код удобств дальн поддержк аналитическ подхож решен задач. им хорош знан алгоритм структур данных. хобби, увлечен интернет-технологии, путешествия, фотография, кулинария. организова вед местн школ кружок по программирован для детей. последн врем интерес машин обучен opencv профил на habrahabr: http://habrahabr.ru/users/shurik2533/  java developer  java java ee oracle pl/sql python data analysis big data recommender systems machine learning\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs processed\n",
      "9924\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#из каждой профобласти извлекаем ключевые слова, релевантные для нее.\n",
    "#есть и общие слова. каждая профобласть должна иметь свой вектор множителей весов\n",
    "#верная зп должна добавлять какой-то вес (сортировать результат)\n",
    "#для каждой профобласти считать свой tfidf? и вектор кол-ва найденных слов умножать на него?\n",
    "#не забыть учитывать тип занятости\n",
    "\n",
    "#собираем вектор из таких фич, чтобы определенная часть в большей степени соответствовала определенной специялизации\n",
    "#тогда потом, когда будем считать косинусное расстояние будет больше созпадать с нужной профобластью\n",
    "\n",
    "\n",
    "#!считал не верно. считаем tfidf для каждой специализации. на основе этого выбираем самые значимые из специализации\n",
    "#слова из которых затем строитятся вектора. Попробовать использовать vocabulary\n",
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "db = TinyDB('vacancies.json')\n",
    "db2 = TinyDB('vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "# db = TinyDB('vacancies.json')\n",
    "# vacancies = db.all()\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "vac_by_spec = {}\n",
    "i = 0\n",
    "#keyskills, title, description\n",
    "for vacancy in vacancies:\n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "            \n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "    \n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "    \n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    \n",
    "    for spec in vacancy['specializations']:\n",
    "        if spec['id'] not in vac_by_spec:\n",
    "            vac_by_spec[spec['id']] = []\n",
    "        vac_by_spec[spec['id']].append(p_doc)\n",
    "    \n",
    "#     i = i+1\n",
    "#     if i > 100:\n",
    "#         break\n",
    "print 'docs processed'\n",
    "words = set()\n",
    "for key in vac_by_spec:\n",
    "    corpus = vac_by_spec[key]\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformer = TfidfTransformer()\n",
    "    X_tfidf = transformer.fit_transform(X)\n",
    "    spec_means = X_tfidf.mean(axis=0)\n",
    "    spec_means_arr = numpy.squeeze(numpy.asarray(spec_means))\n",
    "    res = heapq.nlargest(350, range(len(spec_means_arr)), spec_means_arr.take)\n",
    "    for i in res:\n",
    "        words.add(vectorizer.get_feature_names()[i])\n",
    "        \n",
    "print len(words)\n",
    "pickle.dump( words, open( \"spec_dict2.p\", \"wb\" ) )\n",
    "print 'finish'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6316672   0.44943642  0.6316672   0.        ]\n",
      " [ 0.          0.81818021  0.          0.57496187]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "corpus = ['aa bb cc', 'bb bb dd']\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X)\n",
    "print X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish collect docs\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "db = TinyDB('vacancies.json')\n",
    "db2 = TinyDB('vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "# db = TinyDB('vacancies.json')\n",
    "# vacancies = db.all()\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "i = 0\n",
    "p_docs = []\n",
    "#keyskills, title, description\n",
    "for vacancy in vacancies:\n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "            \n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "    \n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "    \n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    p_docs.append(p_doc)\n",
    "    \n",
    "print 'finish collect docs'\n",
    "\n",
    "voc = pickle.load( open( \"spec_dict2.p\", \"rb\" ) )\n",
    "vectorizer = CountVectorizer(min_df=1, vocabulary=voc)\n",
    "X = vectorizer.fit_transform(p_docs)\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X)\n",
    "pickle.dump( vectorizer, open( \"count_vectorizer.p\", \"wb\" ) )\n",
    "pickle.dump( transformer, open( \"tfidf_transformer.p\", \"wb\" ) )\n",
    "\n",
    "print 'finish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 113289.0 - 35969.0 - 0.317497727052\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "db = TinyDB('vacancies.json')\n",
    "db2 = TinyDB('vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "# db = TinyDB('vacancies.json')\n",
    "# vacancies = db.all()\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "i = 0.0\n",
    "p_docs = []\n",
    "none = 0.0\n",
    "#keyskills, title, description\n",
    "for vacancy in vacancies:\n",
    "    if vacancy['salary'] == None:\n",
    "        none = none+1\n",
    "    i = i+1\n",
    "#     if i > 10:\n",
    "#         break\n",
    "    \n",
    "print 'finish {} - {} - {}'.format(i, none, none/i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      "vacancies done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3c680237e6be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_by_type\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresume_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    413\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[1;32m--> 415\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "features_by_type = {}\n",
    "for employment_type in dictionaries_json['employment']:\n",
    "    features_by_type[employment_type['id']] = []\n",
    "\n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT 10\"\"\")\n",
    "data = []\n",
    "for item in cursor:\n",
    "    feature = []\n",
    "    vacancy = json.loads(item[0])\n",
    "    data.append(vacancy['name'])\n",
    "    \n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "            \n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "    \n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "    \n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    \n",
    "    feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "    tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "    \n",
    "    if features_by_type[vacancy['employment']['id']] == None:\n",
    "        features_by_type[vacancy['employment']['id']] = []\n",
    "    features_by_type[vacancy['employment']['id']].append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "db.close()\n",
    "\n",
    "print('vacancies done')\n",
    "\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\", \"Authorization\" : \"Bearer T5MIT6GVV85LSVR75CB7U768TR3PFGS990I3QJNFV6A4CBQJF6M30G0MOT8U2V8I\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/mine\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "me = r1.read()\n",
    "me_json = json.loads(me)\n",
    "\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(me_json['items'][0]['id']), headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "resume = r1.read()\n",
    "resume_json = json.loads(resume)\n",
    "\n",
    "feature = []\n",
    "resume_type = 'full'\n",
    "\n",
    "resume_type = resume_json['employment']['id']\n",
    "\n",
    "#description\n",
    "p_doc = ''\n",
    "doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "doc = re.sub('&quot;', '', doc)\n",
    "doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', doc.strip())\n",
    "for word in words:\n",
    "    word = stemmer.stemWord(word.strip())\n",
    "    if len(word.strip()) > 1:\n",
    "        p_doc = p_doc + \" \" + word\n",
    "\n",
    "#title\n",
    "p_title = ''\n",
    "title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', title.strip())\n",
    "for title_word in words:\n",
    "    title_word = stemmer.stemWord(title_word)\n",
    "    if len(title_word.strip()) > 1:\n",
    "        p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "#keyskills\n",
    "p_skills = ''\n",
    "res_skills = resume_json['skill_set']\n",
    "for skill in res_skills:\n",
    "    words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        if len(word.strip()) > 1:\n",
    "            p_skills = p_skills + \" \" + word.strip()\n",
    "\n",
    "p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "\n",
    "feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "feature = tfidf_transformer.transform(feature_p_doc)\n",
    "\n",
    "features = features_by_type[resume_type]\n",
    "\n",
    "result = cosine_similarity(feature, features)\n",
    "res = heapq.nlargest(10, range(len(result[0])), result[0].take)\n",
    "for i in res:\n",
    "    print result[0][i]\n",
    "    print data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9924\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "9924\n",
      "0.583040144288\n",
      "0.518259144701\n",
      "0.448771156518\n",
      "0.435977176192\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "doc = \"java разработчик программист api rrreree\"\n",
    "X = count_vectorizer.transform([doc])\n",
    "X1 = tfidf_transformer.transform(X)\n",
    "print len(X.toarray()[0])\n",
    "for v in X.toarray()[0]:\n",
    "    if v != 0:\n",
    "        print v\n",
    "\n",
    "print len(X1.toarray()[0])\n",
    "for v in X1.toarray()[0]:\n",
    "    if v != 0:\n",
    "        print v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " любл готов вредн привычек не им аккуратн  повар  поварск дел\n",
      "processed 1000 vаcancies\n",
      "processed 2000 vаcancies\n",
      "processed 3000 vаcancies\n",
      "processed 4000 vаcancies\n",
      "processed 5000 vаcancies\n",
      "processed 6000 vаcancies\n",
      "processed 7000 vаcancies\n",
      "processed 8000 vаcancies\n",
      "processed 9000 vаcancies\n",
      "processed 10000 vаcancies\n",
      "0.369745464628\n",
      "17136190\n",
      "0.352538100623\n",
      "16764555\n",
      "0.28401864007\n",
      "17135847\n",
      "0.28179223153\n",
      "17135823\n",
      "0.2696830206\n",
      "17137164\n",
      "0.265194214145\n",
      "17134022\n",
      "0.239828494588\n",
      "17135165\n",
      "0.229601478081\n",
      "17136834\n",
      "0.220165661334\n",
      "17136978\n",
      "0.204926291534\n",
      "17135840\n",
      "0.181030884969\n",
      "16827698\n",
      "0.174920378348\n",
      "16026656\n",
      "0.166019536197\n",
      "16710006\n",
      "0.159519643985\n",
      "17134738\n",
      "0.157873346428\n",
      "16722702\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "def get_my_resume():\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    headers = {\"User-Agent\": \"hh-recommender\", \"Authorization\" : \"Bearer T5MIT6GVV85LSVR75CB7U768TR3PFGS990I3QJNFV6A4CBQJF6M30G0MOT8U2V8I\"}\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/resumes/mine\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "    me = r1.read()\n",
    "    me_json = json.loads(me)\n",
    "\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "#     conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(me_json['items'][0]['id']), headers=headers)\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(\"5bdc936300031347c20039ed1f644868457044\"), headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "    resume = r1.read()\n",
    "    resume_json = json.loads(resume)\n",
    "\n",
    "    feature = []\n",
    "    resume_type = 'full'\n",
    "\n",
    "    resume_type = resume_json['employment']['id']\n",
    "\n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "\n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    res_skills = resume_json['skill_set']\n",
    "    for skill in res_skills:\n",
    "        words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "                \n",
    "    #salary\n",
    "    salary = None\n",
    "    if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "        salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "    max_salary = 500000.0\n",
    "    if salary >= max_salary:\n",
    "        salary = max_salary\n",
    "\n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    print p_doc\n",
    "    feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "    feature = tfidf_transformer.transform(feature_p_doc)\n",
    "    return feature.toarray(), salary\n",
    "\n",
    "\n",
    "def get_vacancies(offset, rows):\n",
    "    features = []\n",
    "\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    #будет задвоение, когда во время выборки в несколько запросом добавляются новые данные\n",
    "    cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 5 DAY) LIMIT {}, {}\"\"\".format(offset, rows))\n",
    "    vacancy_ids = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    for item in cursor:\n",
    "        feature = []\n",
    "        vacancy = json.loads(item[0])\n",
    "        vacancy_ids.append(vacancy['id'])\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        salaries.append(salary)\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "        \n",
    "        cnt = 0\n",
    "        for f in tfidf_feature_p_doc.toarray()[0]:\n",
    "            if f > 0:\n",
    "                cnt = cnt + 1\n",
    "            \n",
    "        if cnt > 1:\n",
    "            features.append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "    cursor.close()\n",
    "    return features, vacancy_ids, salaries\n",
    "\n",
    "def get_recommended(resume_feature, vacancy_features, resume_salary, vacancy_salaries, vacancy_ids):\n",
    "    new_vacancy_features = []\n",
    "    new_vacancy_ids = []\n",
    "    if resume_salary == None:\n",
    "        new_vacancy_features = vacancy_features\n",
    "        new_vacancy_ids = vacancy_ids\n",
    "    else:\n",
    "        i = 0\n",
    "        for vac_salary in vacancy_salaries:\n",
    "            if vac_salary == None:\n",
    "                new_vacancy_features.append(vacancy_features[i])\n",
    "                new_vacancy_ids.append(vacancy_ids[i])\n",
    "            else:\n",
    "                min_resume_salary = resume_salary - (resume_salary * 0.2)\n",
    "                max_resume_salary = resume_salary + (resume_salary * 0.8)\n",
    "                if vac_salary >= min_resume_salary and vac_salary <= max_resume_salary:\n",
    "                    new_vacancy_features.append(vacancy_features[i])\n",
    "                    new_vacancy_ids.append(vacancy_ids[i])\n",
    "                \n",
    "            i = i+1    \n",
    "    \n",
    "    if len(new_vacancy_features) > 0:\n",
    "        c_result = cosine_similarity(resume_feature, new_vacancy_features)\n",
    "        res = heapq.nlargest(15, range(len(c_result[0])), c_result[0].take)\n",
    "\n",
    "        similarities = []\n",
    "        ids = []\n",
    "        for j in res:\n",
    "            similarities.append(c_result[0][j])\n",
    "            ids.append(new_vacancy_ids[j])\n",
    "    return similarities, ids\n",
    "\n",
    "feature, salary = get_my_resume()\n",
    "\n",
    "count = 1000\n",
    "similarities = []\n",
    "ids = []\n",
    "features = get_vacancies(0, count)\n",
    "features, vacancy_ids, salaries = get_vacancies(0, count)\n",
    "\n",
    "f_len = len(features)\n",
    "\n",
    "r_similarities, r_ids = get_recommended(feature, features, salary, salaries, vacancy_ids)\n",
    "similarities = similarities + r_similarities\n",
    "ids = ids + r_ids\n",
    "\n",
    "i = 0\n",
    "while f_len > 0:\n",
    "    features, vacancy_ids, salaries = get_vacancies(i*count, count)\n",
    "    f_len = len(features)\n",
    "    if f_len > 0:\n",
    "        r_similarities, r_ids = get_recommended(feature, features, salary, salaries, vacancy_ids)\n",
    "        similarities = similarities + r_similarities\n",
    "        ids = ids + r_ids\n",
    "    \n",
    "    i = i+1\n",
    "    print 'processed {} vаcancies'.format(i*count)\n",
    "        \n",
    "    if i == 10:\n",
    "        break\n",
    "        \n",
    "max_similarities = heapq.nlargest(15, range(len(numpy.asarray(similarities))), numpy.asarray(similarities).take)\n",
    "for ind in max_similarities:\n",
    "    print similarities[ind]\n",
    "    print ids[ind]\n",
    "    \n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-01 17:13:25.445153\n",
      "processed 1000 vаcancies\n",
      "processed 2000 vаcancies\n",
      "processed 3000 vаcancies\n",
      "processed 4000 vаcancies\n",
      "processed 5000 vаcancies\n",
      "processed 6000 vаcancies\n",
      "processed 7000 vаcancies\n",
      "processed 8000 vаcancies\n",
      "processed 9000 vаcancies\n",
      "processed 10000 vаcancies\n",
      "processed 11000 vаcancies\n",
      "processed 12000 vаcancies\n",
      "processed 13000 vаcancies\n",
      "processed 14000 vаcancies\n",
      "processed 15000 vаcancies\n",
      "processed 16000 vаcancies\n",
      "processed 17000 vаcancies\n",
      "processed 18000 vаcancies\n",
      "processed 19000 vаcancies\n",
      "processed 20000 vаcancies\n",
      "c13f3c94ff023cf7d20039ed1f30766857326a\n",
      "for 17185733 similarity is 0.241420428222\n",
      "for 16858436 similarity is 0.231747031736\n",
      "for 15685222 similarity is 0.217491396879\n",
      "for 17098616 similarity is 0.213119123972\n",
      "for 16825405 similarity is 0.209957690914\n",
      "for 17197588 similarity is 0.201732166924\n",
      "for 17193832 similarity is 0.197981865485\n",
      "for 16165915 similarity is 0.195952985719\n",
      "for 16716632 similarity is 0.193102134053\n",
      "for 17190074 similarity is 0.190184213484\n",
      "for 17188217 similarity is 0.189807166359\n",
      "for 17190141 similarity is 0.183846894776\n",
      "for 15045209 similarity is 0.182566398696\n",
      "for 17124148 similarity is 0.179818133491\n",
      "for 15045200 similarity is 0.177550143234\n",
      "for 17182223 similarity is 0.17651011305\n",
      "for 17182223 similarity is 0.17651011305\n",
      "for 15431158 similarity is 0.170411555353\n",
      "for 16679291 similarity is 0.161486448948\n",
      "for 16711861 similarity is 0.151652867442\n",
      "4527975dff02f59d5f0039ed1f534356744c6e\n",
      "for 17190592 similarity is 0.32853290184\n",
      "for 16803389 similarity is 0.308989905585\n",
      "for 15685222 similarity is 0.297546103535\n",
      "for 17182094 similarity is 0.288522303422\n",
      "for 17182094 similarity is 0.288522303422\n",
      "for 17014281 similarity is 0.277741825232\n",
      "for 17185733 similarity is 0.27079151884\n",
      "for 17190074 similarity is 0.244443850617\n",
      "for 17182193 similarity is 0.243885821073\n",
      "for 17182193 similarity is 0.243885821073\n",
      "for 16825405 similarity is 0.243197307423\n",
      "for 16585330 similarity is 0.239942405205\n",
      "for 17182830 similarity is 0.225159026085\n",
      "for 17182830 similarity is 0.225159026085\n",
      "for 17183677 similarity is 0.224284520785\n",
      "for 16858436 similarity is 0.218734527172\n",
      "for 16683078 similarity is 0.216280094867\n",
      "for 17183052 similarity is 0.207465712192\n",
      "for 16165915 similarity is 0.198576857039\n",
      "for 17187549 similarity is 0.1932426447\n",
      "848f3de0ff000c40da0039ed1f736563726574\n",
      "for 17185733 similarity is 0.261828800225\n",
      "for 16858436 similarity is 0.242504778706\n",
      "for 17098616 similarity is 0.228706829185\n",
      "for 15685222 similarity is 0.227540897071\n",
      "for 16165915 similarity is 0.222043158323\n",
      "for 16825405 similarity is 0.22128041777\n",
      "for 17190141 similarity is 0.200561978596\n",
      "for 17190074 similarity is 0.198996694012\n",
      "for 17197588 similarity is 0.198967574545\n",
      "for 17188217 similarity is 0.197002169865\n",
      "for 17193832 similarity is 0.195268668255\n",
      "for 16716632 similarity is 0.194427885552\n",
      "for 17124148 similarity is 0.191434033214\n",
      "for 15431158 similarity is 0.183046982862\n",
      "for 15045209 similarity is 0.180064459207\n",
      "for 16711861 similarity is 0.178968946636\n",
      "for 15045200 similarity is 0.175116947872\n",
      "for 17182223 similarity is 0.174091170544\n",
      "for 17182223 similarity is 0.174091170544\n",
      "for 17193643 similarity is 0.171526618719\n",
      "04589be6ff01f2fef60039ed1f56534e424e42\n",
      "for 17196267 similarity is 0.347377411128\n",
      "for 17183846 similarity is 0.31507728119\n",
      "for 17194918 similarity is 0.259246490004\n",
      "for 17198776 similarity is 0.249693840602\n",
      "for 17196960 similarity is 0.193201172172\n",
      "for 16476271 similarity is 0.171531032001\n",
      "for 17052776 similarity is 0.168706725035\n",
      "for 17188895 similarity is 0.15081751129\n",
      "for 17188748 similarity is 0.14584607867\n",
      "for 17184427 similarity is 0.141031629954\n",
      "for 17192323 similarity is 0.140576082179\n",
      "for 17182928 similarity is 0.125456917786\n",
      "for 17193953 similarity is 0.124317609046\n",
      "for 17198486 similarity is 0.122805150635\n",
      "for 17182648 similarity is 0.121051896874\n",
      "for 17182648 similarity is 0.121051896874\n",
      "for 17182688 similarity is 0.117621283548\n",
      "for 17182688 similarity is 0.117621283548\n",
      "for 17192040 similarity is 0.112092600228\n",
      "for 16583323 similarity is 0.110703442662\n",
      "a588f755ff02418ca00039ed1f5a75387a3955\n",
      "for 17181878 similarity is 0.0\n",
      "for 17181879 similarity is 0.0\n",
      "for 17181881 similarity is 0.0\n",
      "for 17181892 similarity is 0.0\n",
      "for 17181894 similarity is 0.0\n",
      "for 17181908 similarity is 0.0\n",
      "for 17181911 similarity is 0.0\n",
      "for 16941968 similarity is 0.0\n",
      "for 17181916 similarity is 0.0\n",
      "for 17181924 similarity is 0.0\n",
      "for 17181925 similarity is 0.0\n",
      "for 17181938 similarity is 0.0\n",
      "for 17181944 similarity is 0.0\n",
      "for 17181949 similarity is 0.0\n",
      "for 17181961 similarity is 0.0\n",
      "for 17181967 similarity is 0.0\n",
      "for 17181972 similarity is 0.0\n",
      "for 14690439 similarity is 0.0\n",
      "for 17181974 similarity is 0.0\n",
      "for 17181980 similarity is 0.0\n",
      "d7c90e2eff02418ca50039ed1f356b4e504642\n",
      "for 16977650 similarity is 0.328960023028\n",
      "for 17191425 similarity is 0.303407733652\n",
      "for 16895056 similarity is 0.295275719024\n",
      "for 17187260 similarity is 0.252307966964\n",
      "for 17182323 similarity is 0.241233208258\n",
      "for 17182323 similarity is 0.241233208258\n",
      "for 17192228 similarity is 0.239448984579\n",
      "for 16873362 similarity is 0.232595859938\n",
      "for 17192930 similarity is 0.218048261794\n",
      "for 16898603 similarity is 0.209261470205\n",
      "for 17190491 similarity is 0.194549025145\n",
      "for 17183024 similarity is 0.184081680497\n",
      "for 17187244 similarity is 0.162514360595\n",
      "for 17066755 similarity is 0.160708895232\n",
      "for 15065374 similarity is 0.155506579317\n",
      "for 15065375 similarity is 0.155506579317\n",
      "for 16432831 similarity is 0.154820607105\n",
      "for 17190667 similarity is 0.154806479736\n",
      "for 16976331 similarity is 0.153476546131\n",
      "for 17189731 similarity is 0.15340405254\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      "for 15431158 similarity is 0.362254145256\n",
      "for 17124148 similarity is 0.29410149051\n",
      "for 15685222 similarity is 0.289767363695\n",
      "for 17182094 similarity is 0.271701788903\n",
      "for 17182094 similarity is 0.271701788903\n",
      "for 17182830 similarity is 0.258590119731\n",
      "for 17182830 similarity is 0.258590119731\n",
      "for 16825405 similarity is 0.255158467325\n",
      "for 16165915 similarity is 0.246060346584\n",
      "for 17193561 similarity is 0.244257352046\n",
      "for 17185733 similarity is 0.233767330032\n",
      "for 16803389 similarity is 0.224634473624\n",
      "for 16679291 similarity is 0.218911853448\n",
      "for 17182193 similarity is 0.211796680403\n",
      "for 17182193 similarity is 0.211796680403\n",
      "for 14121053 similarity is 0.210809709834\n",
      "for 17182643 similarity is 0.210268995969\n",
      "for 17182643 similarity is 0.210268995969\n",
      "for 17190592 similarity is 0.209984133237\n",
      "for 17183052 similarity is 0.197427628001\n",
      "total time 117.014307022 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "start_time = time.time()\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "    \n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "def get_resumes():\n",
    "    salaries = []\n",
    "    features = []\n",
    "    ids = []\n",
    "    areas = []\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(\"\"\"SELECT item FROM resumes WHERE is_active=1\"\"\")\n",
    "    for item in cursor:\n",
    "        resume_json = json.loads(item[0])\n",
    "        feature = []\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        if resume_json['skills'] != None:\n",
    "            doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "            doc = re.sub('&quot;', '', doc)\n",
    "            doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', doc.strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word.strip())\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        if resume_json['title'] != None:\n",
    "            title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', title.strip())\n",
    "            for title_word in words:\n",
    "                title_word = stemmer.stemWord(title_word)\n",
    "                if len(title_word.strip()) > 1:\n",
    "                    p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        res_skills = resume_json['skill_set']\n",
    "        for skill in res_skills:\n",
    "            words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "\n",
    "        #salary\n",
    "        salary = None\n",
    "        if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "            salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        \n",
    "        \n",
    "        res_areas = []\n",
    "        if resume_json['area'] == None:\n",
    "            res_areas.append(areas_map[\"1\"])\n",
    "        else :\n",
    "            res_areas.append(areas_map[resume_json['area']['id']])\n",
    "        for area in resume_json['relocation']['area']:\n",
    "            res_areas.append(areas_map[area['id']])\n",
    "        areas.append(res_areas)\n",
    "        \n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        feature = tfidf_transformer.transform(feature_p_doc)\n",
    "        features.append(feature.toarray())\n",
    "        salaries.append(salary)\n",
    "        ids.append(resume_json['id'])\n",
    "    cursor.close()\n",
    "    return features, salaries, ids, areas\n",
    "\n",
    "\n",
    "def get_vacancies(offset, rows):\n",
    "    features = []\n",
    "\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    #будет задвоение, когда во время выборки в несколько запросом добавляются новые данные\n",
    "    cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT {}, {}\"\"\".format(offset, rows))\n",
    "    vacancy_ids = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    titles = []\n",
    "    areas = []\n",
    "    for item in cursor:\n",
    "        feature = []\n",
    "        vacancy = json.loads(item[0])\n",
    "        vacancy_ids.append(vacancy['id'])\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "                \n",
    "        titles.append(vacancy['name'])\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        salaries.append(salary)\n",
    "        try:\n",
    "            areas.append(areas_map[vacancy['area']['id']])\n",
    "        except KeyError:\n",
    "            print 'missed area id {} for vacancy {}'.format(vacancy['area']['id'], vacancy['id'])\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area_vacancy = r1.read()\n",
    "            missed_area_vacancy_json = json.loads(missed_area_vacancy)\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/areas/{}\"\n",
    "                         .format(missed_area_vacancy_json['area']['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area = r1.read()\n",
    "            missed_area_json = json.loads(missed_area)\n",
    "            areas_map[vacancy['area']['id']] = missed_area_json['parent_id']\n",
    "            areas.append(missed_area_json['parent_id'])\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "            \n",
    "        features.append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "    cursor.close()\n",
    "    return features, vacancy_ids, salaries, titles, areas\n",
    "\n",
    "def get_recommended(resume_feature, vacancy_features, resume_salary, vacancy_salaries, vacancy_ids, vac_titles, resume_areas, vacancies_area):\n",
    "    pre_vacancy_features = []\n",
    "    pre_vacancy_ids = []\n",
    "    pre_vac_titles = []\n",
    "    pre_vacancy_salaries = []\n",
    "    j = 0\n",
    "    for vac_area in vacancies_area:\n",
    "        if vac_area in resume_areas:\n",
    "            pre_vacancy_features.append(vacancy_features[j])\n",
    "            pre_vacancy_ids.append(vacancy_ids[j])\n",
    "            pre_vac_titles.append(vac_titles[j])\n",
    "            pre_vacancy_salaries.append(vacancy_salaries[j])\n",
    "        j = j+1\n",
    "        \n",
    "    new_vacancy_features = []\n",
    "    new_vacancy_ids = []\n",
    "    new_vac_titles = []\n",
    "    if resume_salary == None:\n",
    "        new_vacancy_features = pre_vacancy_features\n",
    "        new_vacancy_ids = pre_vacancy_ids\n",
    "        new_vac_titles = pre_vac_titles\n",
    "    else:\n",
    "        i = 0\n",
    "        for vac_salary in pre_vacancy_salaries:\n",
    "            if vac_salary == None:\n",
    "                new_vacancy_features.append(pre_vacancy_features[i])\n",
    "                new_vacancy_ids.append(pre_vacancy_ids[i])\n",
    "                new_vac_titles.append(pre_vac_titles[i])\n",
    "            else:\n",
    "                min_resume_salary = resume_salary - (resume_salary * 0.2)\n",
    "                max_resume_salary = resume_salary + (resume_salary * 0.8)\n",
    "                if vac_salary >= min_resume_salary and vac_salary <= max_resume_salary:\n",
    "                    new_vacancy_features.append(pre_vacancy_features[i])\n",
    "                    new_vacancy_ids.append(pre_vacancy_ids[i])\n",
    "                    new_vac_titles.append(pre_vac_titles[i])\n",
    "                \n",
    "            i = i+1    \n",
    "    \n",
    "    similarities = []\n",
    "    ids = []\n",
    "    titles = []\n",
    "    if len(new_vacancy_features) > 0:\n",
    "        c_result = cosine_similarity(resume_feature, new_vacancy_features)\n",
    "        res = heapq.nlargest(20, range(len(c_result[0])), c_result[0].take)\n",
    "        \n",
    "        for j in res:\n",
    "            similarities.append(c_result[0][j])\n",
    "            ids.append(new_vacancy_ids[j])\n",
    "            titles.append(new_vac_titles[j])\n",
    "    return similarities, ids, titles\n",
    "\n",
    "resume_features, resume_salaries, resume_ids, resume_areas = get_resumes()\n",
    "\n",
    "count = 1000\n",
    "features = get_vacancies(0, count)\n",
    "features, vacancy_ids, salaries, titles, vacancy_areas = get_vacancies(0, count)\n",
    "\n",
    "f_len = len(features)\n",
    "\n",
    "res_similarities = {}\n",
    "res_recommended_ids = {}\n",
    "res_recommended_titles = {}\n",
    "for idx, val in enumerate(resume_features):  \n",
    "    r_similarities, r_ids, r_titles = get_recommended(resume_features[idx], features, resume_salaries[idx], salaries, vacancy_ids, \n",
    "                                                      titles, resume_areas[idx], vacancy_areas)\n",
    "    res_similarities[resume_ids[idx]] = r_similarities\n",
    "    res_recommended_ids[resume_ids[idx]] = r_ids\n",
    "    res_recommended_titles[resume_ids[idx]] = r_titles\n",
    "\n",
    "i = 0\n",
    "while f_len > 0:\n",
    "    features, vacancy_ids, salaries, titles, vacancy_areas = get_vacancies(i*count, count)\n",
    "    f_len = len(features)\n",
    "    if f_len > 0:\n",
    "        for idx, val in enumerate(resume_features):\n",
    "            r_similarities, r_ids, r_titles = get_recommended(resume_features[idx], features, resume_salaries[idx], salaries, vacancy_ids, \n",
    "                                                              titles, resume_areas[idx], vacancy_areas)\n",
    "            res_similarities[resume_ids[idx]] = res_similarities[resume_ids[idx]] + r_similarities\n",
    "            res_recommended_ids[resume_ids[idx]] = res_recommended_ids[resume_ids[idx]] + r_ids\n",
    "            res_recommended_titles[resume_ids[idx]] = res_recommended_titles[resume_ids[idx]] + r_titles\n",
    "            \n",
    "    i = i+1\n",
    "    print 'processed {} vаcancies'.format(i*count)\n",
    "        \n",
    "    if i == 20:\n",
    "        break\n",
    "\n",
    "for resume_id in res_similarities.keys():\n",
    "    print resume_id\n",
    "    similarities = res_similarities[resume_id]\n",
    "    ids = res_recommended_ids[resume_id]\n",
    "    titles = res_recommended_titles[resume_id]\n",
    "    max_similarities = heapq.nlargest(20, range(len(numpy.asarray(similarities))), numpy.asarray(similarities).take)\n",
    "    cursor = db.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"\"\"UPDATE recommendations SET is_active=0 WHERE resume_id='{}'\"\"\".format(resume_id))\n",
    "    except BaseException:\n",
    "        db.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "    for ind in max_similarities:\n",
    "        cursor = db.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"INSERT INTO recommendations (resume_id, vacancy_id, updated, is_active, similarity, vacancy_title) VALUES ('{}', {}, now(), 1, {}, '{}')\"\"\".format(resume_id, ids[ind], similarities[ind], titles[ind].encode('utf-8').strip()))\n",
    "        except BaseException:\n",
    "            db.rollback()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "        print 'for {} similarity is {}'.format(ids[ind], similarities[ind])\n",
    "    db.commit()\n",
    "        \n",
    "db.commit()\n",
    "db.close()\n",
    "\n",
    "print 'total time {} sec\\n'.format(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "r.set('foo', 'bar')\n",
    "r.get('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-06 16:25:07.609814\n",
      "loaded 2000\n",
      "loaded 4000\n",
      "loaded 6000\n",
      "loaded 8000\n",
      "loaded 10000\n",
      "missed area id 3309 for vacancy 15891681\n",
      "loaded 12000\n",
      "loaded 14000\n",
      "loaded 16000\n",
      "loaded 18000\n",
      "loaded 20000\n",
      "loaded 22000\n",
      "loaded 24000\n",
      "loaded 26000\n",
      "loaded 28000\n",
      "loaded 30000\n",
      "loaded 32000\n",
      "loaded 34000\n",
      "loaded 36000\n",
      "loaded 38000\n",
      "loaded 40000\n",
      "loaded 42000\n",
      "loaded 44000\n",
      "loaded 46000\n",
      "loaded 48000\n",
      "loaded 50000\n",
      "loaded 52000\n",
      "loaded 54000\n",
      "loaded 56000\n",
      "loaded 58000\n",
      "loaded 60000\n",
      "loaded 62000\n",
      "loaded 64000\n",
      "loaded 66000\n",
      "loaded 68000\n",
      "loaded 70000\n",
      "loaded 72000\n",
      "loaded 74000\n",
      "loaded 76000\n",
      "loaded 78000\n",
      "loaded 80000\n",
      "loaded 82000\n",
      "loaded 84000\n",
      "loaded 86000\n",
      "loaded 88000\n",
      "loaded 90000\n",
      "loaded 92000\n",
      "loaded 94000\n",
      "loaded 96000\n",
      "loaded 98000\n",
      "loaded 100000\n",
      "loaded 102000\n",
      "loaded 104000\n",
      "loaded 106000\n",
      "loaded 108000\n",
      "loaded 110000\n",
      "loaded 112000\n",
      "loaded 114000\n",
      "loaded 116000\n",
      "loaded 118000\n",
      "loaded 120000\n",
      "loaded 122000\n",
      "loaded 124000\n",
      "loaded 126000\n",
      "loaded 128000\n",
      "loaded 130000\n",
      "loaded 132000\n",
      "loaded 134000\n",
      "loaded 136000\n",
      "loaded 138000\n",
      "loaded 140000\n",
      "loaded 142000\n",
      "loaded 144000\n",
      "loaded 146000\n",
      "loaded 148000\n",
      "loaded 150000\n",
      "loaded 152000\n",
      "finish  at 34.5748804132 min\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# save all vacancies into redis\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "import time\n",
    "import datetime\n",
    "import redis\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "start_time = time.time()\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "    \n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "def get_vacancies(offset, rows):\n",
    "    features = []\n",
    "\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    #будет задвоение, когда во время выборки в несколько запросом добавляются новые данные\n",
    "    cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT {}, {}\"\"\".format(offset, rows))\n",
    "    vacancy_ids = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    titles = []\n",
    "    areas = []\n",
    "    for item in cursor:\n",
    "        feature = []\n",
    "        vacancy = json.loads(item[0])\n",
    "        vacancy_ids.append(vacancy['id'])\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "                \n",
    "        titles.append(vacancy['name'])\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        salaries.append(salary)\n",
    "        try:\n",
    "            areas.append(areas_map[vacancy['area']['id']])\n",
    "        except KeyError:\n",
    "            print 'missed area id {} for vacancy {}'.format(vacancy['area']['id'], vacancy['id'])\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area_vacancy = r1.read()\n",
    "            missed_area_vacancy_json = json.loads(missed_area_vacancy)\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/areas/{}\"\n",
    "                         .format(missed_area_vacancy_json['area']['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area = r1.read()\n",
    "            missed_area_json = json.loads(missed_area)\n",
    "            areas_map[vacancy['area']['id']] = missed_area_json['parent_id']\n",
    "            areas.append(missed_area_json['parent_id'])\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "            \n",
    "        features.append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "    cursor.close()\n",
    "    return features, vacancy_ids, salaries, titles, areas\n",
    "\n",
    "timeout = 6*24*60*60\n",
    "vac_cnt = 2000\n",
    "\n",
    "features, vacancy_ids, salaries, titles, areas = get_vacancies(0, vac_cnt)\n",
    "cnt = len(features)\n",
    "for idx, val in enumerate(features): \n",
    "    data = {}\n",
    "    data['features'] = json.dumps(features[idx].tolist()).encode(\"zlib\")\n",
    "    data['salary'] = salaries[idx]\n",
    "    data['area'] = areas[idx]\n",
    "    r.hmset(vacancy_ids[idx], data)\n",
    "    r.expire(vacancy_ids[idx], timeout)\n",
    "    \n",
    "i = 0\n",
    "while cnt > 0:\n",
    "    features, vacancy_ids, salaries, titles, areas = get_vacancies(i*vac_cnt, vac_cnt)\n",
    "    cnt = len(features)\n",
    "    for idx, val in enumerate(features): \n",
    "        data = {}\n",
    "        data['features'] = json.dumps(features[idx].tolist()).encode(\"zlib\")\n",
    "        data['salary'] = salaries[idx]\n",
    "        data['area'] = areas[idx]\n",
    "        r.hmset(vacancy_ids[idx], data)\n",
    "        r.expire(vacancy_ids[idx], timeout)\n",
    "    print 'loaded {}'.format(i*vac_cnt+vac_cnt)\n",
    "    i = i+1\n",
    "\n",
    "    \n",
    "print \"finish  at {} min\".format((time.time()-start_time)/60.0)\n",
    "\n",
    "#print data['features'].decode('zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-21 18:51:32.485513\n",
      "/vacancies?per_page=200&date_from=2016-06-21T18:46:34&date_to=2016-06-21T18:51:34&page=0\n",
      "80\n",
      "starting t1\n",
      "20.9694349766 sec\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#vacancy downloader 2\n",
    "import httplib\n",
    "import json\n",
    "import MySQLdb\n",
    "import ConfigParser\n",
    "import time\n",
    "import threading\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import heapq\n",
    "import numpy\n",
    "import re \n",
    "import Stemmer\n",
    "import datetime\n",
    "import redis\n",
    "\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "start_time = time.time()\n",
    "timeout = 5*24*60*60\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "conn.close()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "    \n",
    "with open( \"count_vectorizer.p\", \"rb\" ) as f:\n",
    "    count_vectorizer = pickle.load(f)\n",
    "    \n",
    "with open( \"tfidf_transformer.p\", \"rb\" ) as f:\n",
    "    tfidf_transformer = pickle.load(f)\n",
    "    \n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "    \n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "cursor.close()\n",
    "\n",
    "def get_vacancy_ids():\n",
    "    vacancy_ids = []\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    per_page = 200\n",
    "    page = 0\n",
    "    count = per_page\n",
    "    date_from = (datetime.datetime.now() - datetime.timedelta(minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    date_to = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    while count == per_page:\n",
    "        path = (\"/vacancies?per_page={}&date_from={}&date_to={}&page={}\"\n",
    "                .format(per_page, date_from, date_to, page))\n",
    "        print path\n",
    "\n",
    "        conn.request(\"GET\", path, headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        vacancies = r1.read()\n",
    "        conn.close()\n",
    "\n",
    "        count = len(json.loads(vacancies)['items'])\n",
    "        page = page+1\n",
    "        for item in json.loads(vacancies)['items']:\n",
    "            vacancy_ids.append(item['id'])\n",
    "    return vacancy_ids\n",
    "        \n",
    "\n",
    "def process_vacancies(vacancy_ids):\n",
    "    headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "    for vac_id in vacancy_ids:\n",
    "        conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        if r1 != 200:\n",
    "            conn.close()\n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "        vacancy_txt = r1.read()\n",
    "        conn.close()\n",
    "        vacancy = json.loads(vacancy_txt)\n",
    "        \n",
    "        feature = []\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "\n",
    "        try:\n",
    "            area_id = areas_map[vacancy['area']['id']]\n",
    "        except KeyError:\n",
    "            print 'missed area id {} for vacancy {}'.format(vacancy['area']['id'], vacancy['id'])\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            if r1 != 200:\n",
    "                conn.close()\n",
    "                conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "                conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "                r1 = conn.getresponse()\n",
    "            missed_area_vacancy = r1.read()\n",
    "            conn.close()\n",
    "            missed_area_vacancy_json = json.loads(missed_area_vacancy)\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/areas/{}\"\n",
    "                         .format(missed_area_vacancy_json['area']['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area = r1.read()\n",
    "            conn.close()\n",
    "            missed_area_json = json.loads(missed_area)\n",
    "            areas_map[vacancy['area']['id']] = missed_area_json['parent_id']\n",
    "            area_id = missed_area_json['parent_id']\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "            \n",
    "        data = {}\n",
    "        data['features'] = json.dumps(tfidf_feature_p_doc.toarray()[0].tolist()).encode(\"zlib\")\n",
    "        data['salary'] = salary\n",
    "        data['area'] = area_id\n",
    "        r.hmset(vacancy['id'], data)\n",
    "        r.expire(vacancy['id'], timeout)\n",
    "\n",
    "\n",
    "ids = get_vacancy_ids()\n",
    "print len(ids)\n",
    "vac_id_chunks=[ids[x:x+100] for x in xrange(0, len(ids), 100)]\n",
    "t_num = 1;\n",
    "threads = []\n",
    "for vac_id_chunk in vac_id_chunks:\n",
    "    print 'starting t{}'.format(t_num)\n",
    "    t_num = t_num + 1\n",
    "    t = threading.Thread(target=process_vacancies, kwargs={'vacancy_ids': vac_id_chunk})\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "\n",
    "db.close()\n",
    "print \"{} sec\".format(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-21 20:27:33.750931\n",
      " водительск удостоверен категор bc налич личн знан английск язык базов уровен координац сотрудник сервисн отдел веден учет рабоч времен координац сотрудник отдел планирован работ отдел заключен договор заказчик поиск клиент веден отчетн документац работ дебиторк составлен коммерческ предложен участ проектирован объектов; координац монтажн бригад руководител бригад на объектах; изучен нов технолог асу, участ семинар выставках; экспертн сервисн поддержк объектов: диагностик ремонт бд сист видеонаблюден опс; отладк по представител компаний; при обращен от обслужива объектов; разработк инструкц для сервисн специалист компании;\n",
      " vs language vb net before have deal with delphi solid knowledge in web technology asp net html css javascript ajax experience at asp net controls library like devexpress telerik knowledge sql server, linq, typed datasets, ado.net, basic knowledge of orm winforms. architecture knowledge multilayer, soa, mvc. knowledge on web-services, wcf, wwf, now work on product which use k2 tools for workflow processing. patterns, refactoring, expirienced with nunit, code covering have deal with some 3d party products, like devexpress, telerik controls, document management system, workflow process system, nhibernate. some tools resharper, nunit, fxcop, svn, bug-tracking system but now using just tfs everywhere. responsible for developing web winforms products for uk insurance industry using vb net also my job involves writing documentation negotiation with uk colleages have experience at business trip to uk have deal with bug fixing, support, testing as well. currently, i'm taking part at winform vb.net project 2.0, using 3d party document management system, workfow, wsto. before took part in several web projects(c#. wcf, linq, typed datasets, ajax, etc)\n",
      " работ ос windows linux уверен знан html css jquery php работ git знан английск язык на высок уровн разработк сопровожден сайт\n",
      " подготовк абитуриент для сдач егэ ги отзыв работ можн найт по ссылк http smr repetitors info comments php isachenkomv\n",
      " для подтвержден навык гот выполн тестов задан имеет профил на github но там мал чег https github com vanya разработк на android увлека достаточн давн был наработк по двум личн проект мог отдельн дат описание) так же приним участ хакатон \"ребят из сибири\" (апрел 2015) разработа клиент-серверн приложен для команд (команд заня втор место) принима участ soft-парад 2013 (3d сред для визуализац химическ физическ явлений). побед номинац \"перспективн проекты\" нттм 2013. разработк систем смс авторизац для доступ wifi точк разработк систем интеграц баннерн реклам веб страниц разработк систем интеграц информацион мен заведен веб страниц разработк систем администрирован отображен статистик для вышеупомянут сист\n",
      " разработк тестирован по\n",
      " мног всег сдела\n",
      " the organization of the department work setting objectives work progress monitoring segmentation prospective customer portfolio in terms of risk and credit load identifying promising new customer segments analysis of the impact of crm-activities and changes in credit risk procedures on portfolio performance. development of proposals to control risks in the crm-active. preparation of conclusions on the effects on the risk of new credit products treatments. development of proposals to change the lending procedures. approval of changes of credit procedures of new products, lending new client segments. development and approval of test campaigns promising portfolio assessment of their results. prepare regular reports in sections of the prospective portfolio risk indicators, changing the risk profile of the portfolio perspective. working with pl/sql, sas (base, guide). achievements: failure to develop additional rules by which reduced the level of outstanding debt for new disbursements and increased profitability by product; add customer portfolio segmentation process and to determine the client's credit load for better conduct cross-sale companies; automated procedure for forecasting losses on products; automated reporting signal of risk indicators;\n",
      " любл сложн нестандартн задач дума что мо оп решен нетипичн задач поможет улучшен инфраструктур компан достижен оптимизац скорост работ текущ веб приложен создан отдел разработк веб решен создан нов сайт мар на битрикс организац хостинг для всех сайт компан внедрен методолог kanban отдел организац работ веб отдел (найм адаптац персонала, разработк систем мотивации, регламентац взаимодейств заказчиками) техническ оптимизац текущ веб-проект разработк архитектур интеграцион ит-проект компании, выбор технолог реализаций, проектн управлен\n",
      " установк на рабоч станц операцион сист ос установк офисн программн обеспечен установк стандартн набор программн обеспечен по установк необходим ин лицензион программн обеспечен установк обновлен настройк антивирусн программн обеспечен включ агент администрирован установк настройк программн обеспечен корпоративн сет на рабоч станциях. обновлен (по) по заявк пользователей. поддержк работоспособн состоян программн обеспечен рабоч станций. техническ поддержк пользователей, консультац пользовател по вопрос работ ос, по, оргтехники, локальн сет по телефону. техническ поддержк пользовател проведен работ помощ по (rdp ин по). проведен профилактическ работ по поддержан работоспособн средств вычислительн техники. подключен зам внешн устройств, проведен тестирован средств вычислительн техники. перемещен подключен перемещен вычислительн техник существ сет при налич сетев кабелей. организац нов рабоч места. подключен вычислительн техник ин настроен заран сетев администратор сетев оборудован локальн компьютерн сет при налич доступн настроен сетев порт сетев кабелей. диагностика, тестирован мелк ремонт «на месте» пользовател отдельн устройств средств вычислительн техники, кабельн лин локальн компьютерн сет ил при необходим кабинет автоматизац проведен профилактическ работ. диагностик всех компонент вычислительн техники, написан акт диагностик оборудован указан неисправн компонент рекомендац частичн ил полн замен оборудования. передач акт инженер по автоматизации. посл приобретен оборудован ег установка, при необходим зам ос по, дальн передач пользователю. сборк персональн компьютеров. мониторинг работ диагностик компьютерн оборудован информацион систем цел своевремен выявлен неисправностей. веден техническ документац по выполнен заявок пользователей. своевремен выполнен заявок систем р3 согласн договор на обслуживание.\n",
      " готовк безалкогольн алкогольн коктейл оформлен витрин барн стойк содержан их порядк принят заказ от посетител\n",
      " оп работ программирован ос семейств linux windows знан cad сист solidworks компас altium disigner mathcad mathlab simulink язык программирован python микроконтроллер вся линейк stm fx битн avr ки ос реальн времени: freertos, chibios знан осн модел osi, протокол маршрутизац ospf,iergrp, настройк маршрутизатор cisco хобби: схемотехника, программирование, сноуборд, музыка. обязан разработк кодек\n",
      " оп работ программирован ос семейств linux windows офисн программ open office ms office знан cad сист solidworks компас altium disigner mathcad mathlab simulink язык программирован python javasсript jquery микроконтроллеры, вся линейк stm32fx, 8-битн avr'ки ос реальн времени: freertos, chibios знан осн модел osi, протокол маршрутизац ospf,iergrp, настройк маршрутизатор cisco хобби: схемотехника, программирование, сноуборд, музыка. разработк поддержк встраива по для устройств входя сист горн добыч угл отладк запуск по на прототип тестирован плат на работоспособн внедр процесс разработк контрол верс законч нескольк устройств программ для их тестирован всё для отдел эксплуатации)\n",
      " разработк поддержк внутрен по компан sql основн продукт компан агрегирова баз дан объявлен получен нескольк десятк источник информац выполня работ на всем цикл от сбор дан из свободн источников, актуализацию, автоматическ валидац агрегацию. написан представлений, скалярн функций, храним процедур. оптимизац выполнен запросов, анализ план выполнения, оптимизац индексов. объ баз дан десятк гигабайтов. сейчас компан баз регион россии.\n",
      " им профильн высш образован красн дипл бакалавр занима разработк собствен веб проект им оп работ со мног популярн cms drupal wordpress занима разработк на php delphi perl имеет оп работ бд postgres sql mysql. гот обучению. занима создан наполнен раскрутк собствен сайт использ основн cms drupal пример сайт http www armnotebooks com http readersworld ru\n",
      " личн предпочтен при кодирован при изменен чуж код стара не отступа от стил предыдущ программист вно нов код разворачива стара следова здоров логик ег для прост пониман пиш комментар редк основн оглавлении; -предпочита работа техническ задан предварительн обсуд его; не придержива ни одн из схем ил подход программированию, выбира её по текущ задач квалификац коллег. дан момент мо трудов обязан вход работ инструмент delphi xe seattle mysql php немн javascript использу объектн ориентирова процедурн программирован приход сталкива мног поточн учет специфик наш продукт придержива модульности, наскольк эт возможно. для хранен дан использу реляцион схем привод их трет форме.\n",
      " хобб страйкбол свободн врем занима том числ создан собствен заказн проект веб сист целеустремл ум работа команд как качеств одн из член команд так качеств тимлида. стрессоустойчивость, гибкость, неконфликтность, способн оперативн самостоятельн осваива нов платформ технологии, обучаем доработк сопровожден по для управлен систем телемеханик\n",
      " хобб страйкбол свободн врем занима том числ создан собствен заказн проект веб сист целеустремл ум работа команд как качеств одн из член команд так качеств тимлида. стрессоустойчивость, гибкость, неконфликтность, способн оперативн самостоятельн осваива нов платформ технологии, обучаем курирован всех вопрос сфер ит внедр мотивацион коеффициент работ сотрудник техническ поддержк запуст сист электрон документооборот университет курир разработк информацион сист как вид настольн приложен перв очеред ис угту, реализ учёт учебн деятельн университета), так веб приложен (личн кабинет студент преподавателя, электрон табель, электрон индивидуальн план преподавателя) консолидирова серверн инфраструктур отказоусточив кластер реализова сист защит персональн дан рамк выделя средств разработа запуст небольш сет сенсорн киоск для подготовк печат заявлен студент (windows store app django backend cups) настоя врем том числ курир разработк личн кабинет студент преподавател университет университетск соц сет использован дан из корпоративн ис\n",
      " хобб страйкбол свободн врем занима том числ создан собствен заказн проект веб сист целеустремл ум работа команд как качеств одн из член команд так качеств тимлида. стрессоустойчивость, гибкость, неконфликтность, способн оперативн самостоятельн осваива нов платформ технологии, обучаем курирован всех вопрос сфер ит внедр мотивацион коеффициент работ сотрудник техническ поддержк запуст сист электрон документооборот университет курир разработк информацион сист как вид настольн приложен перв очеред ис угту, реализ учёт учебн деятельн университета), так веб приложен (личн кабинет студент преподавателя, электрон табель, электрон индивидуальн план преподавателя) консолидирова серверн инфраструктур отказоусточив кластер реализова сист защит персональн дан рамк выделя средств разработа запуст небольш сет сенсорн киоск для подготовк печат заявлен студент (windows store app django backend cups) настоя врем том числ курир разработк личн кабинет студент преподавател университет университетск соц сет использован дан из корпоративн ис\n",
      " понима как должн быт постро автоматизирова систем управлен групп компан мог постро мог из разрознен источник получа информац стро постоя обновля отчет на основ мог стро схем бюджетирования, на их основ осуществля оперативн управлен казначейством. им огромн оп корпоративн кредитовании, как со сторон заемщик так со сторон банка. ярославл компан инвестир строительств многоквартирн дом коттеджн поселк осуществля подготовк защит инвестицион проект регион на инвестицион совет надзор за строительств объект приемк работ контрагент\n",
      " реализова проект ярославич вступлен мастеркард ofac выпуск карт модул карт создан продукт гис гмп модул вклад создан продуктов линейк открыт офис ip связ един модульн систем вступлен расчетн сист сбербанк лиценз фсб, организац информацион безопасн главярстрой: документ на строительств дом договор со строител систем продаж квартир премиум ремонт совкомбанк кредит мал бизнес по программ sgm прогнозирован превентивн изменен услов комитет по реструктуризац задолжен продаж закладн агенств по реструктуризац ипотечн кредит мдм банк открыт ярославл офис запуск кредитован вертикал создан розничн сет создан систем торговл предоставлен товарн кредит (не через банк). ярославл компан инвестир строительств многоквартирн дом коттеджн поселк осуществля подготовк защит инвестицион проект регион на инвестицион совет надзор за строительств объект приемк работ контрагент\n",
      " прям продаж проведен переговор финансов анализ юридическ лиц мсб проектн финансирован анализ риск расчет резерв участ судх взыскан проблемн задолжен формирован команд обучен сотрудник контрол уровн знан анализ потребн клиент конкурентн среды, создан на эт основ массов программ кредитования, внедрен программ кредитования, достижен планов показател выдач, портфеля, уровн просрочки. реализац проект област недвижим инвестиц строительств многоквартирн дом коттеджн поселк надзор за строительств приемк работ контрагент прям продаж объект подготовк защит инвестицион проект на инвестицион совет\n",
      " курир банк направлен ит банковск карт казначейств валютн отдел норматив обеспечива открыт нов офис заместител председател кредитн комитет банк привлека личн крупн клиент банк за год кредитн портфел увелич раза, депозитн портфел раз. реализова проект вступлен банк мастеркард. спроектирова запуст ит-инфраструктур для работ удален офисов. ввел банк стандартн банковск продукт (вклады, потребительск кредиты, банковск карты).\n",
      " исполнительн ответствен стара выполн раньш срок внимательн вчитава документац не конфликтн пунктуальн вредн привычек нет хобб нов технолог выполнен работ разработк мебел полн цикл от замер составлен дизайн до полн комплект чертеж учет фурнитур файл на станк чпу woodwop\n",
      " исполнительн ответствен стара выполн раньш срок внимательн вчитава документац не конфликтн пунктуальн вредн привычек нет хобб нов технолог выполнен работ разработк мебел полн цикл от замер составлен дизайн до полн комплект чертеж учет фурнитур файл на станк чпу woodwop\n",
      " влад язык английск pre intermediate владен пк различн по на уровн уверен пользовател знан язык программирован язык технолог net базов знан sql mssql знан unity оп разработк игр для мобильн устройств. технолог .net: windows forms, ado.net, devexpress, devex xaf(web). имеют знан photoshop, такж мног друг программ среды, не связа программированием. им водительск прав (категор b, c, водительск стаж 2007 года). дополнительн информация: исследователь, не теоретик, получа навык опыт, работ чем-т ответствен пунктуальный, предпочита дисциплинирова общество, жестк указа правил желан обуча исследова не кур адаптир приспосаблива разработк игр на движк unity\n",
      " налич водительск удостоверен категор налич уверен пользовател пк консультац клиент по услуг тариф компан создан заявок sd сбор информац по проблем проведен обучен для нов сотрудник очн удален проведен обучен для действ сотрудник работ над улучшен показател csi, врем разговора, контрольн лист), продукт.\n",
      " налич водительск удостоверен категор налич уверен пользовател пк мониторинг качеств разговор оператор кц прослушк онлайн офлайн заполнен оценочн форм анализ показател качеств обслуживан по кц выявлен потребн обучен сотрудник кц организац контрол работ групп эксперт процесс мониторинг разговоров, выдач обратн связи, корректировк оценочн форм;\n",
      " руководств ит подразделен компан создан комплектован ит подразделен управлен крупн ит проект создан поддержк ит инфраструктур построен работ удален филлиал компан выстраиван политик отношен внешн поставщик ит продукт услуг (outsourcing); -разработк по баз дан (постановк задач, программирован т.п.); -разработк учетн сист на платформ 1с предприят руководств ит отдел автоматизац производств внедрен сист управлен склад wms поддержк совершенствован erp систем iscala работ поставщик ит оборудован услуг веден переговор заключен договор внедрен систем документооборот доработк существ сист (платформ ms sql, .net)\n",
      " управлен крупн ит проект руководств подразделен компан создан поддержк ит инфраструктур создан комплектован ит подразделен построен работ удален филлиал компан выстраиван политик отношен внешн поставщик ит продукт услуг (outsourcing); -разработк по баз дан (постановк задач, программирован т.п.); -разработк учетн сист на платформ 1с:предприят 7.7. явля руководител проект по автоматизац учет сыр готов продукц учет на региональн предприят компан элеватор мукомольн завод сахарн савод консолидац отчетн центральн офис компан должностн обязан включа анализ бизнес процесс предприятий. разработк предложен по усовершенствован стандартизац учет на предприятиях. участ разработк методик веден учета, обследован экспресс диагностик существ систем учет на предприятии. разработк проект техническ задан на автоматизац учет на предприятиях. -проведен тендер сред поставщик сист автоматизац элеватор перерабатыва предприятий, разработк систем критер выбор наилучш программн комплекс для решен поставлен задачи. разработк заключен договор поставщик программн комплекса. согласован техническ задан на разработк ис подрядчиком, -составлен бюджет проект контрол за ег исполнением. -планирован этап работ, отслеживан срок выполнен проект (ms project). -контрол ход разработк информацион системы, внесен необходим изменен тз, предварительн тестирован компонент системы, -контрол работ по тестирован поэтапн (контурному) запуск готов систем промышлен эксплуатацию. на настоя момент основн результат работ возглавля мно групп можн отнести: -проведен модернизац (создан нуля, для некотор предприятий) ит инфраструктур 19 производствен предприят (входя групп «разгуляй») регион российск федерации. внедрен корпоративн ит стандарты. -выбра внедр на предприят автоматизирова систем учет сыр готов продукции. -на 10 предприят созда ит отделы, провед подбор обучен персонала.\n",
      " it manager project lead since december till present time am responsible for the project of developing it infrastructure in regional enterprises the project includes grain warehouses mills and sugar refineries which belong to the company the main goals of this project are reequipment of regional enterprises with new computers, network infrastructure deployment and software installation according to corporate it standards. also the project includes organization of it support, creation and implantation information system for grain and sugar accounting. my main responsibilities include the following: analysis of business process and suggest the methods of improvement it infrastructure conditions and software standardization on the enterprises; work with outsourcing companies; managing the project documentation. coordination of communication process inside the company. discussing and submitting the technical requirements to system suppliers. project planning and due date control (using ms project). budget planning and execution control. monitoring the processes of hardware installation and software system development, preliminary testing of software system. specific achievements include: the most suitable in relation of quality-price solutions supplier was chosen. successful reequipment of regional enterprises creation the new grain and sugar accounting systems and implementation at the enterprises. after the end of project my responsibilities include managing the it infrastructure and software support, recruitment and training it staff for regional enterprises. coordinating work of regional it departments (total 27 employees) and outsourcing companies.\n",
      " возможн все прост иногд эт может потребова больш времен денег ил эт может быт маг результат можн реализова ваш запрос быстр качествен дешев но вы может выбра тольк опц стрессоустойчив легкообучаемость, аналитическ склад ума, умен наход решен нестандартн ситуациях, умен наход подход людям, готовн работа команд ил же как независим игрок, готовн командировкам, включ длительн командировки, готовн переезду. крым явля приоритетн направлением. знак itil itsm процессами, зна как внедр использова их организации. интерес проекты. люб тип проектов: ит проекты, поиск решений, подготовк команд (включ подбор, найм тренинги), проработк бизнес решен внедрен бизнес идей. высок интерес работ стрессов экстремальн ситуациях: дедлайн вчера; все \"горит\"; эт невозможно; зде билет на самолет перв классом, задач необходим выполн течен 2-х дне 2000 километр отсюда. периодическ пиш программ для личн нужд (работ бд, работ сетью, обработк текста) на delphi (pascal), c++, visual basic (visual studio), vba (excel), php, perl. высок интерес нов задачам, необходим поиск решен ним. всегд откр для нов опыт нов предложений. ярославск техническ центр ятц цод па вымпелк больш модульн облачн дат центр ярославск регион проект стандарт tier основн зон ответствен сет включ сет техническ центр систем хранен сервер включ переконигурац обработк событ операцион системы. организац сборки, монтажа, коммутац маркировк активн ит оборудован соединений. обеспечен развит штатн функционирован ит-инфраструктуры. подготовк обучен персонала. руководств подразделением. itil: процесс управлен конфигурациями, процесс управлен изменениями, участ разработк внедрен мониторинга, участ разработк внедрен процесс управлен инцидентами, процесс упправлен проблемами. организац веден кроссов журналов. контрол склад зип. обеспечен своевремен предоставлен сервис доступн систем. разработк организац дежурн смены, подготовк дежурн администраторов.\n",
      " возможн все прост иногд эт может потребова больш времен денег ил эт может быт маг результат можн реализова ваш запрос быстр качествен дешев но вы может выбра тольк опц стрессоустойчив легкообучаемость, аналитическ склад ума, умен наход решен нестандартн ситуациях, умен наход подход людям, готовн работа команд ил же как независим игрок, готовн командировкам, включ длительн командировки, готовн переезду. крым явля приоритетн направлением. знак itil itsm процессами, зна как внедр использова их организации. интерес проекты. люб тип проектов: ит проекты, поиск решений, подготовк команд (включ подбор, найм тренинги), проработк бизнес решен внедрен бизнес идей. высок интерес работ стрессов экстремальн ситуациях: дедлайн вчера; все \"горит\"; эт невозможно; зде билет на самолет перв классом, задач необходим выполн течен 2-х дне 2000 километр отсюда. периодическ пиш программ для личн нужд (работ бд, работ сетью, обработк текста) на delphi (pascal), c++, visual basic (visual studio), vba (excel), php, perl. высок интерес нов задачам, необходим поиск решен ним. всегд откр для нов опыт нов предложений. ярославск техническ центр ятц цод па вымпелк больш модульн облачн дат центр ярославск регион проект стандарт tier основн зон ответствен сет включ сет техническ центр систем хранен сервер включ переконигурац обработк событ операцион системы. организац сборки, монтажа, коммутац маркировк активн ит оборудован соединений. обеспечен развит штатн функционирован ит-инфраструктуры. подготовк обучен персонала. руководств подразделением. itil: процесс управлен конфигурациями, процесс управлен изменениями, участ разработк внедрен мониторинга, участ разработк внедрен процесс управлен инцидентами, процесс упправлен проблемами. организац веден кроссов журналов. контрол склад зип. обеспечен своевремен предоставлен сервис доступн систем. разработк организац дежурн смены, подготовк дежурн администраторов.\n",
      " everything is possible but sometimes it can take longer time and it can be costful than planned or it can be magic anyway as result you can get your request performed fast, with high quality and cheap, but you can choose any of these options. am quick learner, have an analytical way of thinking, ability to find the decision in non-standard situations and find an approach to people. have leadership ability but also can be good teamplayer or independent player. am ready to go for business trips including long business trip(s), also am ready for the relocation. crimea (crimean destination) is my priority. know itil and itsm processes and know how to implement and use it. am basically interested in any types of projects: it projects, search for the solutions, team building (including hiring and also training team), business development and implementation. am highly motivated and ready to work in stressful situations or in extreme situations (example: deadline was yesterday; everything burns; this is not possible; this is your first class flight tickets and this task should be done in two days in two thousand kilometers far from here). occasionally develop applications for the personal usage (operations with database, with network and text processing) by the following instruments: delphi (pascal), visual studio (c++, visual basic), vba (excel), php, perl. free consultations by mail. always open for new experience, for new opportunities. yaroslavl technical center ytc dc pjsc vimpelcom large scale modular cloud data center in the yaroslavl region tier iii by project design areas of responsibility lan including lan of technical center san servers (including hardware maintenance and os events). assembly, installation, switching and marking of it equipment and patch-cables. providing staff development and normal operation of it infrastructure. team management. itil: configuration management, change management, participation in development and implementation of monitoring process, participation in development and implementation of incident management, problem management. development and implementation of cable journal, storage of spare parts. development and organization of duty shifts. training of duty administrators.\n",
      " базов знан jquery erlang lisp clojure racket scheme haskell ocaml обязан разработк поддержк проект business analyst дополнен для геоинформацион систем arcgis перевод част функциональн на доработк функциональн отч тов\n",
      " ключев навык построен корпоративн локальн сет сетев оборудован mikrotik qtech eltex cisco hp ос windows server linux ubuntu debian freebsd routeros разработк прикладн системн по ruby ruby on rails) java (andriod) c/c++ (arduino) 5. 1c:предприят 7.x, 8.x (установка, администрирование, сопровождение). 6. инфраструктур связи: postfix asterisk обязан разработк внедрен сист учет выпуск продукц сопряжен их упп руководств обслуживан информацион технолог компан поддержк усовершенствован лвс центральн офис удален структурн подразделен компан отдельн рабоч станций, сервер периферийн устройств определен согласован руководств перечн оборудован программн обеспечения, необходим достаточн для функционирован компьютерн сет поддержк рабоч состоян сервер организац планирован закупк информационно-техническ оборудования, перифер программн продуктов. организац работ по отладке, опытн эксплуатац поэтапн внедрен комплекс программн информационно-техническ средств участ работ по совершенствован документооборот информацион обм координац работ системн администратор программист 1с участ планирован бюджетирован деятельн отдел информацион технолог подконтрольн территории: распределен территориальн (до км) лвс г.вельск (оптоволокон канал связи, радиомосты) удален \"полевых\" филиа (пасьва, тегра, няндома) филиа г. москв (торгов дом, участок продаж) техника: серверная: физическ сервер (различн по комплектации, используем для сред виртуализац esxi) хранилищ (san/nas) сетевая: 17 основн коммутацион узл радиом 2.4, 5, 70 ггц 30 коммутатор l2 корнев l3 коммутатор (основн сет видео) инфраструктур корпоративн связи: почтовая: postfix/dovecot (мультидомен, филиальн структура) телефония: asterisk (организац взаимодейств gsm тфоп шлюзами, подключен нескольк sip провайдерам)\n",
      " построен корпоративн локальн сет сетев оборудован mikrotik qtech eltex cisco hp ос windows server linux ubuntu debian freebsd routeros разработк прикладн системн по ruby ruby on rails java (andriod) c/c++ (arduino) 5. 1c:предприят 7.x, 8.x (установка, администрирование, сопровождение). 6. инфраструктур связи: postfix asterisk разработк внедрен сист учет выпуск продукц сопряжен их упп руководств обслуживан информацион технолог компан поддержк усовершенствован лвс центральн офис удален структурн подразделен компан отдельн рабоч станций, сервер периферийн устройств определен согласован руководств перечн оборудован программн обеспечения, необходим достаточн для функционирован компьютерн сет поддержк рабоч состоян сервер организац планирован закупк информационно-техническ оборудования, перифер программн продуктов. организац работ по отладке, опытн эксплуатац поэтапн внедрен комплекс программн информационно-техническ средств участ работ по совершенствован документооборот информацион обм координац работ системн администратор программист 1с участ планирован бюджетирован деятельн отдел информацион технолог подконтрольн территории: распределен территориальн (до км) лвс г.вельск (оптоволокон канал связи, радиомосты) удален \"полевых\" филиа (пасьва, тегра, няндома) филиа г. москв (торгов дом, участок продаж) техника: серверная: физическ сервер (различн по комплектации, используем для сред виртуализац esxi) хранилищ (san/nas) сетевая: 17 основн коммутацион узл радиом 2.4, 5, 70 ггц 30 коммутатор l2 корнев l3 коммутатор (основн сет видео) инфраструктур корпоративн связи: почтовая: postfix/dovecot (мультидомен, филиальн структура) телефония: asterisk (организац взаимодейств gsm тфоп шлюзами, подключен нескольк sip провайдерам)\n",
      " хобб акустическ электрогитар разработк нов сервис поддержк действ сред них интернет магазин битрикс сайт каталог modx revo сервис раскладк керамическ плитк сервис уч та входя исходя звонк для колл центр\n",
      " оп разработк веб приложен работ команд net ms sql jquery asp net mvc angularjs разработк систем\n",
      " разработа мно проект http pda ru forum index php showtopic комплекс приложен для охра автомобил использован телефон на баз ос android так же для слежен за ним http gmail sms blogspot.com/ web-приложен на язык python для платформ google app engine, котор след за электрон почт и, при получен нов письма, использу google calendar, присыла смс-уведомление. бол 6000 скачиваний. http://hive-game.blogspot.com/ реализац игр hive(улей) на язык javascript. техническ администратор сайт http://www.3dfashion.biz настройк vps (ubuntu), dns, ftp, apache, nginx, mysql, ssh, переезд хостинг на хостинг. ответственность, коммуникабельность, дружелюбие, добропорядочность, оп общен людьми, аккуратность. препода язык программирован\n",
      " разработа мно проект http pda ru forum index php showtopic комплекс приложен для охра автомобил использован телефон на баз ос android так же для слежен за ним http gmail sms blogspot.com/ web-приложен на язык python для платформ google app engine, котор след за электрон почт и, при получен нов письма, использу google calendar, присыла смс-уведомление. бол 6000 скачиваний. http://hive-game.blogspot.com/ реализац игр hive(улей) на язык javascript. техническ администратор сайт http://www.3dfashion.biz настройк vps (ubuntu), dns, ftp, apache, nginx, mysql, ssh, переезд хостинг на хостинг. twitter: http://twitter.com/olegt_ github: https://github.com/olegt ответственность, коммуникабельность, дружелюбие, добропорядочность, оп общен людьми, аккуратность. препода высш математик информатик\n",
      " отличн знан тк рф том числ вопрос по район крайн север вахтов метод работ совершенств влад пк пакет ms office том числ vba налич автомобил водительск удостоверен категор «в». оп организац кадров делопроизводств «нуля». создан многофункциональн центр нул развертыван контрол за it инфраструктур планирован бюджетирован разработк систем мотивац обучен\n",
      " инженер исследовател богат опыт разработк интерфейс ас прикладн программирован на delphi python labview участ маркетингов проект журналистик занима научн исследован европ автор научн публикац по моделирован визуализац магнитн пол неразруша контролю. им практическ оп по проектирован монтаж силов распределительн щитов, устройств защитн автоматики, заземления, экранирования. свободн врем интерес систем охлажден вычислительн систем, вопрос информацион безопасности, защит данных. занима самообучен област инженерн психологии, юзабилит интерфейсов. по мер надобн осваива люб программн продукты. гот уч дел опытом. разработк по для сопряжен аппаратн комплекс охра периметр систем видеорегистрац geovision через устройств дискретн ввод вывод advantech pci delphi labview разработк программн эмулятор переферийн устройств для geovision разработк оборудован для преобразован сигналов. координирован этап проекта.\n",
      " projects tula cannoneers android java android приложен для болельщик футбольн клуб арсена тул ег помощ возможн узна результат последн матч новост клуб такж просмотрет турнирн таблиц соста команд поддержк устройств android 2.1+ подробно: http://scherbatykh.ru/#section-tulacannoneers tsudesk, android, java android-приложен для студент преподавател тульск государствен университет (тулгу). позволя просматрива учебн расписание, новост вуза, такж успеваем по семестрам. присутств интерактивн карт корпус общежитий. (с использован google maps api). поддержк устройств android 2.1+ подробно: http://scherbatykh.ru/#section-tsudesk loyalty system, android, java разработк android-приложен серверн api (php mysql) для систем лояльности. приложен позволя оператор организац проверя баланс, такж зачисля списыва балл клиентов, сканиру штрих-код клиентск карт (ean-13). общен удален баз происход по средств разработа api. такж контролир привязк организац конкретн устройству(п imei). подробно: http://scherbatykh.ru/#section-loyalty sbquick, android, java android-клиент для работ мобильн банк \"сбербанк\". ег помощ можн узна текущ баланс карт, оплат мобильн телефон проч услуги. ведёт удобн истор всех соверша операций. не треб подключен сет интернет, взаимодейств мобильн банк происход через sms-команды. поддержк устройств android 2.1+ подробно: http://scherbatykh.ru/#section-sbquick \"город глазов\", android, java android-приложение. удобн доступ новост глазова, афиш событ города. полн справочник организац адрес телефонами. поддержк устройств android 2.1+ подробно: http://scherbatykh.ru/#section-cityapp httpsniffer, windows, c++. сниффер для перехват входящих/исходя http-пакет межд браузер сервером. выборк пакет происход по зада параметр (тип данных, мест поиск пакете, тип пакета, одиночн пакет ил вся сесс хост т.д.). возможн работ как консольн приложения, так windows service. использ windows api, pcap boost. github: https://github.com/fincode/httpsniffer desktop application face recognition mobile cryptoapi js net\n",
      " java android development winapi net sql sqlite mysql etc javascript html css github profile https github com fincode sbquick android java мобильн клиент для работ мобильн банк сбербанк функциона проверк баланс сб-карты; 2. пополнен счет мобильн телефона; 3. оплат второстепен услуг; 4. учет sms-уведомлен от сбербанка; 5. gps-навигация. отображен ближайш банкоматов. in progres; https://github.com/fincode/sbquick httpsniffer, c++. httpsniffer is packet sniffer tool that captures all http requests/responses sent between the web browser and the web server. https://github.com/fincode/httpsniffer rss and twitter reader, android, java. https://github.com/fincode/dfeed console application winapi tcp ip android application java android sdk sharepoint agent wss moss\n",
      " умен работа больш объем информац сжат срок многозадачн дополнительн информац работ со специализирова по intraservice crm siebel smart logger genesys ccpulse configuration manager oracle bi publisher crm siebel зуп прогнозирован объем звонк планирован необходим числ персона составлен график работ персона зависим от нагрузк на call центр по дням недел времен суток формирован оперативн график работ\n",
      " уверен пользовател пк базов знан ос windows офис photoshop delphi html gpss assembler мониторинг крупн сет проведен маркетинглвл политик\n",
      " профессиональн пользовател windows ms office power point adobe acrobat professional интернет веден переговор потенциальн заказчик продвижен компан на рынк создан корпоративн сайт создан фирмен стил реклам подготовк документац для получен лиценз на осуществлен деятельн подготовк ткп оценк рентабельн подготовк тендерн документац участ торг на электрон площадках; организац обучен по повышен квалификац сотрудник (охра труда, техник безопасности, пожарно-техническ минимум, промбезопасность) контрол налич действ удостоверений.\n",
      " организаторск способн эффективн планирован высок уровен самоорганизац самодисциплин ответствен эффективн распределен использован ресурс стрессоустойчив способн концентрац на задач способн анализ способн обучен целеустремлен работоспособн лояльност открыт новому; системн мышлен системн подход решен проблем; умен работа команде; –умен убежда отстаива своё мнение; умен слуша других; инициативность; профессиональн пользовател пк (windows. ms office. power point. adobe acrobat professional, интернет, 1с). веден переговор потенциальн заказчик продвижен компан на рынк создан корпоративн сайт создан фирмен стил реклам подготовк документац для получен лиценз на осуществлен деятельн подготовк ткп оценк рентабельн подготовк тендерн документац участ торг на электрон площадках; организац обучен по повышен квалификац сотрудник (охра труда, техник безопасности, пожарно-техническ минимум, промбезопасность) контрол налич действ удостоверений.\n",
      " responsible for company comeback communications regional coomunication managers coordination speakers coordination and training social traffic analysis social media management social media marketing marketing management google analytics copywriting official headhunter spokesperson speakers coordination and training pr agency and regional coomunication managers coordination russian regions ukraine kazakhstan belarus social media management one specialist inhouse and board of interns communities in fb https://www.facebook.com/headhuntergroup), vk (http://vk.com/headhunter), twitter (https://twitter.com/hh_ru), odnoklassniki (http://www.odnoklassniki.ru/headhunter), instagram (https://instagram.com/hh_ru). project management and producing (social media apps and digital). social traffic analysis with google analytics. habrahabr blog editor, writer and inspirer: http://habrahabr.ru/company/hh producing two radio-shows on moscow 24 and city fm. headhunter became the most quoted company in russian and cis media (labour market and economics), company began to perceive as the most trusted job-site and one of the most innovative internet-companies as well as leader of hr-branding expertise. brought kpis in social media to drive the most important business indicators and themes. as result: dramatically increased traffic (x6 in years) and useful conversions.\n",
      " ответствен усидчив организован честност внимательн мелоч хорош памя при хранен отпуск товарн материальн ценност обеспечен правильн безопасн эксплуатац оборудован обеспечен сохран ценност на склад при товар распределен по мест хранен подач торгов зал возврат брак поставщикам; комплектован товар по заявк клиентов, упаковка; заказ транспорта, отгрузк товара.\n",
      " за врем сво работ сфер ит осво применя множеств технолог применен котор был эффективн кажд конкретн случа операцион систем администрирован windows linux ubuntu терминальн клиент на баз linux wtware, систем виртуализац xen. продукты: iis, apache, microsoft office всех верс различн вариант от друг разработчик (openoffice, libreoffice). программирование: c#, delphi, fastreport, sql баз дан ms sql, interbase/firebird, paradox/dbf, mysql- проектирован баз дан учет нормализац структур, администрирование, анализ выполнен запросов, построен индексов, написан отладк храним процедур триггеров. удален управлен через программ ammyy, teamviewer, rdp, ssh, vnc администрирован сетей: dhcp (linux, windows), dns, wins, vlan, настройк туннел pptp ipsec спокойн характер, любл обща людьм мог публичн выступать. мне нрав занима делом, котор виж смысл процесс выполнен котор видн получа результат. увлечения: боулинг, велосипед, английск язык, фотограф видеосъемка, автомобили, управленческ поединки, дебаты, клуб ораторов, плавание. организац эффективн работ сотрудник отдел сопровожден взаимодейств отдел продаж внедрен документирован разработк распределен запрос контрол качеств срок работ над запрос работ над запрос клиент работ продукт компан участ разработк нов функционала, эскалац исправлен обнаружен ошибок разработк схем мотивац обучен сотрудник решен сложн запросов, связа функционирован системы, пробл производительностью, оптимизац существ бизнес-процессов, построен sql запрос отчетн формах.\n",
      " любл разбира нов задач облада хорош пространствен мышлен облада опыт программирован на язык чем занима на сво нынешн работ навык программирован на asp mvc xna им хорош математическ подготовк вед занят для школьник по программирован рамк программ дополнительн образован \"открытие\" (http://otkrytie.edu.yar.ru/ разработк игр на движк unity\n",
      " основн язык имел небольш оп разработк на js html год вмест двум однокурсник разработа игр cleaney под windows phone для конкурс imagine cup https goo gl hb tiz проект вошел лучш мир представля ег нью йорк перед международн жюри. хож походы, пою, игра на гитаре, любл путешествовать. разработк клиентск част многопользовательск игр на unity язык для работ ui использ ngui работа над составлен сцен разработк вспомогательн средств систем локализац хранен загрузк конфиг сжат атлас систем управлен событ сервера. такж вел небольш игров проекта, внедря сист git submodule для использован общ функциона межд проект\n",
      " основн язык имел небольш оп разработк на js html год вмест двум однокурсник работа над проект игр cleaney для телефон windows phone для конкурс imagine cup проект вошел лучш мир представля ег нью йорк перед международн жюри. ключев навыки: ответственность, обучаемость, организаторск способн (в течен лет руковож туристическ групп категорийн походах). разработк клиентск част многопользовательск игр на unity язык для работ ui использ ngui работа над составлен сцен разработк вспомогательн средств систем локализац хранен загрузк конфиг сжат атлас систем управлен событ сервера.\n",
      " аналитическ склад ум умен работа больш объем информац целеустремлен умен довод начат до конц пунктуальн честност порядочн добросовестн способн обучен желан совершенствова профессиональн сфер разработк игров по на язык javascript под unity написан шейдер работ анимац ds max adobe photoshop работ систем контрол верс тестирован парн программирован оптимизац рефакторинг\n",
      " аналитическ склад ум умен работа больш объем информац целеустремлен умен довод начат до конц пунктуальн честност порядочн добросовестн способн обучен желан совершенствова профессиональн сфер разработк игров по на язык javascript под unity написан шейдер работ анимац ds max adobe photoshop работ систем контрол верс тестирован парн программирован оптимизац рефакторинг\n",
      " графическ дизайнер дизайн полиграфическ продукц верстк многостраничн издан фирмен стил\n",
      " лет опыт полиграфическ дизайн подкреплен ти год обучен на математик теоретик корн побед вернул программирован фриланс web разработк php верстк вебстуд стартап виде gps сервис, фриланс как frontend разработчик команде. профили: http://github.com/tyllo http://bitbucket.org/tyllo последн работа: приложение: http://tyllo.github.io/framework7-vuejs/ код: https://github.com/tyllo/framework7-vuejs креды: demo/demo на основ стил библиотек framework7 созда приложен для личн кабинет компан vsct.info. бекенд на основ yii2 написа ещ раньш для spa-приложения, встроен сайт компании. приложен был использова flux-архитектура, компонентн подход, модульн css, для иконок svg-спрайты. можн посмотрет как приложен работает, введ креды: demo/demo (= вы увид рандомн данные). jade, scss, es2015, gulp, webpack, svg-sprites, компонентн css. framework7, vuejs, vue-router, vuex flux для vuejs набор технологий, использова проекте. стек технологий, котор хорош знак применя на практике: scss, jade, es2015, webpack, gulp php, bash, mysql, yii2 знак ruby on rails, rspec bootstrap zurb foundtion git, svn, jira restfull, cors, spa adobe photoshop, adobe illustrator, adobe indesign backbonejs, requirejs, underscorejs, vuejs flux, leafletjs, emberjs, lodashjs, snap.svg, momentjs, c3.js, reactjs, redux framework7 mochajs, chaijs, qunit windows, cygwin, sublime text, cloud9, conemu, git-shell, vagrant некотор пример работ: 1) spa записн книжка: http://notebook.tests.xdraw.ru backbonejs, requirejs, underscorejs, jquery, bootstrap, cors, restfull http://notebook.tests.xdraw.ru https://github.com/tyllo/notebook.tests 2) spa личн кабинет для клиент vsct.info jade, scss, es2015, gulp, webpack на клиенте: vuejs, vue-router, zurb foundtion на сервере: yii2, soap, cors, restfull, auth-token http://vsct.info/account/login закр для паблик был реализова api soap-сервер на yii2 spa-приложен на основ иммутабельной, реактивной, mvvm библитек vuejs. верстк на основ существ сайта.) 3) гибридн приложен личн кабинет vsct.info: http://tyllo.github.io/framework7-vuejs/ login/password: demo/demo jade, scss, es2015, gulp, webpack, svg-sprites, компонентн css framework7, vuejs, vue-router, vuex flux для vuejs, svg-спрайт процесс разработки, тестов приложение. пок реализ визуальн составляющая. http://tyllo.bitbucket.org/vsct.info.mobile 4) видео/gps сервис для авторегистраторов. ria стад разработки. гибрид плеер карты, функц live-просмотра, просмотр архива. emberjs, snap.svg, leafletjs, momentjs, qunit, ruby on rails, rspec 5) пробн приложен для ознакомлен reactjs, redux, react-router: http://tyllo.github.io/react.test/ 6) вертск по прототип psd: http://tyllo.bitbucket.com/abns личн кабинет http://tyllo.bitbucket.com/dobroe-delo адаптивн верстк http://tyllo.bitbucket.com/excelente адаптивн верстк разработк поддержк webui для продукт компан spa вертск\n",
      " увлека мот компан владеет несколк направлен порт ольг компан ольгалес рыблеспр автосервис бриз работ рекламн агенств типограф контрол выполнен качеств работ договор взаимодейств партнер проведен совместн акц подготовк азработк материал для выставк рти шин каучук 2013, 2014гг. разработк изготовлен каталог продукц шинами, рыбн продукц (фот продукции, обработк фотоматериал для каталога, дизайн каталога), макет баннеров, буклетов, брендирован витрин магазин рыблеспром. изготовлен flash-баннеров. изготовлен дизайн сайт автосервис (http://www.briz-sto.ru). веден переговоров, сосставлен тз для изготовлен сайт порт ольг (http://www.port-olga.com), дизайн сайта. развертыван готов сайт на собствен хостинге. изготовлен сайт (подбор материалов, дизайн, разработк сайта) для партнер компан бриз http://boto.ddns.net\n",
      " разработк win forms wpf приложен администрирован бд\n",
      " администрирован сервер nix freebsd win сервер диагностическ профилактическ работ по поддержан серверн парк компан компьютерн техник коммуникацион оборудован сопровожден баз дан антивирусн криптографическ защит сет подготовк предложен по информацион техническ развит на след бюджетн год подготовк техническ требован закупа товар работ услуг определен существен услов государствен контракт договор при подготовк размещен заказ такж участ приемк согласован акт сдачи-приемк товаров, работ услуг по государствен контракт договорам, заключен по результат размещен заказов. организац работ по подготовк государствен закупок установлен сфер деятельности, такж исполнен заключен государствен контракт договоров. веден учет средств вычислительной, коммуникацион техник принтеров. веден контрол исполнен сторон организац договорн обязательств. организац контрол регламентн обслуживан средств вычислительной, коммуникацион техник принтеров. мониторинг состоян средств вычислительной, коммуникацион техник принтеров. администрирован сервер уровн предприят (freebsd, aix, *nix, win2003-2012 сервера), диагностическ профилактическ работ по поддержан серверн парк компании, компьютерн техник коммуникацион оборудования.\n",
      " администрирован сервер nix freebsd win сервер диагностическ профилактическ работ по поддержан серверн парк компан компьютерн техник коммуникацион оборудован сопровожден баз дан антивирусн криптографическ защит сет подготовк предложен по информацион техническ развит на след бюджетн год подготовк техническ требован закупа товар работ услуг определен существен услов государствен контракт договор при подготовк размещен заказ такж участ приемк согласован акт сдачи-приемк товаров, работ услуг по государствен контракт договорам, заключен по результат размещен заказов. организац работ по подготовк государствен закупок установлен сфер деятельности, такж исполнен заключен государствен контракт договоров. веден учет средств вычислительной, коммуникацион техник принтеров. веден контрол исполнен сторон организац договорн обязательств. организац контрол регламентн обслуживан средств вычислительной, коммуникацион техник принтеров. мониторинг состоян средств вычислительной, коммуникацион техник принтеров. администрирован сервер уровн предприят (freebsd, aix, *nix, win2003-2012 сервера), диагностическ профилактическ работ по поддержан серверн парк компании, компьютерн техник коммуникацион оборудования.\n",
      " влад начальн навык скриптинг на python писа скрипт мониторинг баз oracle обновлен как сисадмин сейчас ускорен изуча книг по программирован вскор перейд изучен flask django базов знан по html css (прав тем для wordpress-сайтов). ум администрирова linux-систем (был администратор приложен oebs на oracle linux), администрирован веб-сервер (apache, nginx) осв быстро. администрирова ms sql 2000-2008, немн postgres поэт mysql разберусь. част администрирован сервер бд подтян за месяц, junior django разработчик стан течен 4-5 месяцев. гот совмеща одновремен работ системн администратор недоджуниором. подготовк предложен по информацион техническ развит на след бюджетн год подготовк техническ требован закупа товар работ услуг определен существен услов государствен контракт договор при подготовк размещен заказ такж участ приемк согласован акт сдачи-приемк товаров, работ услуг по государствен контракт договорам, заключен по результат размещен заказов. организац работ по подготовк государствен закупок установлен сфер деятельности, такж исполнен заключен государствен контракт договоров. веден учет средств вычислительной, коммуникацион техник принтеров. веден контрол исполнен сторон организац договорн обязательств. организац контрол регламентн обслуживан средств вычислительной, коммуникацион техник принтеров. мониторинг состоян средств вычислительной, коммуникацион техник принтеров. администрирован сервер уровн предприят (freebsd, aix, *nix, win2003-2012 сервера), диагностическ профилактическ работ по поддержан серверн парк компании, компьютерн техник коммуникацион оборудования.\n",
      " владен техническ средств знан активн изучен частност технолог asp net mvc web api entity framework проект использова ninject automapper nunit оп разработк мобильн приложен под ios на js (titanium) оп применен java, c++, sql знак python, php оп работ visual studio (2008 2015) intellij idea, git, svn, jira небольш оп использован wpf batch свободн врем читаю, изготавлив печатн платы, ковыря arduino, что-нибуд паяю. прислушива мнен других, ответственный, любл изуча нов технологии. ищ работу, котор можн совмеща посещен занят магистратуре. акад городок не рассматриваю. больш всег мен интерес asp.net mvc, гот рассмотрет wpf windows phone разработку. пример одн из последн маленьк проект https://github.com/beta-tank/tictactoe разработк мобильн приложен под ios на js использован titanium разработк backend на стек технолог asp net sql server\n",
      " the lack of experience of some products libraries does not prevent me quickly learn and use them in practice operations research mathematical programming and optimization problems and data analysis data mining machine learning) area of interest, carried out the implementation of the algorithms of these areas, such as algorithms discriminant analysis, factor analysis, the ann, decision trees c4.5, cluster analysis (k-means, forel), familiar with other methods of machine learning. from the optimization algorithms implemented branch and bound, dynamic programming method, genetic algorithm, simplex method ... my articles: dynamic lot size model: http://eureka-operationresearch.blogspot.com/2011/09/dynamic-lot-size-model.html эвристик составлен расписан занятий: http://habrahabr.ru/post/148717/ coursera.org: machine learning (stanford university, andrew ng) введен машин обучен (высш школ экономики, yandex) (at the present time) designed and implemented applications devoloped modules back end system of electronic tax statements created soap based web services integration with the information systems of partners control electronic signature module for tax assessments module for correspondence audit optimization of the commodity items matching algorithm and much more used technologies and tools: ms sql server 2012, java 7, jax-ws (cxf), ws-security, jax-b, itext, jdbc, junit, maven, tomcat\n",
      " отсутств опыт по нек продукт библиотек не меша мне оперативн изуч задействова на практик их област исследован операц математическ программирован оптимизацион задач анализ дан data mining machine learning сфер интересов, выполня реализац алгоритм из дан областей, так как алгоритм дискриминантн анализа, факторн анализа, инс, дерев решен c4.5, кластерн анализ (k-means, forel), знак ин метод machine learning. из оптимизацион алгоритм реализовыва метод ветв границ, метод динамическ программирования, генетическ алгоритмы, симплекс-метод... мо статьи: dynamic lot size model: http://eureka-operationresearch.blogspot.com/2011/09/dynamic-lot-size-model.html эвристик составлен расписан занятий: http://habrahabr.ru/post/148717/ linkedin: http://www.linkedin.com/pub/suren-tamrazyan/34/a0/a80 мойкруг: http://stamrazyan.moikrug.ru/ курс coursera.org: machine learning (stanford university, andrew ng) введен машин обучен (высш школ экономики, yandex) (в настоя время) организац процесс разработк непосредствен разработк программн обеспечен разработа след модул back end систем электрон налогов отчетн на основ soap веб сервис интеграц информацион систем партнер модул управлен эцп модул налогов расчетов. модул камеральн проверок оптимизац алгоритм сопоставлен товарн позиц межд разн налогоплательщик мног друг используем технолог инструментальн средства: ms sql server 2012, java, jax-ws (cxf), ws-security, jax-b, itext, jdbc, junit, maven, tomcat\n",
      " отсутств опыт по нек продукт библиотек не меша мне оперативн изуч задействова на практик их област исследован операц математическ программирован оптимизацион задач анализ дан data mining machine learning сфер интересов, выполня реализац алгоритм из дан областей, так как алгоритм дискриминантн анализа, факторн анализа, инс, дерев решен c4.5, кластерн анализ (k-means, forel), знак ин метод machine learning. из оптимизацион алгоритм реализовыва метод ветв границ, метод динамическ программирования, генетическ алгоритмы, симплекс-метод... мо статьи: dynamic lot size model: http://eureka-operationresearch.blogspot.com/2011/09/dynamic-lot-size-model.html эвристик составлен расписан занятий: http://habrahabr.ru/post/148717/ linkedin: http://www.linkedin.com/pub/suren-tamrazyan/34/a0/a80 мойкруг: http://stamrazyan.moikrug.ru/ курс coursera.org: machine learning (stanford university, andrew ng) введен машин обучен (высш школ экономики, yandex) (в настоя время) организац процесс разработк непосредствен разработк программн обеспечен разработа след модул back end систем электрон налогов отчетн на основ soap веб сервис интеграц информацион систем партнер модул управлен эцп модул налогов расчетов. модул камеральн проверок оптимизац алгоритм сопоставлен товарн позиц межд разн налогоплательщик мног друг используем технолог инструментальн средства: ms sql server 2012, java, jax-ws (cxf), ws-security, jax-b, itext, jdbc, junit, maven, tomcat\n",
      " the lack of experience of some products libraries does not prevent me quickly learn and use them in practice operations research mathematical programming and optimization problems and data analysis data mining machine learning) area of interest, carried out the implementation of the algorithms of these areas, such as algorithms discriminant analysis, factor analysis, the ann, decision trees c4.5, cluster analysis (k-means, forel), familiar with other methods of machine learning. from the optimization algorithms implemented branch and bound, dynamic programming method, genetic algorithm, simplex method ... my articles: dynamic lot size model: http://eureka-operationresearch.blogspot.com/2011/09/dynamic-lot-size-model.html эвристик составлен расписан занятий: http://habrahabr.ru/post/148717/ coursera.org: machine learning (stanford university, andrew ng) введен машин обучен (высш школ экономики, yandex) (at the present time) designed and implemented applications devoloped modules back end system of electronic tax statements created soap based web services integration with the information systems of partners control electronic signature module for tax assessments module for correspondence audit optimization of the commodity items matching algorithm and much more used technologies and tools: ms sql server 2012, java 7, jax-ws (cxf), ws-security, jax-b, itext, jdbc, junit, maven, tomcat\n",
      " свободн от работ врем изуча unity вмест товарищ разрабатыва игр проход служб войск связ пиш стих рассказ удален работ компан excit group разработк нул по веб част систем для бизнес консалтинг построен математическ обработк граф послед визуализац используем стэк технолог soft server python postgresql networkx pygraphviz web-server: ruby on rails 4.0 для связ веб-част сервер использ tcp-socket самописн протоколом. на дан момент проект наход стад бета-тестирования. релиз запланирова на август 2016.\n",
      " игра на гитар ум вс что не ум науч участ разработк финансов приложен разработк api бэкенд yii фронтенд angularjs работ баз postgresql git redmine\n",
      " базов знан html css php js jquery ajax mysql уверен знан парадигм ооп знан php инфраструктур устройств веб приложен пониман шаблон проектирован mvc работ файл сесс кук mysql базов синтаксис объединен таблиц; умен разбира чуж коде; оп проектирован бд; стремлен разбира изуча новое; умерен перфекционизм работе; разработк сайт на движк компан php js mysql smarty bootstrap jquery composer svn git последн пар месяц дела http nedvi com\n",
      " https play google com store apps details id com instaforex android forex humor поддержива приложен https play google com store apps details id com instaforex forexpedia https play google com store apps details?id=club.ruchat оп разработк клиент-серверн приложен для андроид. https://github.com/max-makeychik http://stackoverflow.com/users/3937738/max-makeychik разработк приложен для покупок одежд\n",
      " https play google com store apps details id com instaforex android forex humor поддержива приложен https play google com store apps details id com instaforex forexpedia оп разработк клиент серверн приложен для андроид https://play.google.com/store/apps/details?id=club.ruchat разработк приложен для android на язык java\n",
      " трудолюбив общительн ответсвен монтаж опс скуд видеонаблюден резервн освещен техническ обслуживан опс скуд видеонаблюден програмирован болид аргус спектр си норд альтоник ввывод систем видеонаблюден сет локальн глобальн подготовк объект сдач частн гос.службы, управлен техническ персоналом,проектирован опс,скуд видеонаблюдения.\n",
      " стремл изуча нов технолог подход разработк хорош реша задач на построен алгоритм свободн врем реша задан контест на topcoder сам контест не участвова хорош работа команд получа удовольств от программирован хоч развива эт направлении. желательн работ по разработк back-end части. зна c#, платформ .net. оп работ wpf, wcf, sqlite. понима основ ооп паттерн mvc. гот радост изуча нов язык библиотек при необходимости. разработк программ на\n",
      " сбор анализ требован проектирован решен постановк задач разработчик подготовк документац том числ по гост продукт smg круинг веб сервис позволя судовладельц круингов агентств наход моряк\n",
      " java netty apache commons log as flex mysql mercurial svn jira scrum был оп работ javascript html использ python качеств язык для написан утил для автоматизац очен интерес но недостаточн опыта) c++. проект геро нов саг https vk com app сервер java apache mina клиент flash достижен разработа сист проведен турнир вкладк турнир написа утилит для тестирован баланс характеристик юнит прост тон всяк мелк исправлен улучшений. декабр 2015 работа над проект \"королевства\" https://vk.com/app4917198_12416177 достижения: написа утилит для тестирован карт; написа утилит для имитацион тестирован экономическ баланс постройк королевства;\n",
      " хорош организаторск способн ориентирован на результат умен работа услов ограничен времен способн принят самостоятельн решен умен работа команд дисциплинирова стремл развит обучен ум любл работа рук монтаж настройк сетев оборудован связ доступ транспорт антен ubiquti mikrotik nec aclatel wavion коммутатор link edge core mikrotik snr маршрутизатор mikrotik wi fi мост прокладк подключен кабельн лин utp, ofc, коаксиальный, питание); поиск устранен причин аварий; поиск закупк необходим материал оборудования; техническ поддержк консультац абонентов.\n",
      " последн год проход обучен сибгут служб арм хорош организаторск способн ориентирован на результат умен работа услов ограничен времен способн принят самостоятельн решен умен работа команд дисциплинирован, обучаем. соблюден стандарт реализац проект установлен агентств личн подчинен консультант промоутер организацион сопровожден акц обеспечен персона всем необходим контрол работ персона консультант промоутер тт формирован отчетн по акц контрол выкладк продукц на полк магазинов; фотоотчет.\n",
      " разработк внедрен проект комплексн автоматизац планирован управлен техническ отдел администрирован ос microsoft windows microsoft windows server nix reebsd mac os проектирован монтаж скс сист контрол доступ сист учет рабоч времени. проектирован создан распределен сет на баз сист linux, freebsd, mikrotik routeros, cisco, juniper. создан сист виртуализац на основ vmware esxi server, microsoft hyper-v, nutanix. проектирован монтаж обслуживан скс том числ распределен it аутсорсинг на предприят от до рабоч мест\n",
      " оп администрирован сет под управлен ms windows установк настройк администрирован оп администрирован ms windows server оп настройк почтов офисн программ оп монтаж локальн вычислительн сет умен консультирова телефон режим планирован работ отдел такж программист уда нных подразделен разработк методическ материал инструкц соответств руководств по использован эксплуатац автоматизирова задач сист программн средств осуществлен контрол за их выполнен обеспечен работ прикладн по филиала; администрирован официальн сайт филиа http://tulapost.ru/; краткосрочн командировк на удалён объект филиала: техническ поддержк пользователей, диагностик неисправн настройк оборудования. техническ обслуживан информацион систем. системн администрирование: сеть, состоя из бол чем 150 рабоч станц входя ad windows 300 удалён рабоч станций. операцион систем на рабоч станц от windows 98 до windows xp. *сборк настройк компьютеров; *обслуживан оргтехники; *поддержк пользователей, обучен молод сотрудников. *администрирован федеральн проект правительств росс тульск област по разворачиван обслуживан распределен сет пкд (пункт коллективн доступ интернет) *администрирован сайт tulapost.ru *автоматизац мелк технологическ процесс помощ ide delphi (работ суб firebird) *монтаж сет *мелк ремонт электроник (смен конденсатор на материнск платах, ремонт бп)\n",
      "processed 80\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-86e2b6f157dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mp_res\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 339\u001b[1;33m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    340\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m         \u001b[1;32mprint\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#recommender2\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import heapq\n",
    "import numpy\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "import time\n",
    "import datetime\n",
    "import redis\n",
    "import threading\n",
    "from multiprocessing import Pool\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "start_time = time.time()\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "conn.close()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "conn.close()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "  \n",
    "with open( \"count_vectorizer.p\", \"rb\" ) as f:\n",
    "    count_vectorizer = pickle.load(f)\n",
    "    \n",
    "with open( \"tfidf_transformer.p\", \"rb\" ) as f:\n",
    "    tfidf_transformer = pickle.load(f)\n",
    "\n",
    "def get_resumes():\n",
    "    db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "    db.autocommit(True)\n",
    "    db.set_character_set('utf8')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute('SET NAMES utf8;')\n",
    "    cursor.execute('SET CHARACTER SET utf8;')\n",
    "    cursor.execute('SET character_set_connection=utf8;')\n",
    "    cursor.close()\n",
    "\n",
    "    salaries = []\n",
    "    features = []\n",
    "    ids = []\n",
    "    areas = []\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT r.item \n",
    "        FROM resumes r \n",
    "        WHERE r.is_active=1 AND NOT EXISTS (\n",
    "            SELECT * \n",
    "            FROM recommendations rec\n",
    "            WHERE rec.resume_id=r.item_id and rec.is_active=1\n",
    "        ) \"\"\")\n",
    "    for item in cursor:\n",
    "        resume_json = json.loads(item[0])\n",
    "        feature = []\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        if resume_json['skills'] != None:\n",
    "            doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "            doc = re.sub('&quot;', '', doc)\n",
    "            doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', doc.strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word.strip())\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        if resume_json['title'] != None:\n",
    "            title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', title.strip())\n",
    "            for title_word in words:\n",
    "                title_word = stemmer.stemWord(title_word)\n",
    "                if len(title_word.strip()) > 1:\n",
    "                    p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        res_skills = resume_json['skill_set']\n",
    "        for skill in res_skills:\n",
    "            words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "\n",
    "        #salary\n",
    "        salary = None\n",
    "        if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "            salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "            \n",
    "        #experience\n",
    "        if resume_json['experience'] != None and len(resume_json['experience'])> 0 and resume_json['experience'][0]['description'] != None:\n",
    "            experience_description = resume_json['experience'][0]['description']\n",
    "            doc = re.sub('<[^>]*>', '', experience_description.lower())\n",
    "            doc = re.sub('&quot;', '', doc)\n",
    "            doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', doc.strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word.strip())\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_doc = p_doc + \" \" + word\n",
    "            \n",
    "        \n",
    "        \n",
    "        res_areas = []\n",
    "        if resume_json['area'] == None:\n",
    "            res_areas.append(areas_map[\"1\"])\n",
    "        else :\n",
    "            res_areas.append(areas_map[resume_json['area']['id']])\n",
    "        for area in resume_json['relocation']['area']:\n",
    "            res_areas.append(areas_map[area['id']])\n",
    "        areas.append(res_areas)\n",
    "        \n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        feature = tfidf_transformer.transform(feature_p_doc)\n",
    "        features.append(feature.toarray())\n",
    "        salaries.append(salary)\n",
    "        ids.append(resume_json['id'])\n",
    "    cursor.close()\n",
    "    db.close()\n",
    "    return features, salaries, ids, areas\n",
    "\n",
    "resume_features, resume_salaries, resume_ids, resume_areas = get_resumes()\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_vacancy_ids(vacancies):\n",
    "    pre_vacancy_similarities = {}\n",
    "    pre_vacancy_ids = {}\n",
    "\n",
    "    for idx, val in enumerate(resume_features):\n",
    "        new_vacancy_features = []\n",
    "        new_vacancy_ids = []\n",
    "        for vac_id, vac_data in vacancies.iteritems():\n",
    "            if resume_areas[idx][0] == vac_data['area'] and (resume_salaries[idx] == None or vac_data['salary'] == 'None'):\n",
    "                new_vacancy_features.append(json.loads(vac_data['features'].decode('zlib')))\n",
    "                new_vacancy_ids.append(vac_id)\n",
    "            elif resume_areas[idx][0] == vac_data['area']:\n",
    "                min_resume_salary = resume_salaries[idx] - (resume_salaries[idx] * 0.2)\n",
    "                max_resume_salary = resume_salaries[idx] + (resume_salaries[idx] * 0.8)\n",
    "                vac_salary = float(vac_data['salary'])\n",
    "                if vac_salary >= min_resume_salary and vac_salary <= max_resume_salary:\n",
    "                    new_vacancy_features.append(json.loads(vac_data['features'].decode('zlib')))\n",
    "                    new_vacancy_ids.append(vac_id)\n",
    "                    \n",
    "        similarities = []\n",
    "        ids = []\n",
    "        if len(new_vacancy_features) > 0:\n",
    "            c_result = cosine_similarity(resume_features[idx], new_vacancy_features)\n",
    "            res = heapq.nlargest(20, range(len(c_result[0])), c_result[0].take)\n",
    "\n",
    "            for j in res:\n",
    "                similarities.append(c_result[0][j])\n",
    "                ids.append(new_vacancy_ids[j])\n",
    "        \n",
    "        lock.acquire()\n",
    "        try:\n",
    "            if resume_ids[idx] not in pre_vacancy_similarities:\n",
    "                pre_vacancy_similarities[resume_ids[idx]] = similarities\n",
    "                pre_vacancy_ids[resume_ids[idx]] = ids\n",
    "            else:\n",
    "                pre_vacancy_similarities[resume_ids[idx]] = pre_vacancy_similarities[resume_ids[idx]] + similarities\n",
    "                pre_vacancy_ids[resume_ids[idx]] = pre_vacancy_ids[resume_ids[idx]] + ids\n",
    "        finally:\n",
    "            lock.release()\n",
    "            \n",
    "    return len(vacancies), pre_vacancy_similarities, pre_vacancy_ids\n",
    "\n",
    "tp_res = [] \n",
    "tpool = Pool(3) \n",
    "def iterate_ids(start):\n",
    "    cnt = 500\n",
    "    rcursor = r.scan(cursor=start, count=cnt)\n",
    "    vacancies = {}\n",
    "    for vac_id in rcursor[1]:\n",
    "        vacancies[vac_id] = r.hgetall(vac_id)\n",
    "    tres = tpool.apply_async(process_vacancy_ids, (vacancies,))\n",
    "    tp_res.append(tres)\n",
    "    while (rcursor[0] != 0):\n",
    "        rcursor = r.scan(cursor=rcursor[0], count=cnt)\n",
    "        vacancies = {}\n",
    "        for vac_id in rcursor[1]:\n",
    "            vacancies[vac_id] = r.hgetall(vac_id)\n",
    "        tres = tpool.apply_async(process_vacancy_ids, (vacancies,))\n",
    "        tp_res.append(tres)\n",
    "\n",
    "iterate_ids(0)\n",
    "\n",
    "c = 0\n",
    "pre_vacancy_similarities = {}\n",
    "pre_vacancy_ids = {}\n",
    "for tr in tp_res:\n",
    "    cnt, p_vacancy_similarities, p_vacancy_ids = tr.get()\n",
    "    for resume_id in p_vacancy_similarities.keys():\n",
    "        if resume_id not in pre_vacancy_similarities:\n",
    "            pre_vacancy_similarities[resume_id] = p_vacancy_similarities[resume_id]\n",
    "            pre_vacancy_ids[resume_id] = p_vacancy_ids[resume_id]\n",
    "        else:\n",
    "            pre_vacancy_similarities[resume_id] = pre_vacancy_similarities[resume_id]+p_vacancy_similarities[resume_id]\n",
    "            pre_vacancy_ids[resume_id] = pre_vacancy_ids[resume_id]+p_vacancy_ids[resume_id]\n",
    "    \n",
    "    c = c+cnt\n",
    "    print 'processed {}'.format(c)\n",
    "\n",
    "def finalize_recommendations(resume_id):\n",
    "    result = []\n",
    "    similarities = pre_vacancy_similarities[resume_id]\n",
    "    ids = pre_vacancy_ids[resume_id]\n",
    "    max_similarities = heapq.nlargest(20, range(len(numpy.asarray(similarities))), numpy.asarray(similarities).take)\n",
    "\n",
    "    db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "    db.autocommit(True)\n",
    "    db.set_character_set('utf8')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute('SET NAMES utf8;')\n",
    "    cursor.execute('SET CHARACTER SET utf8;')\n",
    "    cursor.execute('SET character_set_connection=utf8;')\n",
    "    cursor.close()\n",
    "\n",
    "    cursor = db.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"\"\"UPDATE recommendations SET is_active=0 WHERE resume_id='{}'\"\"\".format(resume_id))\n",
    "    except BaseException as ex:\n",
    "        print ex\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "        \n",
    "    for ind in max_similarities:\n",
    "        conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(ids[ind]), headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        if r1.status != 200:\n",
    "            conn.close()\n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(ids[ind]), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "        t_vacancy = r1.read()\n",
    "        conn.close()\n",
    "        t_vacancy_json = json.loads(t_vacancy)\n",
    "        try:\n",
    "            title = t_vacancy_json['name'].encode('utf-8').strip()\n",
    "        except KeyError as ex:\n",
    "            print ex\n",
    "            title = 'Title temporary not found'\n",
    "\n",
    "        \n",
    "        cursor = db.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO recommendations (resume_id, vacancy_id, updated, is_active, similarity, vacancy_title) \n",
    "                VALUES ('{}', {}, now(), 1, {}, '{}')\n",
    "            \"\"\".format(resume_id, ids[ind], similarities[ind], title))\n",
    "        except BaseException as err:\n",
    "            print err\n",
    "        finally:\n",
    "            cursor.close()\n",
    "        \n",
    "        result.append('{}. for {} similarity is {}'.format(resume_id, ids[ind], similarities[ind]))\n",
    "\n",
    "    db.close()\n",
    "    return result\n",
    "\n",
    "p_res = [] \n",
    "pool = Pool(7) \n",
    "for resume_id in pre_vacancy_similarities.keys():\n",
    "    res = pool.apply_async(finalize_recommendations, (resume_id,))\n",
    "    p_res.append(res)\n",
    "    \n",
    "for t in p_res:\n",
    "    res = t.get()\n",
    "    for s in res:\n",
    "        print s\n",
    "        \n",
    "\n",
    "print 'total time {} sec\\n'.format(time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "абр\n",
      "хабр\n"
     ]
    }
   ],
   "source": [
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "print stemmer.stemWord('абром')\n",
    "print stemmer.stemWord('хабру')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
