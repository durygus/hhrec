{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.729580198569\n",
      "0.957427107756\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "a1 = [1, 1, 1, 1]\n",
    "a2 = [2, 54, 13, 15]\n",
    "a3 = [2, 2, 4, 3]\n",
    "print 1 - spatial.distance.cosine(a1, a2)\n",
    "print 1 - spatial.distance.cosine(a1, a3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 OK\n",
      "200 OK\n",
      "{u'url': u'https://api.hh.ru/areas/1', u'id': u'1', u'name': u'\\u041c\\u043e\\u0441\\u043a\\u0432\\u0430'}\n",
      "[]\n",
      "[u'1.221', u'1.295', u'1.89']\n",
      "{u'currency': u'RUR', u'amount': 135000}\n",
      "{u'id': u'full', u'name': u'\\u041f\\u043e\\u043b\\u043d\\u0430\\u044f \\u0437\\u0430\\u043d\\u044f\\u0442\\u043e\\u0441\\u0442\\u044c'}\n",
      "[{u'id': u'fullDay', u'name': u'\\u041f\\u043e\\u043b\\u043d\\u044b\\u0439 \\u0434\\u0435\\u043d\\u044c'}, {u'id': u'remote', u'name': u'\\u0423\\u0434\\u0430\\u043b\\u0435\\u043d\\u043d\\u0430\\u044f \\u0440\\u0430\\u0431\\u043e\\u0442\\u0430'}]\n",
      "{u'months': 116}\n",
      "Java Developer\n",
      "Опытный разработчик программного обеспечения. Имею широкий спектр навыков, знаний и технологий. Разрабатываю быстрые программы, работающие с большими объемами данных в многопотоковой среде. Уделяю большое внимание качеству кода и удобству дальнейшей поддержки. Аналитически подхожу к решению задач. Имею хорошие знания алгоритмов и структур данных.\n",
      "\n",
      "Хобби, увлечения\n",
      "Интернет-технологии, путешествия, фотография, кулинария. \n",
      "Организовал и веду в местной школе кружок по программированию для детей.\n",
      "В последнее время интересуюсь машинным обучением и OpenCV\n",
      "\n",
      "Профиль на Habrahabr: http://habrahabr.ru/users/shurik2533/\n",
      "[u'Java', u'Java EE', u'Oracle Pl/SQL', u'Python', u'Data Analysis', u'Big Data', u'recommender systems', u'machine learning']\n",
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import httplib\n",
    "import json\n",
    "headers = {\"Authorization\": \"Bearer PHUAM0L3PU56VNT041CJM3MUTGSABUCDTMBVHEAMF5CGCDEEIC7VFFT4VLOP0GQP\", \n",
    "           \"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"/resumes/mine\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "print r1.status, r1.reason\n",
    "data = r1.read()\n",
    "for i in range(len(json.loads(data)['items'])):\n",
    "    resume_id = json.loads(data)['items'][i]['id']\n",
    "    conn.request(\"GET\", \"/resumes/{0}\".format(resume_id), headers=headers)\n",
    "    r2 = conn.getresponse()\n",
    "    print r2.status, r2.reason\n",
    "    resume_data = r2.read()\n",
    "    features = [1,2,3]\n",
    "    print json.loads(resume_data)['area'] #по этому отсекаем сразу\n",
    "    print json.loads(resume_data)['relocation']['area'] #этот туда же\n",
    "    \n",
    "    \n",
    "    print [d['id'] for d in json.loads(resume_data)['specialization']] #для этого надо взять список всех специализаций. потом взять специализации по всем резюме и на их основе составить веса\n",
    "    print json.loads(resume_data)['salary']\n",
    "    print json.loads(resume_data)['employment']\n",
    "    print json.loads(resume_data)['schedules']\n",
    "    print json.loads(resume_data)['total_experience']\n",
    "    print json.loads(resume_data)['skill_set']\n",
    "    print json.loads(resume_data)['title'] #из этого достаем слова\n",
    "    print json.loads(resume_data)['skills']\n",
    "    \n",
    "    print features\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16823578\n",
      "found\n",
      "16823572\n",
      "found\n",
      "16823571\n",
      "found\n",
      "16823570\n",
      "found\n",
      "16823568\n",
      "found\n",
      "16823564\n",
      "found\n",
      "16823563\n",
      "found\n",
      "16823562\n",
      "found\n",
      "16823561\n",
      "found\n",
      "16823560\n",
      "found\n",
      "16823558\n",
      "found\n",
      "16823557\n",
      "found\n",
      "16823556\n",
      "found\n",
      "16823554\n",
      "found\n",
      "16823551\n",
      "found\n",
      "16823549\n",
      "found\n",
      "16823547\n",
      "found\n",
      "16823544\n",
      "found\n",
      "16823543\n",
      "found\n",
      "16823542\n",
      "found\n",
      "16823540\n",
      "found\n",
      "16823539\n",
      "found\n",
      "16823538\n",
      "found\n",
      "16823536\n",
      "found\n",
      "16823534\n",
      "found\n",
      "16823530\n",
      "found\n",
      "16823519\n",
      "found\n",
      "16823516\n",
      "found\n",
      "16823506\n",
      "found\n",
      "16823503\n",
      "found\n",
      "16823500\n",
      "found\n",
      "16823499\n",
      "found\n",
      "16823498\n",
      "found\n",
      "16823497\n",
      "found\n",
      "16823494\n",
      "found\n",
      "16823492\n",
      "found\n",
      "16823491\n",
      "found\n",
      "16823474\n",
      "found\n",
      "16823469\n",
      "found\n",
      "16823455\n",
      "found\n",
      "16823450\n",
      "found\n",
      "16823444\n",
      "found\n",
      "16823442\n",
      "found\n",
      "16823439\n",
      "found\n",
      "16823416\n",
      "found\n",
      "16823414\n",
      "found\n",
      "16823410\n",
      "found\n",
      "16823408\n",
      "found\n",
      "16823407\n",
      "found\n",
      "16823406\n",
      "found\n",
      "16823404\n",
      "found\n",
      "16823402\n",
      "found\n",
      "16823401\n",
      "found\n",
      "16823392\n",
      "found\n",
      "16823390\n",
      "found\n",
      "16823387\n",
      "found\n",
      "16823384\n",
      "found\n",
      "16823383\n",
      "found\n",
      "16823379\n",
      "found\n",
      "16823378\n",
      "found\n",
      "16823377\n",
      "found\n",
      "16823374\n",
      "found\n",
      "16823370\n",
      "found\n",
      "16823368\n",
      "found\n",
      "16823366\n",
      "found\n",
      "16823364\n",
      "found\n",
      "16823361\n",
      "found\n",
      "16823356\n",
      "found\n",
      "16823355\n",
      "found\n",
      "16823352\n",
      "found\n",
      "16823349\n",
      "found\n",
      "16823346\n",
      "found\n",
      "16823344\n",
      "found\n",
      "16823343\n",
      "found\n",
      "16823342\n",
      "found\n",
      "16823339\n",
      "found\n",
      "16823338\n",
      "found\n",
      "16823337\n",
      "found\n",
      "16823335\n",
      "found\n",
      "16823334\n",
      "found\n",
      "16823333\n",
      "found\n",
      "16823332\n",
      "found\n",
      "16823330\n",
      "found\n",
      "16823329\n",
      "found\n",
      "16823328\n",
      "found\n",
      "16823327\n",
      "found\n",
      "16823325\n",
      "found\n",
      "16823319\n",
      "found\n",
      "16823315\n",
      "found\n",
      "16823311\n",
      "found\n",
      "16823308\n",
      "found\n",
      "16823307\n",
      "found\n",
      "16823306\n",
      "found\n",
      "16823305\n",
      "found\n",
      "16823303\n",
      "found\n",
      "16823300\n",
      "found\n",
      "16823299\n",
      "found\n",
      "16823297\n",
      "found\n",
      "16823294\n",
      "found\n",
      "16823293\n",
      "found\n",
      "16823278\n",
      "found\n",
      "16823276\n",
      "found\n",
      "16823275\n",
      "found\n",
      "16823266\n",
      "found\n",
      "16823264\n",
      "found\n",
      "16823263\n",
      "found\n",
      "16823262\n",
      "found\n",
      "16823261\n",
      "found\n",
      "16823259\n",
      "found\n",
      "16823258\n",
      "found\n",
      "16823257\n",
      "found\n",
      "16823256\n",
      "found\n",
      "16823255\n",
      "found\n",
      "16823254\n",
      "found\n",
      "16823253\n",
      "found\n",
      "16823252\n",
      "found\n",
      "16823250\n",
      "found\n",
      "16823249\n",
      "found\n",
      "16823248\n",
      "found\n",
      "16823244\n",
      "found\n",
      "16823240\n",
      "found\n",
      "16823235\n",
      "found\n",
      "16823233\n",
      "found\n",
      "16823232\n",
      "found\n",
      "16823231\n",
      "found\n",
      "16823229\n",
      "found\n",
      "16823227\n",
      "found\n",
      "16823226\n",
      "found\n",
      "16823225\n",
      "found\n",
      "16823223\n",
      "found\n",
      "16823221\n",
      "found\n",
      "16823218\n",
      "found\n",
      "16823216\n",
      "found\n",
      "16823213\n",
      "found\n",
      "16823212\n",
      "found\n",
      "16823209\n",
      "found\n",
      "16823208\n",
      "found\n",
      "16823206\n",
      "found\n",
      "16823202\n",
      "found\n",
      "16823201\n",
      "found\n",
      "16823199\n",
      "found\n",
      "16823198\n",
      "found\n",
      "16823196\n",
      "found\n",
      "16823194\n",
      "found\n",
      "16823192\n",
      "found\n",
      "16823190\n",
      "found\n",
      "16823188\n",
      "found\n",
      "16823187\n",
      "found\n",
      "16823186\n",
      "found\n",
      "16823181\n",
      "found\n",
      "16823179\n",
      "found\n",
      "16823177\n",
      "found\n",
      "16823175\n",
      "found\n",
      "16823174\n",
      "found\n",
      "16823171\n",
      "found\n",
      "16823166\n",
      "found\n",
      "16823164\n",
      "found\n",
      "16823162\n",
      "found\n",
      "16823161\n",
      "found\n",
      "16823160\n",
      "found\n",
      "16823159\n",
      "found\n",
      "16823158\n",
      "found\n",
      "16823157\n",
      "found\n",
      "16823156\n",
      "found\n",
      "16823155\n",
      "found\n",
      "16823154\n",
      "found\n",
      "16823153\n",
      "found\n",
      "16823152\n",
      "found\n",
      "16823151\n",
      "found\n",
      "16823149\n",
      "found\n",
      "16823148\n",
      "found\n",
      "16823147\n",
      "found\n",
      "16823146\n",
      "found\n",
      "16823145\n",
      "found\n",
      "16823144\n",
      "found\n",
      "16823143\n",
      "found\n",
      "16823142\n",
      "found\n",
      "16823141\n",
      "found\n",
      "16823139\n",
      "found\n",
      "16823138\n",
      "found\n",
      "16823137\n",
      "found\n",
      "16823136\n",
      "found\n",
      "16823134\n",
      "found\n",
      "16823133\n",
      "found\n",
      "16823132\n",
      "found\n",
      "16823131\n",
      "found\n",
      "16823130\n",
      "found\n",
      "16823129\n",
      "found\n",
      "16823128\n",
      "found\n",
      "16823126\n",
      "found\n",
      "16823125\n",
      "found\n",
      "16823124\n",
      "found\n",
      "16823123\n",
      "found\n",
      "16823121\n",
      "found\n",
      "16823119\n",
      "found\n",
      "16823117\n",
      "found\n",
      "16823116\n",
      "found\n",
      "16823115\n",
      "found\n",
      "16823114\n",
      "found\n",
      "16823113\n",
      "found\n",
      "16823112\n",
      "found\n",
      "16823111\n",
      "found\n",
      "16823110\n",
      "found\n",
      "16823109\n",
      "found\n",
      "16823108\n",
      "found\n",
      "16823107\n",
      "found\n",
      "16823106\n",
      "found\n",
      "16823104\n",
      "found\n",
      "16823103\n",
      "found\n",
      "16823101\n",
      "found\n",
      "16823100\n",
      "found\n",
      "16823099\n",
      "found\n",
      "16823098\n",
      "found\n",
      "16823096\n",
      "found\n",
      "16823095\n",
      "found\n",
      "16823094\n",
      "found\n",
      "16823093\n",
      "found\n",
      "16823092\n",
      "found\n",
      "16823091\n",
      "found\n",
      "16823090\n",
      "found\n",
      "16823089\n",
      "found\n",
      "16823088\n",
      "found\n",
      "16823087\n",
      "found\n",
      "16823085\n",
      "found\n",
      "16823084\n",
      "found\n",
      "16823083\n",
      "found\n",
      "16823081\n",
      "found\n",
      "16823080\n",
      "found\n",
      "16823074\n",
      "found\n",
      "16823069\n",
      "found\n",
      "16823067\n",
      "found\n",
      "16823065\n",
      "found\n",
      "16823063\n",
      "found\n",
      "16823062\n",
      "found\n",
      "16823060\n",
      "found\n",
      "16823058\n",
      "found\n",
      "16823057\n",
      "found\n",
      "16823056\n",
      "found\n",
      "16823055\n",
      "found\n",
      "16823052\n",
      "found\n",
      "16823051\n",
      "found\n",
      "16823050\n",
      "found\n",
      "16823048\n",
      "found\n",
      "16823046\n",
      "found\n",
      "16823045\n",
      "found\n",
      "16823044\n",
      "found\n",
      "16823042\n",
      "found\n",
      "16823040\n",
      "found\n",
      "16823039\n",
      "found\n",
      "16823036\n",
      "found\n",
      "16823035\n",
      "found\n",
      "16823034\n",
      "found\n",
      "16823032\n",
      "found\n",
      "16823030\n",
      "found\n",
      "16823028\n",
      "found\n",
      "16823026\n",
      "found\n",
      "16823024\n",
      "found\n",
      "16823023\n",
      "found\n",
      "16823015\n",
      "found\n",
      "16823014\n",
      "found\n",
      "16823009\n",
      "found\n",
      "16823008\n",
      "found\n",
      "16823003\n",
      "found\n",
      "16823000\n"
     ]
    }
   ],
   "source": [
    "import httplib\n",
    "import json\n",
    "import pickle\n",
    "from tinydb import TinyDB\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "db = TinyDB('/home/shurik2533/vacancies_test.json')\n",
    "k = 0\n",
    "for i in range(16823582, 0, -1): #18822744\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"/vacancies/{0}\".format(i), headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "    k = k+1\n",
    "    if r1.status==200:\n",
    "        vacancy_data = r1.read()\n",
    "        #print json.loads(vacancy_data)['archived']\n",
    "        if json.loads(vacancy_data)['archived'] == False:\n",
    "            db.insert(json.loads(vacancy_data))\n",
    "            print json.loads(vacancy_data)['id']\n",
    "            k=0\n",
    "    if i%1000 == 0:\n",
    "        print i\n",
    "        break;\n",
    "    if k == 100:\n",
    "        print \"break\"\n",
    "        break;\n",
    "    \n",
    "#print db.all()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'7.14': u'\\u0410\\u0432\\u0442\\u043e\\u0437\\u0430\\u043f\\u0447\\u0430\\u0441\\u0442\\u0438', u'16.658': u'\\u041c\\u0443\\u043d\\u0438\\u0446\\u0438\\u043f\\u0430\\u043b\\u0438\\u0442\\u0435\\u0442', u'21.292': u'\\u0422\\u0430\\u043c\\u043e\\u0436\\u0435\\u043d\\u043d\\u043e\\u0435 \\u043e\\u0444\\u043e\\u0440\\u043c\\u043b\\u0435\\u043d\\u0438\\u0435', u'23.362': u'\\u042e\\u0440\\u0438\\u0441\\u043a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442', u'6.336': u'\\u0423\\u0447\\u0435\\u0442 \\u043a\\u0430\\u0434\\u0440\\u043e\\u0432', u'23.165': u'\\u041d\\u0430\\u043b\\u043e\\u0433\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'5.61': u'\\u0414\\u0435\\u043d\\u0435\\u0436\\u043d\\u044b\\u0439 \\u0440\\u044b\\u043d\\u043e\\u043a (money market)', u'17.623': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0435 \\u0443\\u0441\\u043b\\u0443\\u0433\\u0438', u'1.10': u'Web \\u043c\\u0430\\u0441\\u0442\\u0435\\u0440', u'3.166': u'\\u041d\\u0430\\u0440\\u0443\\u0436\\u043d\\u0430\\u044f \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'5.367': u'\\u041c\\u0435\\u0442\\u043e\\u0434\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f, \\u0411\\u0430\\u043d\\u043a\\u043e\\u0432\\u0441\\u043a\\u0438\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'5.366': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043f\\u0440\\u043e\\u0447\\u0438\\u0435', u'5.365': u'\\u041e\\u0446\\u0435\\u043d\\u043a\\u0430 \\u0437\\u0430\\u043b\\u043e\\u0433\\u0430, \\u0421\\u0442\\u043e\\u0438\\u043c\\u043e\\u0441\\u0442\\u0438 \\u0438\\u043c\\u0443\\u0449\\u0435\\u0441\\u0442\\u0432\\u0430', u'5.369': u'\\u041e\\u0442\\u0447\\u0435\\u0442\\u043d\\u043e\\u0441\\u0442\\u044c', u'5.368': u'\\u0420\\u0430\\u0437\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430 \\u043d\\u043e\\u0432\\u044b\\u0445 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u043e\\u0432, \\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'13.268': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'22.518': u'\\u0421\\u043e\\u043c\\u0435\\u043b\\u044c\\u0435', u'22.104': u'\\u041a\\u0435\\u0439\\u0442\\u0435\\u0440\\u0438\\u043d\\u0433', u'29.560': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043c\\u043e\\u043d\\u0442\\u0435\\u0440, \\u041a\\u0430\\u0431\\u0435\\u043b\\u044c\\u0449\\u0438\\u043a', u'29.561': u'\\u042e\\u0432\\u0435\\u043b\\u0438\\u0440', u'11.436': u'\\u041a\\u0438\\u043d\\u043e', u'18.57': u'\\u0413\\u043b\\u0430\\u0432\\u043d\\u044b\\u0439 \\u0438\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'18.56': u'\\u0413\\u043b\\u0430\\u0432\\u043d\\u044b\\u0439 \\u0430\\u0433\\u0440\\u043e\\u043d\\u043e\\u043c', u'22.193': u'\\u041e\\u0444\\u0438\\u0446\\u0438\\u0430\\u043d\\u0442, \\u0411\\u0430\\u0440\\u043c\\u0435\\u043d', u'18.174': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'22.198': u'\\u041e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f \\u0432\\u0441\\u0442\\u0440\\u0435\\u0447, \\u041a\\u043e\\u043d\\u0444\\u0435\\u0440\\u0435\\u043d\\u0446\\u0438\\u0439', u'22.199': u'\\u041e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f \\u0442\\u0443\\u0440\\u0438\\u0441\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u043e\\u0432', u'7.455': u'\\u0428\\u0438\\u043d\\u044b, \\u0414\\u0438\\u0441\\u043a\\u0438', u'22.204': u'\\u041f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b \\u043a\\u0443\\u0445\\u043d\\u0438', u'15.308': u'\\u0422\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u0440\\u0442, \\u041b\\u043e\\u0433\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'14.60': u'\\u0413\\u0443\\u043c\\u0430\\u043d\\u0438\\u0442\\u0430\\u0440\\u043d\\u044b\\u0435 \\u043d\\u0430\\u0443\\u043a\\u0438', u'24.493': u'\\u041f\\u0430\\u0440\\u0438\\u043a\\u043c\\u0430\\u0445\\u0435\\u0440', u'24.492': u'\\u041c\\u0430\\u0441\\u0441\\u0430\\u0436\\u0438\\u0441\\u0442', u'9.22': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'19.421': u'\\u0410\\u043a\\u0442\\u0443\\u0430\\u0440\\u0438\\u0439', u'3.236': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u044b', u'3.230': u'\\u041f\\u0440\\u043e\\u0434\\u0432\\u0438\\u0436\\u0435\\u043d\\u0438\\u0435, \\u0421\\u043f\\u0435\\u0446\\u0438\\u0430\\u043b\\u044c\\u043d\\u044b\\u0435 \\u043c\\u0435\\u0440\\u043e\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u044f', u'2.523': u'CIPA', u'12.326': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'12.322': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u0430\\u043a\\u0442\\u0438\\u043a\\u043e\\u0439', u'16.435': u'\\u041d\\u0418\\u0418', u'11.62': u'\\u0414\\u0438\\u0437\\u0430\\u0439\\u043d, \\u0433\\u0440\\u0430\\u0444\\u0438\\u043a\\u0430, \\u0436\\u0438\\u0432\\u043e\\u043f\\u0438\\u0441\\u044c', u'17.269': u'\\u0422\\u0435\\u043b\\u0435\\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438, \\u0421\\u0435\\u0442\\u0435\\u0432\\u044b\\u0435 \\u0440\\u0435\\u0448\\u0435\\u043d\\u0438\\u044f', u'1.232': u'\\u041f\\u0440\\u043e\\u0434\\u044e\\u0441\\u0435\\u0440', u'5.121': u'\\u041a\\u043e\\u0440\\u043f\\u043e\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u043e\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'5.123': u'\\u041a\\u043e\\u0440\\u0440\\u0435\\u0441\\u043f\\u043e\\u043d\\u0434\\u0435\\u043d\\u0442\\u0441\\u043a\\u0438\\u0435, \\u041c\\u0435\\u0436\\u0434\\u0443\\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u044b\\u0435 \\u043e\\u0442\\u043d\\u043e\\u0448\\u0435\\u043d\\u0438\\u044f', u'5.124': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043d\\u044b\\u0435', u'5.126': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u044b', u'2.454': u'\\u041c\\u0421\\u0424\\u041e, IFRS', u'4.181': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'18.263': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u043f\\u0440\\u0435\\u0434\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u0435\\u043c', u'2.351': u'\\u0426\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0431\\u0443\\u043c\\u0430\\u0433\\u0438', u'24.380': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'23.36': u'\\u0411\\u0430\\u043d\\u043a\\u043e\\u0432\\u0441\\u043a\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'3.305': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u044b\\u0439 \\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433(Trade marketing)', u'26.416': u'FMCG, \\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0433\\u043e \\u043f\\u043e\\u0442\\u0440\\u0435\\u0431\\u043b\\u0435\\u043d\\u0438\\u044f', u'26.415': u'\\u0425\\u0438\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0446\\u0438\\u044f', u'26.414': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u0438\\u043a\\u0430, \\u0444\\u043e\\u0442\\u043e, \\u0432\\u0438\\u0434\\u0435\\u043e', u'26.413': u'\\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u0434\\u043b\\u044f \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'26.412': u'\\u041f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u044b \\u043f\\u0438\\u0442\\u0430\\u043d\\u0438\\u044f', u'26.411': u'\\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'26.410': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u0430\\u044f \\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'9.312': u'\\u0422\\u0443\\u0440\\u0438\\u0437\\u043c, \\u0413\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u044b, \\u0420\\u0435\\u0441\\u0442\\u043e\\u0440\\u0430\\u043d\\u044b', u'9.317': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043c\\u0430\\u043b\\u044b\\u043c \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u043e\\u043c', u'26.419': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'5.234': u'\\u041f\\u0440\\u043e\\u0435\\u043a\\u0442\\u043d\\u043e\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.420': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u043e\\u0440 \\u0431\\u0430\\u0437 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445', u'3.114': u'\\u041a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0430\\u043d\\u0442', u'23.539': u'\\u0410\\u043d\\u0442\\u0438\\u043c\\u043e\\u043d\\u043e\\u043f\\u043e\\u043b\\u044c\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'18.190': u'\\u041d\\u0435\\u0444\\u0442\\u0435\\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'14.37': u'\\u0411\\u0438\\u043e\\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'15.389': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'15.388': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b', u'23.120': u'\\u041a\\u043e\\u0440\\u043f\\u043e\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'12.331': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u043a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'11.134': u'\\u041b\\u0438\\u0442\\u0435\\u0440\\u0430\\u0442\\u0443\\u0440\\u043d\\u0430\\u044f, \\u0420\\u0435\\u0434\\u0430\\u043a\\u0442\\u043e\\u0440\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'5.241': u'\\u041f\\u0440\\u044f\\u043c\\u044b\\u0435 \\u0438\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'5.534': u'\\u0424\\u0430\\u043a\\u0442\\u043e\\u0440\\u0438\\u043d\\u0433', u'18.299': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0438 \\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430 \\u0437\\u0435\\u0440\\u043d\\u043e\\u0432\\u044b\\u0445', u'18.290': u'\\u0421\\u0442\\u0440\\u043e\\u0439\\u043c\\u0430\\u0442\\u0435\\u0440\\u0438\\u0430\\u043b\\u044b', u'14.178': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'18.297': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433', u'18.13': u'\\u0410\\u0432\\u0438\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'13.438': u'\\u041e\\u043f\\u0442\\u0438\\u043a\\u0430', u'18.16': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u044c\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'13.433': u'\\u0420\\u0435\\u0433\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u0443\\u0440\\u0430', u'13.432': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'6.247': u'\\u0420\\u0430\\u0437\\u0432\\u0438\\u0442\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u0430', u'17.441': u'\\u0424\\u0440\\u0430\\u043d\\u0447\\u0430\\u0439\\u0437\\u0438\\u043d\\u0433', u'17.443': u'\\u041c\\u0435\\u0431\\u0435\\u043b\\u044c', u'17.446': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u0438', u'1.161': u'\\u041c\\u0443\\u043b\\u044c\\u0442\\u0438\\u043c\\u0435\\u0434\\u0438\\u0430', u'18.129': u'\\u041b\\u0435\\u0433\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'21.310': u'\\u0422\\u0440\\u0443\\u0431\\u043e\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u044b', u'22.329': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0440\\u0435\\u0441\\u0442\\u043e\\u0440\\u0430\\u043d\\u0430\\u043c\\u0438, \\u0411\\u0430\\u0440\\u0430\\u043c\\u0438', u'15.93': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438, \\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442, \\u041c\\u0443\\u043b\\u044c\\u0442\\u0438\\u043c\\u0435\\u0434\\u0438\\u0430', u'22.175': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'2.179': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.88': u'\\u0418\\u043d\\u0442\\u0435\\u043b\\u043b\\u0435\\u043a\\u0442\\u0443\\u0430\\u043b\\u044c\\u043d\\u0430\\u044f \\u0441\\u043e\\u0431\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'20.396': u'\\u042d\\u043a\\u0441\\u043f\\u043b\\u0443\\u0430\\u0442\\u0430\\u0446\\u0438\\u044f', u'13.489': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u0438\\u0439 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c', u'6.107': u'\\u041a\\u043e\\u043c\\u043f\\u0435\\u043d\\u0441\\u0430\\u0446\\u0438\\u0438 \\u0438 \\u043b\\u044c\\u0433\\u043e\\u0442\\u044b', u'9.532': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0437\\u0430\\u043a\\u0443\\u043f\\u043a\\u0430\\u043c\\u0438', u'5.4': u'Forex', u'18.81': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'18.86': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0441\\u0430\\u0445\\u0430\\u0440\\u0430', u'7.566': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0439\\u0449\\u0438\\u043a', u'7.565': u'\\u0410\\u0432\\u0442\\u043e\\u0436\\u0435\\u0441\\u0442\\u044f\\u043d\\u0449\\u0438\\u043a', u'18.85': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0438 \\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430 \\u0437\\u0435\\u0440\\u043d\\u043e\\u0432\\u044b\\u0445', u'8.575': u'\\u041f\\u043e\\u0436\\u0430\\u0440\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'27.533': u'\\u041f\\u043e\\u043c\\u043e\\u0449\\u043d\\u0438\\u043a \\u043f\\u043e \\u0445\\u043e\\u0437\\u044f\\u0439\\u0441\\u0442\\u0432\\u0443, \\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u044f\\u044e\\u0449\\u0438\\u0439', u'10.472': u'\\u0413\\u0430\\u0437', u'3.40': u'\\u0411\\u0440\\u0435\\u043d\\u0434-\\u043c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442', u'26.427': u'\\u041c\\u0435\\u0442\\u0430\\u043b\\u043b\\u043e\\u043f\\u0440\\u043e\\u043a\\u0430\\u0442', u'5.399': u'\\u0418\\u043f\\u043e\\u0442\\u0435\\u043a\\u0430, \\u0418\\u043f\\u043e\\u0442\\u0435\\u0447\\u043d\\u043e\\u0435 \\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'3.48': u'\\u0412\\u0435\\u0440\\u0441\\u0442\\u0430\\u043b\\u044c\\u0449\\u0438\\u043a', u'12.376': u'\\u041d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u044c', u'18.339': u'\\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'15.167': u'\\u041d\\u0430\\u0443\\u043a\\u0430, \\u041e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'13.155': u'\\u041c\\u043b\\u0430\\u0434\\u0448\\u0438\\u0439 \\u0438 \\u0441\\u0440\\u0435\\u0434\\u043d\\u0438\\u0439 \\u043c\\u0435\\u0434\\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b', u'1.89': u'\\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442', u'17.59': u'\\u0413\\u0421\\u041c, \\u043d\\u0435\\u0444\\u0442\\u044c, \\u0431\\u0435\\u043d\\u0437\\u0438\\u043d', u'12.97': u'\\u0418\\u0441\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u0440\\u044b\\u043d\\u043a\\u0430', u'5.192': u'\\u041e\\u0431\\u043c\\u0435\\u043d\\u043d\\u044b\\u0435 \\u043f\\u0443\\u043d\\u043a\\u0442\\u044b, \\u0411\\u0430\\u043d\\u043a\\u043e\\u043c\\u0430\\u0442\\u044b', u'23.72': u'\\u0417\\u0430\\u043a\\u043e\\u043d\\u043e\\u0442\\u0432\\u043e\\u0440\\u0447\\u0435\\u0441\\u0442\\u0432\\u043e', u'5.195': u'\\u041e\\u041f\\u0415\\u0420\\u0423', u'19.18': u'\\u0410\\u0432\\u0442\\u043e\\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'5.691': u'Private Banking', u'17.538': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438 \\u043f\\u043e \\u0442\\u0435\\u043b\\u0435\\u0444\\u043e\\u043d\\u0443, \\u0422\\u0435\\u043b\\u0435\\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'5.177': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.476': u'\\u0417\\u0435\\u043c\\u0435\\u043b\\u044c\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'23.477': u'\\u0414\\u043e\\u0433\\u043e\\u0432\\u043e\\u0440\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'23.478': u'\\u0420\\u0435\\u0433\\u0438\\u0441\\u0442\\u0440\\u0430\\u0446\\u0438\\u044f \\u044e\\u0440\\u0438\\u0434\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043b\\u0438\\u0446', u'23.479': u'\\u0412\\u0437\\u044b\\u0441\\u043a\\u0430\\u043d\\u0438\\u0435 \\u0437\\u0430\\u0434\\u043e\\u043b\\u0436\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438, \\u041a\\u043e\\u043b\\u043b\\u0435\\u043a\\u0442\\u043e\\u0440\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'14.141': u'\\u041c\\u0430\\u0442\\u0435\\u043c\\u0430\\u0442\\u0438\\u043a\\u0430', u'17.535': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u044b\\u0435 \\u0441\\u0435\\u0442\\u0438', u'11.173': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'24.377': u'\\u041a\\u043e\\u0441\\u043c\\u0435\\u0442\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f', u'12.7': u'PR Consulting', u'9.105': u'\\u041a\\u043e\\u043c\\u043c\\u0435\\u0440\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0411\\u0430\\u043d\\u043a', u'12.5': u'Internet, E-Commerce', u'4.32': u'\\u0410\\u0440\\u0445\\u0438\\u0432\\u0438\\u0441\\u0442', u'24.378': u'\\u0422\\u0440\\u0435\\u043d\\u0435\\u0440\\u0441\\u043a\\u0438\\u0439 \\u0441\\u043e\\u0441\\u0442\\u0430\\u0432', u'24.379': u'\\u0410\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0446\\u0438\\u044f', u'20.58': u'\\u0413\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u044b, \\u041c\\u0430\\u0433\\u0430\\u0437\\u0438\\u043d\\u044b', u'21.506': u'\\u042d\\u043a\\u0441\\u043f\\u0435\\u0434\\u0438\\u0442\\u043e\\u0440', u'21.482': u'\\u0412\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c', u'22.55': u'\\u0413\\u0438\\u0434, \\u042d\\u043a\\u0441\\u043a\\u0443\\u0440\\u0441\\u043e\\u0432\\u043e\\u0434', u'21.481': u'\\u0414\\u0438\\u0441\\u043f\\u0435\\u0442\\u0447\\u0435\\u0440', u'19.284': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u0438', u'9.448': u'\\u042e\\u0440\\u0438\\u0441\\u043f\\u0440\\u0443\\u0434\\u0435\\u043d\\u0446\\u0438\\u044f', u'5.579': u'\\u0420\\u0430\\u0431\\u043e\\u0442\\u0430 \\u0441 \\u043f\\u0440\\u043e\\u0431\\u043b\\u0435\\u043c\\u043d\\u044b\\u043c\\u0438 \\u0437\\u0430\\u0435\\u043c\\u0449\\u0438\\u043a\\u0430\\u043c\\u0438', u'20.233': u'\\u041f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0410\\u0440\\u0445\\u0438\\u0442\\u0435\\u043a\\u0442\\u0443\\u0440\\u0430', u'5.133': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043b\\u0438\\u0437\\u0438\\u043d\\u0433\\u043e\\u0432\\u044b\\u0435', u'10.315': u'\\u0423\\u0433\\u043e\\u043b\\u044c', u'10.258': u'\\u0420\\u0443\\u0434\\u0430', u'4.264': u'\\u0421\\u0435\\u043a\\u0440\\u0435\\u0442\\u0430\\u0440\\u044c', u'13.578': u'\\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442', u'5.304': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u043e\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'9.67': u'\\u0414\\u043e\\u0431\\u044b\\u0447\\u0430 c\\u044b\\u0440\\u044c\\u044f', u'17.401': u'\\u0425\\u0438\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0446\\u0438\\u044f', u'5.51': u'\\u0412\\u043d\\u0443\\u0442\\u0440\\u0435\\u043d\\u043d\\u0438\\u0435 \\u043e\\u043f\\u0435\\u0440\\u0430\\u0446\\u0438\\u0438 (Back Office)', u'22.353': u'\\u0428\\u0435\\u0444-\\u043f\\u043e\\u0432\\u0430\\u0440', u'3.171': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c/\\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'12.180': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.276': u'\\u0421\\u043b\\u0438\\u044f\\u043d\\u0438\\u044f \\u0438 \\u043f\\u043e\\u0433\\u043b\\u043e\\u0449\\u0435\\u043d\\u0438\\u044f', u'22.491': u'\\u041f\\u043e\\u0432\\u0430\\u0440', u'18.212': u'\\u041f\\u043e\\u043b\\u0438\\u0433\\u0440\\u0430\\u0444\\u0438\\u044f', u'19.259': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c \\u043d\\u0430\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u044f', u'21.564': u'\\u0420\\u0430\\u0431\\u043e\\u0447\\u0438\\u0439 \\u0441\\u043a\\u043b\\u0430\\u0434\\u0430', u'1.395': u'\\u0411\\u0430\\u043d\\u043a\\u043e\\u0432\\u0441\\u043a\\u043e\\u0435 \\u041f\\u041e', u'5.370': u'\\u041a\\u0430\\u0437\\u043d\\u0430\\u0447\\u0435\\u0439\\u0441\\u0442\\u0432\\u043e, \\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043b\\u0438\\u043a\\u0432\\u0438\\u0434\\u043d\\u043e\\u0441\\u0442\\u044c\\u044e', u'23.286': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'5.372': u'\\u0411\\u044e\\u0434\\u0436\\u0435\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'29.495': u'\\u0413\\u0440\\u0443\\u0437\\u0447\\u0438\\u043a', u'16.216': u'\\u041f\\u0440\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e', u'22.505': u'\\u0428\\u0432\\u0435\\u0439\\u0446\\u0430\\u0440', u'21.136': u'\\u041b\\u043e\\u0433\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'1.137': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'9.145': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0430, \\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'8.77': u'\\u0418\\u043c\\u0443\\u0449\\u0435\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'29.513': u'\\u0421\\u0442\\u043e\\u043b\\u044f\\u0440', u'29.512': u'\\u0424\\u0430\\u0441\\u043e\\u0432\\u0449\\u0438\\u043a', u'2.33': u'\\u0410\\u0443\\u0434\\u0438\\u0442', u'29.515': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u0438\\u043a', u'8.202': u'\\u041e\\u0445\\u0440\\u0430\\u043d\\u043d\\u0438\\u043a', u'16.38': u'\\u0411\\u043b\\u0430\\u0433\\u043e\\u0442\\u0432\\u043e\\u0440\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'13.587': u'\\u0412\\u0440\\u0430\\u0447-\\u044d\\u043a\\u0441\\u043f\\u0435\\u0440\\u0442', u'29.516': u'\\u0428\\u0432\\u0435\\u044f', u'15.363': u'\\u042e\\u0440\\u0438\\u0441\\u0442\\u044b', u'21.74': u'\\u0417\\u0430\\u043a\\u0443\\u043f\\u043a\\u0438, \\u0421\\u043d\\u0430\\u0431\\u0436\\u0435\\u043d\\u0438\\u0435', u'19.430': u'\\u0423\\u0440\\u0435\\u0433\\u0443\\u043b\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0443\\u0431\\u044b\\u0442\\u043a\\u043e\\u0432', u'24.624': u'\\u041d\\u043e\\u0433\\u0442\\u0435\\u0432\\u043e\\u0439 \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441', u'2.125': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043d\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'3.90': u'\\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442-\\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'7.239': u'\\u041f\\u0440\\u043e\\u043a\\u0430\\u0442, \\u043b\\u0438\\u0437\\u0438\\u043d\\u0433', u'9.321': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u043e\\u043c, \\u0422\\u0440\\u0435\\u043d\\u0438\\u043d\\u0433\\u0438', u'3.98': u'\\u0418\\u0441\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f \\u0440\\u044b\\u043d\\u043a\\u0430', u'14.349': u'\\u0425\\u0438\\u043c\\u0438\\u044f', u'7.235': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'11.71': u'\\u0416\\u0443\\u0440\\u043d\\u0430\\u043b\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'11.76': u'\\u0418\\u0437\\u0434\\u0430\\u0442\\u0435\\u043b\\u044c\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'19.19': u'\\u0410\\u0433\\u0435\\u043d\\u0442', u'14.340': u'\\u0424\\u0438\\u0437\\u0438\\u043a\\u0430', u'12.197': u'\\u041e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u043e\\u0435 \\u043a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.221': u'\\u041f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0420\\u0430\\u0437\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'1.225': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'5.132': u'\\u041b\\u0438\\u0437\\u0438\\u043d\\u0433', u'20.573': u'\\u0422\\u0435\\u043d\\u0434\\u0435\\u0440\\u044b', u'15.730': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440\\u0438\\u044f', u'19.282': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'19.283': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0436\\u0438\\u0437\\u043d\\u0438', u'2.463': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440-\\u043a\\u0430\\u043b\\u044c\\u043a\\u0443\\u043b\\u044f\\u0442\\u043e\\u0440', u'22.11': u'\\u0410\\u0432\\u0438\\u0430\\u0431\\u0438\\u043b\\u0435\\u0442\\u044b', u'2.467': u'ACCA', u'2.465': u'GAAP', u'2.464': u'\\u041e\\u0441\\u043d\\u043e\\u0432\\u043d\\u044b\\u0435 \\u0441\\u0440\\u0435\\u0434\\u0441\\u0442\\u0432\\u0430', u'2.342': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u0430\\u043d\\u0430\\u043b\\u0438\\u0437', u'2.343': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'2.469': u'\\u041f\\u0435\\u0440\\u0432\\u0438\\u0447\\u043d\\u0430\\u044f \\u0434\\u043e\\u043a\\u0443\\u043c\\u0435\\u043d\\u0442\\u0430\\u0446\\u0438\\u044f', u'2.468': u'\\u0422\\u041c\\u0426', u'2.344': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u043c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442', u'11.99': u'\\u041a\\u0430\\u0437\\u0438\\u043d\\u043e \\u0438 \\u0438\\u0433\\u043e\\u0440\\u043d\\u044b\\u0439 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441', u'3.318': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433\\u043e\\u043c', u'3.253': u'\\u0420\\u0435\\u043a\\u043b\\u0430\\u043c\\u043d\\u044b\\u0439 \\u0430\\u0433\\u0435\\u043d\\u0442', u'23.21': u'\\u0410\\u0434\\u0432\\u043e\\u043a\\u0430\\u0442', u'23.29': u'\\u0410\\u0440\\u0431\\u0438\\u0442\\u0440\\u0430\\u0436', u'20.83': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'20.375': u'\\u0414\\u0435\\u0432\\u0435\\u043b\\u043e\\u043f\\u0435\\u0440', u'17.303': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u043b\\u044f \\u0431\\u0438\\u0440\\u0436\\u0435\\u0432\\u044b\\u043c\\u0438 \\u0442\\u043e\\u0432\\u0430\\u0440\\u0430\\u043c\\u0438', u'17.302': u'FMCG, \\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0433\\u043e \\u043f\\u043e\\u0442\\u0440\\u0435\\u0431\\u043b\\u0435\\u043d\\u0438\\u044f', u'17.301': u'\\u0422\\u043e\\u0432\\u0430\\u0440\\u044b \\u0434\\u043b\\u044f \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'17.306': u'\\u0422\\u043e\\u0440\\u0433\\u043e\\u0432\\u044b\\u0439 \\u043f\\u0440\\u0435\\u0434\\u0441\\u0442\\u0430\\u0432\\u0438\\u0442\\u0435\\u043b\\u044c', u'5.224': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0445 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u043e\\u0432', u'5.341': u'\\u0424\\u0438\\u043b\\u0438\\u0430\\u043b\\u044b', u'14.87': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440\\u043d\\u044b\\u0435 \\u043d\\u0430\\u0443\\u043a\\u0438', u'17.111': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u0430\\u044f \\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'1.296': u'\\u0422\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u043f\\u0438\\u0441\\u0430\\u0442\\u0435\\u043b\\u044c', u'1.295': u'\\u0422\\u0435\\u043b\\u0435\\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438', u'4.47': u'\\u0412\\u0432\\u043e\\u0434 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445', u'1.359': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u043d\\u0430\\u044f \\u043a\\u043e\\u043c\\u043c\\u0435\\u0440\\u0446\\u0438\\u044f', u'15.146': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0430, \\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'4.127': u'\\u041a\\u0443\\u0440\\u044c\\u0435\\u0440', u'18.451': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'18.453': u'\\u042e\\u0432\\u0435\\u043b\\u0438\\u0440\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'10.323': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u0435\\u0434\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u0435\\u043c', u'17.572': u'\\u0422\\u0435\\u043d\\u0434\\u0435\\u0440\\u044b', u'1.3': u'CTO, CIO, \\u0414\\u0438\\u0440\\u0435\\u043a\\u0442\\u043e\\u0440 \\u043f\\u043e IT', u'1.9': u'Web \\u0438\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'12.92': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'6.254': u'\\u0420\\u0435\\u043a\\u0440\\u0443\\u0442\\u043c\\u0435\\u043d\\u0442', u'1.172': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'6.319': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u043e\\u043c', u'15.320': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u043e\\u043c', u'3.148': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0440\\u0430\\u0431\\u043e\\u0442\\u0435 \\u0441 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442\\u0430\\u043c\\u0438', u'19.109': u'\\u041a\\u043e\\u043c\\u043f\\u043b\\u0435\\u043a\\u0441\\u043d\\u043e\\u0435 \\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u044e\\u0440\\u0438\\u0434\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043b\\u0438\\u0446', u'19.108': u'\\u041a\\u043e\\u043c\\u043f\\u043b\\u0435\\u043a\\u0441\\u043d\\u043e\\u0435 \\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0444\\u0438\\u0437\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u043b\\u0438\\u0446', u'15.281': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'17.112': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u044b\\u0435 \\u043f\\u0440\\u043e\\u0433\\u0440\\u0430\\u043c\\u043c\\u044b', u'1.30': u'\\u0410\\u0440\\u0442-\\u0434\\u0438\\u0440\\u0435\\u043a\\u0442\\u043e\\u0440', u'15.731': u'\\u0417\\u0430\\u043a\\u0443\\u043f\\u043a\\u0438', u'22.316': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0433\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u0430\\u043c\\u0438', u'13.537': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u0438\\u0439 \\u0441\\u043e\\u0432\\u0435\\u0442\\u043d\\u0438\\u043a', u'2.164': u'\\u041d\\u0430\\u043b\\u043e\\u0433\\u0438', u'29.546': u'\\u041a\\u0440\\u0430\\u0441\\u043d\\u043e\\u0434\\u0435\\u0440\\u0435\\u0432\\u0449\\u0438\\u043a', u'29.547': u'\\u041a\\u0443\\u0437\\u043d\\u0435\\u0446', u'2.249': u'\\u0420\\u0430\\u0441\\u0447\\u0435\\u0442 \\u0441\\u0435\\u0431\\u0435\\u0441\\u0442\\u043e\\u0438\\u043c\\u043e\\u0441\\u0442\\u0438', u'29.545': u'\\u041a\\u043e\\u043c\\u043f\\u043b\\u0435\\u043a\\u0442\\u043e\\u0432\\u0449\\u0438\\u043a, \\u0423\\u043a\\u043b\\u0430\\u0434\\u0447\\u0438\\u043a-\\u0443\\u043f\\u0430\\u043a\\u043e\\u0432\\u0449\\u0438\\u043a', u'29.542': u'\\u0414\\u043e\\u0440\\u043e\\u0436\\u043d\\u044b\\u0435 \\u0440\\u0430\\u0431\\u043e\\u0447\\u0438\\u0435', u'29.543': u'\\u0416\\u0435\\u0441\\u0442\\u044f\\u043d\\u0449\\u0438\\u043a', u'29.540': u'\\u0413\\u0430\\u0440\\u0434\\u0435\\u0440\\u043e\\u0431\\u0449\\u0438\\u043a', u'29.541': u'\\u0414\\u0432\\u043e\\u0440\\u043d\\u0438\\u043a, \\u0423\\u0431\\u043e\\u0440\\u0449\\u0438\\u043a', u'29.548': u'\\u041b\\u0438\\u0444\\u0442\\u0435\\u0440', u'29.549': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u0438\\u0441\\u0442 \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u0430', u'27.500': u'\\u041f\\u043e\\u0432\\u0430\\u0440', u'27.501': u'\\u0421\\u0438\\u0434\\u0435\\u043b\\u043a\\u0430', u'7.503': u'\\u0410\\u0432\\u0442\\u043e\\u0441\\u043b\\u0435\\u0441\\u0430\\u0440\\u044c', u'19.285': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043e\\u0442\\u0432\\u0435\\u0442\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438', u'10.80': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'18.142': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u043e\\u0441\\u0442\\u0440\\u043e\\u0435\\u043d\\u0438\\u0435', u'21.176': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'2.425': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0441\\u0442', u'26.439': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u043c\\u0430\\u0442\\u0435\\u0440\\u0438\\u0430\\u043b\\u044b', u'26.437': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0437\\u0430\\u043a\\u0443\\u043f\\u043a\\u0430\\u043c\\u0438', u'10.393': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'13.398': u'\\u041a\\u043b\\u0438\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435 \\u0438\\u0441\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f', u'3.213': u'\\u041f\\u043e\\u043b\\u0438\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 PR', u'20.525': u'\\u0417\\u0435\\u043c\\u043b\\u0435\\u0443\\u0441\\u0442\\u0440\\u043e\\u0439\\u0441\\u0442\\u0432\\u043e', u'20.527': u'\\u041e\\u0442\\u043e\\u043f\\u043b\\u0435\\u043d\\u0438\\u0435, \\u0432\\u0435\\u043d\\u0442\\u0438\\u043b\\u044f\\u0446\\u0438\\u044f \\u0438 \\u043a\\u043e\\u043d\\u0434\\u0438\\u0446\\u0438\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'20.526': u'\\u041e\\u0446\\u0435\\u043d\\u043a\\u0430', u'5.262': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0431\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440\\u0438\\u0435\\u0439', u'17.242': u'\\u041f\\u0440\\u044f\\u043c\\u044b\\u0435 \\u043f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'20.528': u'\\u0412\\u043e\\u0434\\u043e\\u0441\\u043d\\u0430\\u0431\\u0436\\u0435\\u043d\\u0438\\u0435 \\u0438 \\u043a\\u0430\\u043d\\u0430\\u043b\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f', u'20.445': u'\\u0416\\u041a\\u0425', u'20.63': u'\\u0414\\u0438\\u0437\\u0430\\u0439\\u043d/\\u041e\\u0444\\u043e\\u0440\\u043c\\u043b\\u0435\\u043d\\u0438\\u0435', u'11.160': u'\\u041c\\u0443\\u0437\\u044b\\u043a\\u0430', u'18.360': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u044d\\u043d\\u0435\\u0440\\u0433\\u0435\\u0442\\u0438\\u043a\\u0430', u'18.361': u'\\u042d\\u043d\\u0435\\u0440\\u0433\\u0435\\u0442\\u0438\\u043a \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u0430', u'2.337': u'\\u0423\\u0447\\u0435\\u0442 \\u0441\\u0447\\u0435\\u0442\\u043e\\u0432 \\u0438 \\u043f\\u043b\\u0430\\u0442\\u0435\\u0436\\u0435\\u0439', u'2.335': u'\\u0423\\u0447\\u0435\\u0442 \\u0437\\u0430\\u0440\\u0430\\u0431\\u043e\\u0442\\u043d\\u043e\\u0439 \\u043f\\u043b\\u0430\\u0442\\u044b', u'1.82': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'4.271': u'\\u0421\\u0438\\u043d\\u0445\\u0440\\u043e\\u043d\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0434', u'13.49': u'\\u0412\\u0435\\u0442\\u0435\\u0440\\u0438\\u043d\\u0430\\u0440\\u0438\\u044f', u'9.289': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e, \\u041d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u044c', u'11.240': u'\\u041f\\u0440\\u043e\\u0447\\u0435\\u0435', u'11.243': u'\\u0420\\u0430\\u0434\\u0438\\u043e', u'4.278': u'\\u0421\\u043e\\u0442\\u0440\\u0443\\u0434\\u043d\\u0438\\u043a call-\\u0446\\u0435\\u043d\\u0442\\u0440\\u0430', u'3.328': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'17.333': u'\\u0423\\u0441\\u043b\\u0443\\u0433\\u0438 \\u0434\\u043b\\u044f \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'29.556': u'\\u0420\\u0430\\u0437\\u043d\\u043e\\u0440\\u0430\\u0431\\u043e\\u0447\\u0438\\u0439', u'17.334': u'\\u0423\\u0441\\u043b\\u0443\\u0433\\u0438 \\u0434\\u043b\\u044f \\u043d\\u0430\\u0441\\u0435\\u043b\\u0435\\u043d\\u0438\\u044f', u'4.374': u'\\u0410\\u0425\\u041e', u'17.231': u'\\u041f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u044b \\u043f\\u0438\\u0442\\u0430\\u043d\\u0438\\u044f', u'5.460': u'\\u0410\\u0432\\u0442\\u043e\\u043a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'12.6': u'Knowledge management', u'5.41': u'\\u0411\\u0443\\u043c\\u0430\\u0433\\u0438 \\u0441 \\u0444\\u0438\\u043a\\u0441\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043d\\u043e\\u0439 \\u0434\\u043e\\u0445\\u043e\\u0434\\u043d\\u043e\\u0441\\u0442\\u044c\\u044e (fixed Income)', u'17.144': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0430, \\u0424\\u0430\\u0440\\u043c\\u0430\\u0446\\u0435\\u0432\\u0442\\u0438\\u043a\\u0430', u'5.42': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440', u'5.45': u'\\u0412\\u0430\\u043b\\u044e\\u0442\\u043d\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'4.456': u'\\u0412\\u0435\\u0447\\u0435\\u0440\\u043d\\u0438\\u0439 \\u0441\\u0435\\u043a\\u0440\\u0435\\u0442\\u0430\\u0440\\u044c', u'17.418': u'\\u0421\\u0435\\u043b\\u044c\\u0441\\u043a\\u043e\\u0435 \\u0445\\u043e\\u0437\\u044f\\u0439\\u0441\\u0442\\u0432\\u043e', u'17.149': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0440\\u0430\\u0431\\u043e\\u0442\\u0435 \\u0441 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442\\u0430\\u043c\\u0438', u'29.162': u'\\u041d\\u0430\\u043b\\u0430\\u0434\\u0447\\u0438\\u043a', u'19.147': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u043e\\u0435 \\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.400': u'\\u041e\\u043f\\u0442\\u0438\\u043c\\u0438\\u0437\\u0430\\u0446\\u0438\\u044f \\u0441\\u0430\\u0439\\u0442\\u0430 (SEO)', u'23.266': u'\\u0421\\u0435\\u043c\\u0435\\u0439\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'17.417': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u043c\\u0430\\u0442\\u0435\\u0440\\u0438\\u0430\\u043b\\u044b', u'18.404': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'18.568': u'\\u041a\\u043e\\u043d\\u0441\\u0442\\u0440\\u0443\\u043a\\u0442\\u043e\\u0440', u'25.381': u'\\u0421\\u0435\\u0440\\u0432\\u0438\\u0441\\u043d\\u044b\\u0439 \\u0438\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440', u'25.382': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u043d\\u043e\\u0433\\u043e \\u0446\\u0435\\u043d\\u0442\\u0440\\u0430', u'25.383': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u0443 - \\u0441\\u0435\\u0442\\u0435\\u0432\\u044b\\u0435 \\u0438 \\u0442\\u0435\\u043b\\u0435\\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'25.384': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u0443 - \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'25.385': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u0435\\u0440 \\u043f\\u043e \\u0441\\u0435\\u0440\\u0432\\u0438\\u0441\\u0443 - \\u0442\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u0440\\u0442', u'25.386': u'\\u0418\\u043d\\u0441\\u0442\\u0430\\u043b\\u043b\\u044f\\u0446\\u0438\\u044f \\u0438 \\u043d\\u0430\\u0441\\u0442\\u0440\\u043e\\u0439\\u043a\\u0430 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f', u'17.520': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0432\\u0435\\u0446 \\u0432 \\u043c\\u0430\\u0433\\u0430\\u0437\\u0438\\u043d\\u0435', u'17.486': u'\\u0421\\u0430\\u043d\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'17.487': u'\\u0411\\u044b\\u0442\\u043e\\u0432\\u0430\\u044f \\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'18.118': u'\\u041a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c \\u043a\\u0430\\u0447\\u0435\\u0441\\u0442\\u0432\\u0430', u'18.354': u'\\u042d\\u043a\\u043e\\u043b\\u043e\\u0433', u'18.585': u'\\u0410\\u0442\\u043e\\u043c\\u043d\\u0430\\u044f \\u044d\\u043d\\u0435\\u0440\\u0433\\u0435\\u0442\\u0438\\u043a\\u0430', u'22.530': u'\\u041e\\u0444\\u043e\\u0440\\u043c\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0432\\u0438\\u0437', u'22.531': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0442\\u0443\\u0440\\u0438\\u0441\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u043c \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u043e\\u043c', u'2.200': u'\\u041e\\u0444\\u0444\\u0448\\u043e\\u0440\\u044b', u'5.34': u'\\u0410\\u0443\\u0434\\u0438\\u0442, \\u0412\\u043d\\u0443\\u0442\\u0440\\u0435\\u043d\\u043d\\u0438\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'23.314': u'\\u0423\\u0433\\u043e\\u043b\\u043e\\u0432\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'23.311': u'\\u0422\\u0440\\u0443\\u0434\\u043e\\u0432\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'17.15': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u0438, \\u0417\\u0430\\u043f\\u0447\\u0430\\u0441\\u0442\\u0438', u'18.73': u'\\u0417\\u0430\\u043a\\u0443\\u043f\\u043a\\u0438 \\u0438 \\u0441\\u043d\\u0430\\u0431\\u0436\\u0435\\u043d\\u0438\\u0435', u'18.75': u'\\u0417\\u043e\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a', u'2.43': u'\\u0411\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440', u'2.44': u'\\u0411\\u044e\\u0434\\u0436\\u0435\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0438 \\u043f\\u043b\\u0430\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'2.46': u'\\u0412\\u0430\\u043b\\u044e\\u0442\\u043d\\u044b\\u0439 \\u043a\\u043e\\u043d\\u0442\\u0440\\u043e\\u043b\\u044c', u'1.536': u'CRM \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b', u'18.265': u'\\u0421\\u0435\\u043b\\u044c\\u0445\\u043e\\u0437\\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'13.567': u'\\u0414\\u0435\\u0444\\u0435\\u043a\\u0442\\u043e\\u043b\\u043e\\u0433, \\u041b\\u043e\\u0433\\u043e\\u043f\\u0435\\u0434', u'17.279': u'\\u0421\\u0442\\u0430\\u043d\\u043a\\u0438, \\u0442\\u044f\\u0436\\u0435\\u043b\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'19.483': u'\\u041f\\u0435\\u0440\\u0435\\u0441\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'15.313': u'\\u0422\\u0443\\u0440\\u0438\\u0437\\u043c, \\u0413\\u043e\\u0441\\u0442\\u0438\\u043d\\u0438\\u0446\\u044b, \\u0420\\u0435\\u0441\\u0442\\u043e\\u0440\\u0430\\u043d\\u044b', u'13.447': u'\\u041f\\u0441\\u0438\\u0445\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f', u'8.135': u'\\u041b\\u0438\\u0447\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'22.223': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430 \\u0442\\u0443\\u0440\\u0438\\u0441\\u0442\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0445 \\u0443\\u0441\\u043b\\u0443\\u0433', u'16.569': u'\\u0410\\u0440\\u0445\\u0438\\u0432\\u0430\\u0440\\u0438\\u0443\\u0441', u'10.191': u'\\u041d\\u0435\\u0444\\u0442\\u044c', u'21.69': u'\\u0416\\u0435\\u043b\\u0435\\u0437\\u043d\\u043e\\u0434\\u043e\\u0440\\u043e\\u0436\\u043d\\u044b\\u0435 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'26.574': u'\\u0422\\u0435\\u043d\\u0434\\u0435\\u0440\\u044b', u'7.222': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430', u'17.65': u'\\u0414\\u0438\\u043b\\u0435\\u0440\\u0441\\u043a\\u0438\\u0435 \\u0441\\u0435\\u0442\\u0438', u'17.66': u'\\u0414\\u0438\\u0441\\u0442\\u0440\\u0438\\u0431\\u0443\\u0446\\u0438\\u044f', u'11.347': u'\\u0424\\u043e\\u0442\\u043e\\u0433\\u0440\\u0430\\u0444\\u0438\\u044f', u'27.496': u'\\u0412\\u043e\\u0441\\u043f\\u0438\\u0442\\u0430\\u0442\\u0435\\u043b\\u044c, \\u0413\\u0443\\u0432\\u0435\\u0440\\u043d\\u0430\\u043d\\u0442\\u043a\\u0430, \\u041d\\u044f\\u043d\\u044f', u'27.497': u'\\u0434\\u043e\\u043c\\u0440\\u0430\\u0431\\u043e\\u0442\\u043d\\u0438\\u0446\\u0430/\\u0434\\u043e\\u043c\\u0440\\u0430\\u0431\\u043e\\u0442\\u043d\\u0438\\u043a, \\u0413\\u043e\\u0440\\u043d\\u0438\\u0447\\u043d\\u0430\\u044f', u'20.186': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c/\\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'20.189': u'\\u041d\\u0435\\u0436\\u0438\\u043b\\u044b\\u0435 \\u043f\\u043e\\u043c\\u0435\\u0449\\u0435\\u043d\\u0438\\u044f', u'19.28': u'\\u0410\\u043d\\u0434\\u0435\\u0440\\u0440\\u0430\\u0439\\u0442\\u0435\\u0440', u'27.498': u'\\u041f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c', u'27.499': u'\\u0421\\u0430\\u0434\\u043e\\u0432\\u043d\\u0438\\u043a', u'19.357': u'\\u042d\\u043a\\u0441\\u043f\\u0435\\u0440\\u0442-\\u043e\\u0446\\u0435\\u043d\\u0449\\u0438\\u043a', u'20.20': u'\\u0410\\u0433\\u0435\\u043d\\u0442', u'14.355': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u043a\\u0430, \\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442', u'1.211': u'\\u041f\\u043e\\u0434\\u0434\\u0435\\u0440\\u0436\\u043a\\u0430, Helpdesk', u'18.201': u'\\u041e\\u0445\\u0440\\u0430\\u043d\\u0430 \\u0442\\u0440\\u0443\\u0434\\u0430', u'13.131': u'\\u041b\\u0435\\u0447\\u0430\\u0449\\u0438\\u0439 \\u0432\\u0440\\u0430\\u0447', u'21.17': u'\\u0410\\u0432\\u0442\\u043e\\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'21.12': u'\\u0410\\u0432\\u0438\\u0430\\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'13.138': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'18.208': u'\\u041f\\u0438\\u0449\\u0435\\u0432\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'3.244': u'\\u0420\\u0430\\u0434\\u0438\\u043e \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'4.332': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u044f\\u044e\\u0449\\u0438\\u0439 \\u043e\\u0444\\u0438\\u0441\\u043e\\u043c(\\u041effice manager)', u'5.424': u'\\u041f\\u0430\\u0435\\u0432\\u044b\\u0435 \\u0444\\u043e\\u043d\\u0434\\u044b', u'5.426': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0441\\u0442', u'4.338': u'\\u0423\\u0447\\u0435\\u0442 \\u0442\\u043e\\u0432\\u0430\\u0440\\u043e\\u043e\\u0431\\u043e\\u0440\\u043e\\u0442\\u0430', u'9.452': u'\\u0421\\u0442\\u0440\\u0430\\u0445\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'9.238': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e, \\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u044f', u'26.473': u'\\u0421\\u0442\\u0430\\u043d\\u043a\\u0438, \\u0422\\u044f\\u0436\\u0435\\u043b\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'3.31': u'\\u0410\\u0440\\u0442 \\u0434\\u0438\\u0440\\u0435\\u043a\\u0442\\u043e\\u0440', u'9.78': u'\\u0418\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'17.156': u'\\u041c\\u043d\\u043e\\u0433\\u043e\\u0443\\u0440\\u043e\\u0432\\u043d\\u0435\\u0432\\u044b\\u0439 \\u043c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433', u'5.210': u'\\u041f\\u043b\\u0430\\u0441\\u0442\\u0438\\u043a\\u043e\\u0432\\u044b\\u0435 \\u043a\\u0430\\u0440\\u0442\\u044b', u'5.215': u'\\u041f\\u043e\\u0440\\u0442\\u0444\\u0435\\u043b\\u044c\\u043d\\u044b\\u0435 \\u0438\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'17.405': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'5.219': u'\\u041f\\u0440\\u0438\\u0432\\u043b\\u0435\\u0447\\u0435\\u043d\\u0438\\u0435 \\u043a\\u043b\\u0438\\u0435\\u043d\\u0442\\u043e\\u0432', u'17.152': u'\\u041c\\u0435\\u0442\\u0430\\u043b\\u043b\\u043e\\u043f\\u0440\\u043e\\u043a\\u0430\\u0442', u'4.52': u'\\u0412\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c', u'11.293': u'\\u0422\\u0435\\u043b\\u0435\\u0432\\u0438\\u0434\\u0435\\u043d\\u0438\\u0435', u'17.196': u'\\u041e\\u043f\\u0442\\u043e\\u0432\\u0430\\u044f \\u0442\\u043e\\u0440\\u0433\\u043e\\u0432\\u043b\\u044f', u'18.488': u'\\u041c\\u0435\\u0442\\u0440\\u043e\\u043b\\u043e\\u0433', u'18.330': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u0446\\u0435\\u0445\\u043e\\u043c', u'9.345': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u044b', u'23.422': u'\\u0410\\u0432\\u0442\\u043e\\u0440\\u0441\\u043a\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'5.457': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u043e\\u043f\\u0435\\u0440\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435', u'21.480': u'\\u041a\\u043e\\u043d\\u0442\\u0435\\u0439\\u043d\\u0435\\u0440\\u043d\\u044b\\u0435 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'5.103': u'\\u041a\\u0430\\u0441\\u0441\\u043e\\u0432\\u043e\\u0435 \\u043e\\u0431\\u0441\\u043b\\u0443\\u0436\\u0438\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0438\\u043d\\u043a\\u0430\\u0441\\u0441\\u0430\\u0446\\u0438\\u044f', u'5.450': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435 \\u043c\\u0430\\u043b\\u043e\\u0433\\u043e \\u0438 \\u0441\\u0440\\u0435\\u0434\\u043d\\u0435\\u0433\\u043e \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'5.101': u'\\u0422\\u0440\\u0435\\u0439\\u0434\\u0438\\u043d\\u0433, \\u0414\\u0438\\u043b\\u0438\\u043d\\u0433', u'20.484': u'\\u0413\\u0435\\u043e\\u0434\\u0435\\u0437\\u0438\\u044f \\u0438 \\u043a\\u0430\\u0440\\u0442\\u043e\\u0433\\u0440\\u0430\\u0444\\u0438\\u044f', u'21.563': u'\\u041a\\u043b\\u0430\\u0434\\u043e\\u0432\\u0449\\u0438\\u043a', u'5.459': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u0440\\u044b\\u043d\\u043e\\u0447\\u043d\\u044b\\u0435', u'5.458': u'\\u0420\\u0438\\u0441\\u043a\\u0438: \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0435', u'15.288': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e, \\u0410\\u0440\\u0445\\u0438\\u0442\\u0435\\u043a\\u0442\\u0443\\u0440\\u0430', u'5.79': u'\\u0418\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u043a\\u043e\\u043c\\u043f\\u0430\\u043d\\u0438\\u044f', u'18.130': u'\\u0414\\u0435\\u0440\\u0435\\u0432\\u043e\\u043e\\u0431\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430, \\u041b\\u0435\\u0441\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'23.214': u'\\u041f\\u043e\\u043c\\u043e\\u0449\\u043d\\u0438\\u043a', u'17.580': u'\\u041a\\u043b\\u0438\\u043d\\u0438\\u043d\\u0433\\u043e\\u0432\\u044b\\u0435 \\u0443\\u0441\\u043b\\u0443\\u0433\\u0438', u'27.584': u'\\u0420\\u0435\\u043f\\u0435\\u0442\\u0438\\u0442\\u043e\\u0440', u'23.352': u'\\u0426\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0431\\u0443\\u043c\\u0430\\u0433\\u0438, \\u0420\\u044b\\u043d\\u043a\\u0438 \\u043a\\u0430\\u043f\\u0438\\u0442\\u0430\\u043b\\u0430', u'6.309': u'\\u0422\\u0440\\u0435\\u043d\\u0438\\u043d\\u0433\\u0438', u'18.348': u'\\u0425\\u0438\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'19.170': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'3.150': u'\\u041c\\u0435\\u043d\\u0435\\u0434\\u0436\\u043c\\u0435\\u043d\\u0442 \\u043f\\u0440\\u043e\\u0434\\u0443\\u043a\\u0442\\u0430 (Product manager)', u'3.151': u'\\u041c\\u0435\\u0440\\u0447\\u0435\\u043d\\u0434\\u0430\\u0439\\u0437\\u0438\\u043d\\u0433', u'1.25': u'\\u0410\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a', u'18.431': u'\\u0413\\u043b\\u0430\\u0432\\u043d\\u044b\\u0439 \\u043c\\u0435\\u0445\\u0430\\u043d\\u0438\\u043a', u'3.294': u'\\u0422\\u0435\\u043b\\u0435\\u0432\\u0438\\u0437\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'29.555': u'\\u041f\\u0435\\u0440\\u0435\\u043c\\u043e\\u0442\\u0447\\u0438\\u043a', u'29.554': u'\\u041f\\u0430\\u0441\\u0442\\u0443\\u0445, \\u0427\\u0430\\u0431\\u0430\\u043d', u'29.557': u'\\u0421\\u0430\\u043d\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a', u'7.502': u'\\u0422\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0449\\u0438\\u043a', u'29.551': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u0438\\u0441\\u0442 \\u044d\\u043a\\u0441\\u043a\\u0430\\u0432\\u0430\\u0442\\u043e\\u0440\\u0430', u'29.550': u'\\u041c\\u0430\\u0448\\u0438\\u043d\\u0438\\u0441\\u0442 \\u0441\\u0446\\u0435\\u043d\\u044b', u'29.553': u'\\u041e\\u043f\\u0435\\u0440\\u0430\\u0442\\u043e\\u0440 \\u0441\\u0442\\u0430\\u043d\\u043a\\u043e\\u0432', u'29.552': u'\\u041c\\u0435\\u0445\\u0430\\u043d\\u0438\\u043a', u'9.168': u'\\u041d\\u0430\\u0443\\u043a\\u0430, \\u041e\\u0431\\u0440\\u0430\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'15.68': u'\\u0414\\u043e\\u0431\\u044b\\u0447\\u0430 \\u0441\\u044b\\u0440\\u044c\\u044f', u'29.559': u'\\u0428\\u0442\\u0443\\u043a\\u0430\\u0442\\u0443\\u0440', u'29.558': u'\\u0428\\u043b\\u0438\\u0444\\u043e\\u0432\\u0449\\u0438\\u043a', u'7.267': u'\\u0421\\u0435\\u0440\\u0432\\u0438\\u0441\\u043d\\u043e\\u0435 \\u043e\\u0431\\u0441\\u043b\\u0443\\u0436\\u0438\\u0432\\u0430\\u043d\\u0438\\u0435', u'17.24': u'\\u0410\\u043b\\u043a\\u043e\\u0433\\u043e\\u043b\\u044c', u'8.519': u'\\u0418\\u043d\\u043a\\u0430\\u0441\\u0441\\u0430\\u0442\\u043e\\u0440', u'13.185': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'15.346': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u044b, \\u0411\\u0430\\u043d\\u043a\\u0438, \\u0418\\u043d\\u0432\\u0435\\u0441\\u0442\\u0438\\u0446\\u0438\\u0438', u'18.153': u'\\u041c\\u0435\\u0442\\u0430\\u043b\\u043b\\u0443\\u0440\\u0433\\u0438\\u044f', u'26.449': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435/\\u0441\\u0432\\u0435\\u0442\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'26.408': u'\\u0410\\u043b\\u043a\\u043e\\u0433\\u043e\\u043b\\u044c', u'26.409': u'\\u0413\\u0421\\u041c, \\u043d\\u0435\\u0444\\u0442\\u044c, \\u0431\\u0435\\u043d\\u0437\\u0438\\u043d', u'2.434': u'\\u041f\\u043b\\u0430\\u043d\\u043e\\u0432\\u043e-\\u044d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u0443\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435', u'3.64': u'\\u0414\\u0438\\u0437\\u0430\\u0439\\u043d\\u0435\\u0440', u'8.260': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0438\\u0442\\u0435\\u043b\\u044c \\u0421\\u0411', u'26.406': u'\\u0421\\u0435\\u0440\\u0442\\u0438\\u0444\\u0438\\u043a\\u0430\\u0446\\u0438\\u044f', u'26.407': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u0438, \\u0417\\u0430\\u043f\\u0447\\u0430\\u0441\\u0442\\u0438', u'10.54': u'\\u0413\\u0435\\u043e\\u043b\\u043e\\u0433\\u043e\\u0440\\u0430\\u0437\\u0432\\u0435\\u0434\\u043a\\u0430', u'21.53': u'\\u0412\\u042d\\u0414', u'3.209': u'\\u041f\\u043b\\u0430\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0420\\u0430\\u0437\\u043c\\u0435\\u0449\\u0435\\u043d\\u0438\\u0435 \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u044b', u'20.325': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'12.251': u'\\u0420\\u0435\\u0438\\u043d\\u0436\\u0438\\u043d\\u0438\\u0440\\u0438\\u043d\\u0433 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441 \\u043f\\u0440\\u043e\\u0446\\u0435\\u0441\\u0441\\u043e\\u0432', u'12.252': u'\\u0420\\u0435\\u0438\\u043d\\u0436\\u0438\\u043d\\u0438\\u0440\\u0438\\u043d\\u0433, \\u0410\\u0443\\u0442\\u0441\\u043e\\u0440\\u0441\\u0438\\u043d\\u0433 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u043e\\u0439 \\u0444\\u0443\\u043d\\u043a\\u0446\\u0438\\u0438', u'3.206': u'\\u041f\\u0435\\u0447\\u0430\\u0442\\u043d\\u0430\\u044f \\u0440\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430', u'20.287': u'\\u0421\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u0441\\u0442\\u0432\\u043e', u'8.356': u'\\u042d\\u043a\\u043e\\u043d\\u043e\\u043c\\u0438\\u0447\\u0435\\u0441\\u043a\\u0430\\u044f \\u0438 \\u0438\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'9.562': u'\\u0410\\u043d\\u0442\\u0438\\u043a\\u0440\\u0438\\u0437\\u0438\\u0441\\u043d\\u043e\\u0435 \\u0443\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435', u'1.246': u'\\u0420\\u0430\\u0437\\u0432\\u0438\\u0442\\u0438\\u0435 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441\\u0430', u'17.397': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435, \\u0421\\u0432\\u0435\\u0442\\u043e\\u0442\\u0435\\u0445\\u043d\\u0438\\u043a\\u0430', u'1.327': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0430\\u043c\\u0438', u'18.245': u'\\u0420\\u0430\\u0434\\u0438\\u043e\\u044d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'10.470': u'\\u0411\\u0443\\u0440\\u0435\\u043d\\u0438\\u0435', u'10.471': u'\\u041c\\u0430\\u0440\\u043a\\u0448\\u0435\\u0439\\u0434\\u0435\\u0440', u'18.373': u'\\u0421\\u0443\\u0434\\u043e\\u0441\\u0442\\u0440\\u043e\\u0435\\u043d\\u0438\\u0435', u'3.1': u'Below The Line (BTL)', u'11.157': u'\\u041c\\u043e\\u0434\\u0430', u'20.70': u'\\u0416\\u0438\\u043b\\u044c\\u0435', u'3.8': u'PR, \\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433\\u043e\\u0432\\u044b\\u0435 \\u043a\\u043e\\u043c\\u043c\\u0443\\u043d\\u0438\\u043a\\u0430\\u0446\\u0438\\u0438', u'22.39': u'\\u0411\\u0440\\u043e\\u043d\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'4.205': u'\\u041f\\u0435\\u0440\\u0441\\u043e\\u043d\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0430\\u0441\\u0441\\u0438\\u0441\\u0442\\u0435\\u043d\\u0442', u'4.207': u'\\u041f\\u0438\\u0441\\u044c\\u043c\\u0435\\u043d\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0434', u'5.371': u'\\u041a\\u0440\\u0435\\u0434\\u0438\\u0442\\u044b: \\u0440\\u043e\\u0437\\u043d\\u0438\\u0447\\u043d\\u044b\\u0435', u'22.35': u'\\u0411\\u0430\\u043d\\u043a\\u0435\\u0442\\u044b', u'3.507': u'\\u0422\\u0430\\u0439\\u043d\\u044b\\u0439 \\u043f\\u043e\\u043a\\u0443\\u043f\\u0430\\u0442\\u0435\\u043b\\u044c', u'17.324': u'\\u0423\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435 \\u043f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0430\\u043c\\u0438', u'3.509': u'\\u041f\\u0440\\u043e\\u043c\\u043e\\u0443\\u0442\\u0435\\u0440', u'3.508': u'\\u041f\\u0440\\u043e\\u0432\\u0435\\u0434\\u0435\\u043d\\u0438\\u0435 \\u043e\\u043f\\u0440\\u043e\\u0441\\u043e\\u0432, \\u0418\\u043d\\u0442\\u0435\\u0440\\u0432\\u044c\\u044e\\u0435\\u0440', u'9.307': u'\\u0422\\u0440\\u0430\\u043d\\u0441\\u043f\\u043e\\u0440\\u0442, \\u041b\\u043e\\u0433\\u0438\\u0441\\u0442\\u0438\\u043a\\u0430', u'4.429': u'\\u0414\\u0435\\u043b\\u043e\\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'4.428': u'\\u041f\\u043e\\u0441\\u043b\\u0435\\u0434\\u043e\\u0432\\u0430\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0439 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0434', u'15.390': u'\\u0410\\u0432\\u0442\\u043e\\u043c\\u043e\\u0431\\u0438\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441', u'15.391': u'\\u041a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'23.159': u'\\u041c\\u043e\\u0440\\u0441\\u043a\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'3.119': u'\\u041a\\u043e\\u043f\\u0438\\u0440\\u0430\\u0439\\u0442\\u0435\\u0440', u'9.115': u'\\u041a\\u043e\\u043d\\u0441\\u0443\\u043b\\u044c\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'29.544': u'\\u0417\\u0430\\u043f\\u0440\\u0430\\u0432\\u0449\\u0438\\u043a \\u043a\\u0430\\u0440\\u0442\\u0440\\u0438\\u0434\\u0436\\u0435\\u0439', u'5.257': u'\\u0420\\u043e\\u0437\\u043d\\u0438\\u0447\\u043d\\u044b\\u0439 \\u0431\\u0438\\u0437\\u043d\\u0435\\u0441', u'5.106': u'\\u041a\\u043e\\u043c\\u043c\\u0435\\u0440\\u0447\\u0435\\u0441\\u043a\\u0438\\u0439 \\u0431\\u0430\\u043d\\u043a', u'15.140': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433, \\u0420\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430, PR', u'23.2': u'Compliance', u'5.250': u'\\u0420\\u0430\\u0441\\u0447\\u0435\\u0442\\u044b', u'5.394': u'\\u042d\\u043c\\u0438\\u0441\\u0441\\u0438\\u0438', u'14.169': u'\\u041d\\u0430\\u0443\\u043a\\u0438 \\u043e \\u0417\\u0435\\u043c\\u043b\\u0435', u'17.625': u'\\u0420\\u0435\\u0448\\u0435\\u043d\\u0438\\u044f \\u043f\\u043e \\u0430\\u0432\\u0442\\u043e\\u043c\\u0430\\u0442\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438 \\u043f\\u0440\\u043e\\u0446\\u0435\\u0441\\u0441\\u043e\\u0432', u'21.158': u'\\u041c\\u043e\\u0440\\u0441\\u043a\\u0438\\u0435/\\u0420\\u0435\\u0447\\u043d\\u044b\\u0435 \\u043f\\u0435\\u0440\\u0435\\u0432\\u043e\\u0437\\u043a\\u0438', u'20.387': u'\\u041f\\u0440\\u043e\\u0440\\u0430\\u0431', u'22.248': u'\\u0420\\u0430\\u0437\\u043c\\u0435\\u0449\\u0435\\u043d\\u0438\\u0435, \\u041e\\u0431\\u0441\\u043b\\u0443\\u0436\\u0438\\u0432\\u0430\\u043d\\u0438\\u0435 \\u0433\\u043e\\u0441\\u0442\\u0435\\u0439', u'14.364': u'\\u042f\\u0437\\u044b\\u043a\\u0438', u'5.27': u'\\u0410\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a', u'9.94': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438, \\u0418\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442, \\u041c\\u0443\\u043b\\u044c\\u0442\\u0438\\u043c\\u0435\\u0434\\u0438\\u0430', u'9.95': u'\\u0418\\u0441\\u043a\\u0443\\u0441\\u0441\\u0442\\u0432\\u043e, \\u0420\\u0430\\u0437\\u0432\\u043b\\u0435\\u0447\\u0435\\u043d\\u0438\\u044f, \\u041c\\u0430\\u0441\\u0441-\\u043c\\u0435\\u0434\\u0438\\u0430', u'5.23': u'\\u0410\\u043a\\u0446\\u0438\\u0438, \\u0426\\u0435\\u043d\\u043d\\u044b\\u0435 \\u0431\\u0443\\u043c\\u0430\\u0433\\u0438', u'29.511': u'\\u0422\\u043e\\u043a\\u0430\\u0440\\u044c, \\u0424\\u0440\\u0435\\u0437\\u0435\\u0440\\u043e\\u0432\\u0449\\u0438\\u043a', u'29.510': u'\\u0421\\u043b\\u0435\\u0441\\u0430\\u0440\\u044c', u'1.116': u'\\u041a\\u043e\\u043d\\u0442\\u0435\\u043d\\u0442', u'1.117': u'\\u0422\\u0435\\u0441\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'1.110': u'\\u041a\\u043e\\u043c\\u043f\\u044c\\u044e\\u0442\\u0435\\u0440\\u043d\\u0430\\u044f \\u0431\\u0435\\u0437\\u043e\\u043f\\u0430\\u0441\\u043d\\u043e\\u0441\\u0442\\u044c', u'29.514': u'\\u0421\\u0432\\u0430\\u0440\\u0449\\u0438\\u043a', u'29.517': u'\\u0421\\u0431\\u043e\\u0440\\u0449\\u0438\\u043a', u'1.113': u'\\u041a\\u043e\\u043d\\u0441\\u0430\\u043b\\u0442\\u0438\\u043d\\u0433, \\u0410\\u0443\\u0442\\u0441\\u043e\\u0440\\u0441\\u0438\\u043d\\u0433', u'18.298': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433, \\u041c\\u044f\\u0441\\u043e- \\u0438 \\u043f\\u0442\\u0438\\u0446\\u0435\\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'29.582': u'\\u041f\\u0440\\u043e\\u0432\\u043e\\u0434\\u043d\\u0438\\u043a', u'29.583': u'\\u041c\\u0430\\u043b\\u044f\\u0440', u'1.50': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0443\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u044f \\u043f\\u0440\\u0435\\u0434\\u043f\\u0440\\u0438\\u044f\\u0442\\u0438\\u0435\\u043c (ERP)', u'29.581': u'\\u041c\\u043e\\u043d\\u0442\\u0430\\u0436\\u043d\\u0438\\u043a', u'15.96': u'\\u0418\\u0441\\u043a\\u0443\\u0441\\u0441\\u0442\\u0432\\u043e, \\u0420\\u0430\\u0437\\u0432\\u043b\\u0435\\u0447\\u0435\\u043d\\u0438\\u044f, \\u041c\\u0430\\u0441\\u0441-\\u043c\\u0435\\u0434\\u0438\\u0430', u'22.504': u'\\u0425\\u043e\\u0441\\u0442\\u0435\\u0441', u'29.588': u'\\u0414\\u0440\\u0443\\u0433\\u043e\\u0435', u'17.256': u'\\u0420\\u043e\\u0437\\u043d\\u0438\\u0447\\u043d\\u0430\\u044f \\u0442\\u043e\\u0440\\u0433\\u043e\\u0432\\u043b\\u044f', u'2.100': u'\\u041a\\u0430\\u0437\\u043d\\u0430\\u0447\\u0435\\u0439\\u0441\\u0442\\u0432\\u043e', u'2.102': u'\\u041a\\u0430\\u0441\\u0441\\u0438\\u0440, \\u0418\\u043d\\u043a\\u0430\\u0441\\u0441\\u0430\\u0442\\u043e\\u0440', u'16.571': u'\\u0411\\u0438\\u0431\\u043b\\u0438\\u043e\\u0442\\u0435\\u043a\\u0430\\u0440\\u044c', u'16.570': u'\\u0410\\u0442\\u0442\\u0430\\u0448\\u0435', u'13.228': u'\\u041b\\u0435\\u043a\\u0430\\u0440\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0435 \\u043f\\u0440\\u0435\\u043f\\u0430\\u0440\\u0430\\u0442\\u044b', u'13.229': u'\\u041c\\u0435\\u0434\\u0438\\u0446\\u0438\\u043d\\u0441\\u043a\\u043e\\u0435 \\u043e\\u0431\\u043e\\u0440\\u0443\\u0434\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'13.220': u'\\u041f\\u0440\\u043e\\u0432\\u0438\\u0437\\u043e\\u0440', u'12.122': u'\\u041a\\u043e\\u0440\\u043f\\u043e\\u0440\\u0430\\u0442\\u0438\\u0432\\u043d\\u044b\\u0435 \\u0444\\u0438\\u043d\\u0430\\u043d\\u0441\\u044b', u'7.392': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c /\\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430 ', u'2.261': u'\\u0420\\u0443\\u043a\\u043e\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0431\\u0443\\u0445\\u0433\\u0430\\u043b\\u0442\\u0435\\u0440\\u0438\\u0435\\u0439', u'13.227': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'9.521': u'\\u0421\\u043f\\u043e\\u0440\\u0442\\u0438\\u0432\\u043d\\u044b\\u0435 \\u043a\\u043b\\u0443\\u0431\\u044b, \\u0424\\u0438\\u0442\\u043d\\u0435\\u0441, \\u0421\\u0430\\u043b\\u043e\\u043d\\u044b \\u043a\\u0440\\u0430\\u0441\\u043e\\u0442\\u044b', u'18.291': u'\\u0422\\u0430\\u0431\\u0430\\u0447\\u043d\\u0430\\u044f \\u043f\\u0440\\u043e\\u043c\\u044b\\u0448\\u043b\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u044c', u'9.139': u'\\u041c\\u0430\\u0440\\u043a\\u0435\\u0442\\u0438\\u043d\\u0433, \\u0420\\u0435\\u043a\\u043b\\u0430\\u043c\\u0430, PR', u'15.237': u'\\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e, \\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'8.461': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0432\\u0438\\u0434\\u0435\\u043e\\u043d\\u0430\\u0431\\u043b\\u044e\\u0434\\u0435\\u043d\\u0438\\u044f', u'8.462': u'\\u0412\\u0437\\u044b\\u0441\\u043a\\u0430\\u043d\\u0438\\u0435 \\u0437\\u0430\\u0434\\u043e\\u043b\\u0436\\u0435\\u043d\\u043d\\u043e\\u0441\\u0442\\u0438, \\u041a\\u043e\\u043b\\u043b\\u0435\\u043a\\u0442\\u043e\\u0440\\u0441\\u043a\\u0430\\u044f \\u0434\\u0435\\u044f\\u0442\\u0435\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u044c', u'22.529': u'\\u0410\\u043d\\u0438\\u043c\\u0430\\u0446\\u0438\\u044f', u'16.194': u'\\u041e\\u0431\\u0449\\u0435\\u0441\\u0442\\u0432\\u0435\\u043d\\u043d\\u044b\\u0435 \\u043e\\u0440\\u0433\\u0430\\u043d\\u0438\\u0437\\u0430\\u0446\\u0438\\u0438', u'17.358': u'\\u042d\\u043b\\u0435\\u043a\\u0442\\u0440\\u043e\\u043d\\u0438\\u043a\\u0430, \\u0444\\u043e\\u0442\\u043e, \\u0432\\u0438\\u0434\\u0435\\u043e', u'14.91': u'\\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0442\\u0438\\u043a\\u0430, \\u0418\\u043d\\u0444\\u043e\\u0440\\u043c\\u0430\\u0446\\u0438\\u043e\\u043d\\u043d\\u044b\\u0435 \\u0441\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b', u'17.350': u'\\u0426\\u0432\\u0435\\u0442\\u043d\\u044b\\u0435 \\u043c\\u0435\\u0442\\u0430\\u043b\\u043b\\u044b', u'1.203': u'\\u041f\\u0435\\u0440\\u0435\\u0434\\u0430\\u0447\\u0430 \\u0434\\u0430\\u043d\\u043d\\u044b\\u0445 \\u0438 \\u0434\\u043e\\u0441\\u0442\\u0443\\u043f \\u0432 \\u0438\\u043d\\u0442\\u0435\\u0440\\u043d\\u0435\\u0442', u'11.218': u'\\u041f\\u0440\\u0435\\u0441\\u0441\\u0430', u'19.586': u'\\u0412\\u0440\\u0430\\u0447-\\u044d\\u043a\\u0441\\u043f\\u0435\\u0440\\u0442', u'13.128': u'\\u041b\\u0430\\u0431\\u043e\\u0440\\u0430\\u043d\\u0442', u'12.280': u'\\u0421\\u0442\\u0440\\u0430\\u0442\\u0435\\u0433\\u0438\\u044f', u'6.184': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'21.403': u'\\u0411\\u0438\\u0437\\u043d\\u0435\\u0441-\\u0430\\u0432\\u0438\\u0430\\u0446\\u0438\\u044f', u'21.402': u'\\u0413\\u0440\\u0430\\u0436\\u0434\\u0430\\u043d\\u0441\\u043a\\u0430\\u044f \\u0430\\u0432\\u0438\\u0430\\u0446\\u0438\\u044f', u'3.423': u'\\u0410\\u0441\\u0441\\u0438\\u0441\\u0442\\u0435\\u043d\\u0442', u'3.26': u'\\u0410\\u043d\\u0430\\u043b\\u0438\\u0442\\u0438\\u043a', u'14.217': u'\\u041f\\u0440\\u0435\\u043f\\u043e\\u0434\\u0430\\u0432\\u0430\\u043d\\u0438\\u0435', u'9.226': u'\\u041f\\u0440\\u043e\\u0434\\u0430\\u0436\\u0438', u'18.143': u'\\u041c\\u0435\\u0431\\u0435\\u043b\\u044c\\u043d\\u043e\\u0435 \\u043f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e', u'5.163': u'\\u041d\\u0430\\u043b\\u043e\\u0433\\u0438', u'23.442': u'\\u041c\\u0435\\u0436\\u0434\\u0443\\u043d\\u0430\\u0440\\u043e\\u0434\\u043d\\u043e\\u0435 \\u043f\\u0440\\u0430\\u0432\\u043e', u'1.475': u'\\u0418\\u0433\\u0440\\u043e\\u0432\\u043e\\u0435 \\u041f\\u041e', u'1.474': u'\\u0421\\u0442\\u0430\\u0440\\u0442\\u0430\\u043f\\u044b', u'1.274': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u044b \\u0430\\u0432\\u0442\\u043e\\u043c\\u0430\\u0442\\u0438\\u0437\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u043d\\u043e\\u0433\\u043e \\u043f\\u0440\\u043e\\u0435\\u043a\\u0442\\u0438\\u0440\\u043e\\u0432\\u0430\\u043d\\u0438\\u044f', u'1.277': u'\\u0421\\u043e\\u0442\\u043e\\u0432\\u044b\\u0435, \\u0411\\u0435\\u0441\\u043f\\u0440\\u043e\\u0432\\u043e\\u0434\\u043d\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'1.270': u'\\u0421\\u0435\\u0442\\u0435\\u0432\\u044b\\u0435 \\u0442\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433\\u0438\\u0438', u'1.273': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u043d\\u044b\\u0439 \\u0430\\u0434\\u043c\\u0438\\u043d\\u0438\\u0441\\u0442\\u0440\\u0430\\u0442\\u043e\\u0440', u'1.272': u'\\u0421\\u0438\\u0441\\u0442\\u0435\\u043c\\u043d\\u0430\\u044f \\u0438\\u043d\\u0442\\u0435\\u0433\\u0440\\u0430\\u0446\\u0438\\u044f', u'17.440': u'\\u0422\\u0435\\u043a\\u0441\\u0442\\u0438\\u043b\\u044c, \\u041e\\u0434\\u0435\\u0436\\u0434\\u0430, \\u041e\\u0431\\u0443\\u0432\\u044c', u'17.183': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430', u'23.187': u'\\u041d\\u0435\\u0434\\u0432\\u0438\\u0436\\u0438\\u043c\\u043e\\u0441\\u0442\\u044c', u'18.300': u'\\u0422\\u0435\\u0445\\u043d\\u043e\\u043b\\u043e\\u0433, \\u041f\\u0440\\u043e\\u0438\\u0437\\u0432\\u043e\\u0434\\u0441\\u0442\\u0432\\u043e \\u0441\\u0430\\u0445\\u0430\\u0440\\u0430', u'21.275': u'\\u0421\\u043a\\u043b\\u0430\\u0434\\u0441\\u043a\\u043e\\u0435 \\u0445\\u043e\\u0437\\u044f\\u0439\\u0441\\u0442\\u0432\\u043e', u'4.494': u'\\u0423\\u0431\\u043e\\u0440\\u0449\\u0438\\u0446\\u0430', u'5.444': u'\\u0424\\u0438\\u043d\\u0430\\u043d\\u0441\\u043e\\u0432\\u044b\\u0439 \\u043c\\u043e\\u043d\\u0438\\u0442\\u043e\\u0440\\u0438\\u043d\\u0433', u'23.188': u'\\u041d\\u0435\\u0434\\u0440\\u043e\\u043f\\u043e\\u043b\\u044c\\u0437\\u043e\\u0432\\u0430\\u043d\\u0438\\u0435', u'18.84': u'\\u0418\\u043d\\u0436\\u0435\\u043d\\u0435\\u0440, \\u041c\\u044f\\u0441\\u043e- \\u0438 \\u043f\\u0442\\u0438\\u0446\\u0435\\u043f\\u0435\\u0440\\u0435\\u0440\\u0430\\u0431\\u043e\\u0442\\u043a\\u0430', u'4.255': u'\\u0420\\u0435\\u0441\\u0435\\u043f\\u0448\\u0435\\u043d', u'20.490': u'\\u0420\\u0430\\u0431\\u043e\\u0447\\u0438\\u0435 \\u0441\\u0442\\u0440\\u043e\\u0438\\u0442\\u0435\\u043b\\u044c\\u043d\\u044b\\u0445 \\u0441\\u043f\\u0435\\u0446\\u0438\\u0430\\u043b\\u044c\\u043d\\u043e\\u0441\\u0442\\u0435\\u0439', u'23.182': u'\\u041d\\u0430\\u0447\\u0430\\u043b\\u044c\\u043d\\u044b\\u0439 \\u0443\\u0440\\u043e\\u0432\\u0435\\u043d\\u044c, \\u041c\\u0430\\u043b\\u043e \\u043e\\u043f\\u044b\\u0442\\u0430'}\n",
      "0.13013013013\n"
     ]
    }
   ],
   "source": [
    "###специализации\n",
    "import httplib\n",
    "import json\n",
    "headers = {\"Authorization\": \"Bearer PHUAM0L3PU56VNT041CJM3MUTGSABUCDTMBVHEAMF5CGCDEEIC7VFFT4VLOP0GQP\", \n",
    "           \"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"/specializations\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "\n",
    "#print json.loads(r1.read())\n",
    "specs = {}\n",
    "for k in [d['specializations'] for d in json.loads(r1.read())]:\n",
    "    for n in k:\n",
    "        specs[n['id']] = n['name']\n",
    "print specs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.3\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "###ЗП\n",
    "real = 100000.0\n",
    "expected = 100000.0\n",
    "print 1-(abs(expected-real)/max(real,expected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "##опыт работы\n",
    "real = 0\n",
    "expected = 0\n",
    "print (3.0 - abs(real-expected))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##навыки\n",
    "import httplib\n",
    "import json\n",
    "import pickle\n",
    "headers = {\"Authorization\": \"Bearer UGE7369P4FJVPH027PQII1S422JHFJ2NQADKREFO8KTUP1BCIJQ2T6BUQ7CLQMGF\", \n",
    "           \"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"/specializations\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "\n",
    "# print json.loads(r1.read())\n",
    "spec_ids = []\n",
    "spec_names = []\n",
    "for k in [d['specializations'] for d in json.loads(r1.read())]:\n",
    "    for n in k:\n",
    "        spec_ids.append(n['id'])\n",
    "        spec_names.append(n['name'])\n",
    "pickle.dump( spec_ids, open( \"spec_ids.p\", \"wb\" ) )\n",
    "pickle.dump( spec_names, open( \"spec_names.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199376\n",
      "6286\n",
      "1200\n"
     ]
    }
   ],
   "source": [
    "##вытаскиваем из вакансий ключевые навыки\n",
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "db = TinyDB('/home/shurik2533/vacancies.json')\n",
    "db2 = TinyDB('/home/shurik2533/vacancies2.json')\n",
    "vacancies = db.all()\n",
    "vacancies2 = db2.all()\n",
    "all_key_skills = []\n",
    "key_skills_set = set()\n",
    "key_skills_dict = dict()\n",
    "for vacancy in vacancies:\n",
    "#     print vacancy['name']\n",
    "    for skill in vacancy['key_skills']:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            all_key_skills.append(word)\n",
    "            key_skills_set.add(word)\n",
    "            if word in key_skills_dict:\n",
    "                key_skills_dict[word] = key_skills_dict[word]+1;\n",
    "            else:\n",
    "                key_skills_dict[word] = 1;\n",
    "#     print vacancy['description']\n",
    "\n",
    "for vacancy in vacancies2:\n",
    "#     print vacancy['name']\n",
    "    for skill in vacancy['key_skills']:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            all_key_skills.append(word)\n",
    "            key_skills_set.add(word)\n",
    "            if word in key_skills_dict:\n",
    "                key_skills_dict[word] = key_skills_dict[word]+1;\n",
    "            else:\n",
    "                key_skills_dict[word] = 1;\n",
    "#     print vacancy['description']\n",
    "\n",
    "\n",
    "print len(all_key_skills)\n",
    "print len(key_skills_set)\n",
    "for k in key_skills_dict.keys():\n",
    "    if key_skills_dict[k] <= 7:\n",
    "        del key_skills_dict[k]\n",
    "\n",
    "res = list(key_skills_dict.keys())\n",
    "print len(res)\n",
    "pickle.dump( res, open( \"key_skills.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "print len(key_skills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44308\n",
      "11107\n",
      "1968\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "\n",
    "db = TinyDB('/home/shurik2533/vacancies.json')\n",
    "db2 = TinyDB('/home/shurik2533/vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "titles_s = set()\n",
    "for vac in vacancies:\n",
    "    titles_s.add(vac['name'])\n",
    "print len(titles_s)\n",
    "title_words = dict()\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "for title in titles_s:\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', title.lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        if word in title_words:\n",
    "            title_words[word] = title_words[word]+1;\n",
    "        else:\n",
    "            title_words[word] = 1;\n",
    "print len(title_words)\n",
    "for k in title_words.keys():\n",
    "    if title_words[k] <= 8 or len(k.strip()) <= 2:\n",
    "        del title_words[k]\n",
    "print len(title_words)\n",
    "res = list(title_words.keys())\n",
    "pickle.dump( res, open( \"title_words.p\", \"wb\" ) )\n",
    "# vacancies.append(vacancies2)\n",
    "# print len(vacancies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113289\n",
      "290361\n",
      "137670\n",
      " юрист ведущ по претензион исков и договорн работ в област аренд недвижим в тр комплекс новомосковск в город московск ул хабаров м саларьев в администрац всег комплекс занима аренд коммерческ нежил помещен офис и торгов площади. обязанности: переговор с арендаторами. договорн работа. претензион - исков работ (иски, претенз и т. д.) представительств в арбитражн суд и административн процессе. требования: высш юридическ образование. хорош знан договорн работ в област аренд !!! оп работ с арбитражн суд в област «аренд недвижимости» ! условия: . гр/р: 5/2 , с 9.00 ч. – 18.00 ч. з/плата: пок 55 000 руб. - 60 000 руб. ( на рук ), в зависим от ваш опыта! карьерн рост : начальник юридическ отдела, с з/п до 100 000 руб. оплат телефон . оформлен по тк рф социальн пакет по тк рф в ближайш 3 - 4 месяц будет сво столов – бесплатн обеды! проезд: м. саларьево, 420 автобус, (10-12 минут), , остановк по требован «гостиничн комплекс.\n",
      "<p>Юрист (ведущий) по претензионно - исковой и договорной работе в области «Аренда недвижимости» в ТР Комплекс «НовоМосковский», в город Московский, ул. Хабарова (м. Саларьево). В администрацию всего Комплекса, занимающейся арендой коммерческих, нежилых помещений – офисы и торговые площади. </p> <p><strong>Обязанности: </strong></p> <ul> <li>Переговоры с Арендаторами.</li> <li>Договорная работа.</li> <li>Претензионно - исковая работа (иски, претензии и т. д.)</li> <li>Представительство в Арбитражном суде и административном процессе. </li> </ul> <p> <strong>Требования: </strong></p> <ul> <li>Высшее Юридическое образование. </li> <li>Хорошее знание Договорной работы в области Аренды !!!</li> <li>Опыт работы с Арбитражном судом в области «Аренда недвижимости» !</li> </ul> <p><strong>Условия: </strong>.</p> <ul> <li>Гр/р: 5/2 , с 9.00 ч. – 18.00 ч.</li> <li>З/плата: пока 55 000 руб. - 60 000 руб. ( на руки ), в зависимости от Вашего опыта!</li> <li>Карьерный рост : начальник юридического отдела, с з/п до 100 000 руб.</li> <li>Оплата телефона .</li> <li>Оформление по ТК РФ</li> <li>Социальный пакет по ТК РФ</li> <li>В ближайшие 3 - 4 месяца будет своя столовая – бесплатные обеды!</li> </ul> Проезд: М. Саларьево, 420 автобус, (10-12 минут), , остановка по требованию «Гостиничный Комплекс&quot;.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "\n",
    "db = TinyDB('/home/shurik2533/vacancies.json')\n",
    "db2 = TinyDB('/home/shurik2533/vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "docs = []\n",
    "for vac in vacancies:\n",
    "    docs.append(vac['description'])\n",
    "print len(docs)\n",
    "    \n",
    "    \n",
    "doc_words = dict()\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "p_docs = []\n",
    "for doc in docs:\n",
    "    p_doc = \"\"\n",
    "    doc = re.sub('<[^>]*>', '', doc.lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        p_doc = p_doc + \" \" + word\n",
    "        if word in doc_words:\n",
    "            doc_words[word] = doc_words[word]+1;\n",
    "        else:\n",
    "            doc_words[word] = 1;\n",
    "    p_docs.append(p_doc)\n",
    "print len(doc_words)\n",
    "for k in doc_words.keys():\n",
    "    if doc_words[k] == 1 or len(k.strip()) <= 1:\n",
    "        del doc_words[k]\n",
    "print len(doc_words)\n",
    "# pickle.dump( doc_words, open( \"doc_words.p\", \"wb\" ) )\n",
    "pickle.dump( p_docs, open( \"processed_docs.p\", \"wb\" ) )\n",
    "print p_docs[0]\n",
    "print docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Юрист (ведущий) по претензионно - исковой и договорной работе в области «Аренда недвижимости» в ТР Комплекс «НовоМосковский», в город Московский, ул. Хабарова (м. Саларьево). В администрацию всего Комплекса, занимающейся арендой коммерческих, нежилых помещений – офисы и торговые площади.  Обязанности:   Переговоры с Арендаторами. Договорная работа. Претензионно - исковая работа (иски, претензии и т. д.) Представительство в Арбитражном суде и административном процессе.    Требования:   Высшее Юридическое образование.  Хорошее знание Договорной работы в области Аренды !!! Опыт работы с Арбитражном судом в области «Аренда недвижимости» !  Условия: .  Гр/р: 5/2 , с 9.00 ч. – 18.00 ч. З/плата: пока 55 000 руб. - 60 000 руб. ( на руки ), в зависимости от Вашего опыта! Карьерный рост : начальник юридического отдела, с з/п до 100 000 руб. Оплата телефона . Оформление по ТК РФ Социальный пакет по ТК РФ В ближайшие 3 - 4 месяца будет своя столовая – бесплатные обеды!  Проезд: М. Саларьево, 420 автобус, (10-12 минут), , остановка по требованию «Гостиничный Комплекс&quot;.\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "doc = \"<p>Юрист (ведущий) по претензионно - исковой и договорной работе в области «Аренда недвижимости» в ТР Комплекс «НовоМосковский», в город Московский, ул. Хабарова (м. Саларьево). В администрацию всего Комплекса, занимающейся арендой коммерческих, нежилых помещений – офисы и торговые площади. </p> <p><strong>Обязанности: </strong></p> <ul> <li>Переговоры с Арендаторами.</li> <li>Договорная работа.</li> <li>Претензионно - исковая работа (иски, претензии и т. д.)</li> <li>Представительство в Арбитражном суде и административном процессе. </li> </ul> <p> <strong>Требования: </strong></p> <ul> <li>Высшее Юридическое образование. </li> <li>Хорошее знание Договорной работы в области Аренды !!!</li> <li>Опыт работы с Арбитражном судом в области «Аренда недвижимости» !</li> </ul> <p><strong>Условия: </strong>.</p> <ul> <li>Гр/р: 5/2 , с 9.00 ч. – 18.00 ч.</li> <li>З/плата: пока 55 000 руб. - 60 000 руб. ( на руки ), в зависимости от Вашего опыта!</li> <li>Карьерный рост : начальник юридического отдела, с з/п до 100 000 руб.</li> <li>Оплата телефона .</li> <li>Оформление по ТК РФ</li> <li>Социальный пакет по ТК РФ</li> <li>В ближайшие 3 - 4 месяца будет своя столовая – бесплатные обеды!</li> </ul> Проезд: М. Саларьево, 420 автобус, (10-12 минут), , остановка по требованию «Гостиничный Комплекс&quot;.\"\n",
    "doc = re.sub('<[^>]*>', '', doc)\n",
    "print doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(113289, 120498)\n",
      "(113289, 3443)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "doc_words = pickle.load( open( \"processed_docs.p\", \"rb\" ) )\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(doc_words)\n",
    "# T = vectorizer.transform(['java разработчик программист'])\n",
    "\n",
    "print X.shape\n",
    "\n",
    "sel = VarianceThreshold(0.004)\n",
    "X2 = sel.fit_transform(X)\n",
    "# T2 = sel.transform(T)\n",
    "# print T2.toarray()\n",
    "\n",
    "print X2.shape\n",
    "\n",
    "transformer = TfidfTransformer()\n",
    "X2_tfidf = transformer.fit_transform(X2)\n",
    "\n",
    "pickle.dump( vectorizer, open( \"doc_vectorizer.p\", \"wb\" ) )\n",
    "pickle.dump( sel, open( \"count_vectorizer.p\", \"wb\" ) )\n",
    "pickle.dump( transformer, open( \"tfidf_transformer.p\", \"wb\" ) )\n",
    "\n",
    "# T2_tfidf = transformer.transform(T2)\n",
    "# result = cosine_similarity(T2_tfidf, X2_tfidf)\n",
    "\n",
    "# res = heapq.nlargest(10, range(len(result[0])), result[0].take)\n",
    "#print doc_words[result.argmax(axis=1)]\n",
    "# for i in res:\n",
    "#     print doc_words[i]\n",
    "   \n",
    "# from scipy import spatial\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# i = 0\n",
    "# for x_row in X2_tfidf:\n",
    "#     result = cosine_similarity(T2_tfidf, x_row)\n",
    "#     #result = spatial.distance.cosine(, )\n",
    "#     print result\n",
    "#     i = i+1\n",
    "#     if i > 10:\n",
    "#         break\n",
    "#result = spatial.distance.cosine(T2_tfidf, X2_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vacancies?per_page=200&date_from=2016-05-21T13:09:46&date_to=2016-05-21T13:14:46&page=0\n",
      "39\n",
      "starting t1\n",
      "21 sec\n"
     ]
    }
   ],
   "source": [
    "import httplib\n",
    "import json\n",
    "import datetime\n",
    "import MySQLdb\n",
    "import ConfigParser\n",
    "import time\n",
    "import threading\n",
    "from dateutil.parser import parse\n",
    "\n",
    "current_time = lambda: int(round(time.time()))\n",
    "start = current_time()\n",
    "    \n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "cursor.close()\n",
    "\n",
    "def get_vacancy_ids():\n",
    "    vacancy_ids = []\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    per_page = 200\n",
    "    page = 0\n",
    "    count = per_page\n",
    "    date_from = (datetime.datetime.now() - datetime.timedelta(minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    date_to = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    while count == per_page:\n",
    "        path = (\"/vacancies?per_page={}&date_from={}&date_to={}&page={}\"\n",
    "                .format(per_page, date_from, date_to, page))\n",
    "        print path\n",
    "\n",
    "        conn.request(\"GET\", path, headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        vacancies = r1.read()\n",
    "\n",
    "        count = len(json.loads(vacancies)['items'])\n",
    "        page = page+1\n",
    "        for item in json.loads(vacancies)['items']:\n",
    "            vacancy_ids.append(item['id'])\n",
    "    return vacancy_ids\n",
    "        \n",
    "\n",
    "def process_vacancies(vacancy_ids):\n",
    "    headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "    config = ConfigParser.ConfigParser()\n",
    "    config.readfp(open('my.cfg'))\n",
    "\n",
    "    db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                         port=config.getint('mysqld', 'port'), \n",
    "                         user=config.get('mysqld', 'user'), \n",
    "                         passwd=config.get('mysqld', 'password'), \n",
    "                         db=config.get('mysqld', 'database') )\n",
    "    db.set_character_set('utf8')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute('SET NAMES utf8;')\n",
    "    cursor.execute('SET CHARACTER SET utf8;')\n",
    "    cursor.execute('SET character_set_connection=utf8;')\n",
    "    cursor.close()\n",
    "\n",
    "    for vac_id in vacancy_ids:\n",
    "        conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        vacancy = r1.read()\n",
    "        vacancy_json = json.loads(vacancy)\n",
    "        cursor = db.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "          INSERT INTO vacancies (id, updated, item) \n",
    "          VALUES (%s, %s, %s)\n",
    "          ON DUPLICATE KEY UPDATE \n",
    "            updated  = VALUES(updated), \n",
    "            item   = VALUES(item)\"\"\", (vac_id, \n",
    "                                       parse(vacancy_json['published_at']).strftime(\"%Y-%m-%d %H:%M:%S\"), \n",
    "                                       vacancy))\n",
    "        db.commit() \n",
    "        cursor.close()\n",
    "    db.close()\n",
    "\n",
    "ids = get_vacancy_ids()\n",
    "print len(ids)\n",
    "vac_id_chunks=[ids[x:x+100] for x in xrange(0, len(ids), 100)]\n",
    "t_num = 1;\n",
    "threads = []\n",
    "for vac_id_chunk in vac_id_chunks:\n",
    "    print 'starting t{}'.format(t_num)\n",
    "    t_num = t_num + 1\n",
    "    t = threading.Thread(target=process_vacancies, kwargs={'vacancy_ids': vac_id_chunk})\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "\n",
    "db.close()\n",
    "print \"{} sec\".format(current_time()-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#vacancy vectorizer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"\"\"SELECT item FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT 10\"\"\")\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "features_by_type = {}\n",
    "for employment_type in dictionaries_json['employment']:\n",
    "    features_by_type[employment_type['id']] = []\n",
    "\n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "vectorizer = pickle.load( open( \"doc_vectorizer.p\", \"rb\" ) )\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "# # специацизации | уровни зп| навыки (собираем из вакансий) | \n",
    "# # список слов заголовка (собираем из вакансий) | текст (собираем из вакансий)\n",
    "for item in cursor:\n",
    "    feature = []\n",
    "    vacancy = json.loads(item[0])\n",
    "    \n",
    "    #specializations\n",
    "    vac_specializations = vacancy['specializations']\n",
    "    spec_features = [0] * len(spec_ids)\n",
    "    for vac_spec in vac_specializations:\n",
    "        spec_features[spec_ids.index(vac_spec['id'])] = 1        \n",
    "    feature = feature + spec_features\n",
    "    \n",
    "    #salary\n",
    "    salary = 0\n",
    "    if vacancy['salary'] != None:\n",
    "        if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "            salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "        elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "            salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "        elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "            salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "    max_salary = 500000.0\n",
    "    if salary >= max_salary:\n",
    "        salary = max_salary\n",
    "    salary = salary/max_salary\n",
    "    feature.append(salary)\n",
    "    \n",
    "    #keyskills\n",
    "    skill_features = [0] * len(key_skills)\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if word in key_skills:\n",
    "                skill_features[key_skills.index(word)] = 1 \n",
    "    feature = feature + skill_features\n",
    "    \n",
    "    #title\n",
    "    title_features = [0] * len(title_words)\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if title_word in title_words:\n",
    "            title_features[title_words.index(title_word)] = 1\n",
    "    feature = feature + title_features\n",
    "    \n",
    "    #description\n",
    "    p_doc = \"\"\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        p_doc = p_doc + \" \" + word\n",
    "    p_doc_vec = vectorizer.transform([p_doc])\n",
    "    p_doc_vec2 = count_vectorizer.transform(p_doc_vec)\n",
    "    p_doc_vec_tfidf = tfidf_transformer.transform(p_doc_vec2)\n",
    "    feature = feature + list(p_doc_vec_tfidf.toarray()[0])\n",
    "    features_by_type[vacancy['employment']['id']].append(feature)\n",
    "\n",
    "pickle.dump( features_by_type, open( \"features_by_type.p\", \"wb\" ) )\n",
    "cursor.close()\n",
    "\n",
    "db.close()\n",
    "\n",
    "print('done')\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      "Java Developer\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-ea316ec2e81d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    120\u001b[0m     \u001b[0mp_doc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp_doc\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mp_doc_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp_doc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m \u001b[0mp_doc_vec2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_doc_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[0mp_doc_vec_tfidf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_transformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_doc_vec2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp_doc_vec_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    750\u001b[0m         \u001b[0mindptr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mj_indices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 238\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/feature_extraction/text.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    506\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    507\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    509\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "#resume vectorizer\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "headers = {\"User-Agent\": \"hh-recommender\", \"Authorization\" : \"Bearer T5MIT6GVV85LSVR75CB7U768TR3PFGS990I3QJNFV6A4CBQJF6M30G0MOT8U2V8I\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/mine\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "me = r1.read()\n",
    "me_json = json.loads(me)\n",
    "print me_json['items'][0]['id']\n",
    "\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(me_json['items'][0]['id']), headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "resume = r1.read()\n",
    "resume_json = json.loads(resume)\n",
    "\n",
    "feature = []\n",
    "resume_type = 'full'\n",
    "\n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "vectorizer = pickle.load( open( \"doc_vectorizer.p\", \"rb\" ) )\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "# # специацизации | уровни зп| навыки (собираем из вакансий) | \n",
    "# # список слов заголовка (собираем из вакансий) | текст (собираем из вакансий)\n",
    "\n",
    "resume_type = resume_json['employment']['id']\n",
    "\n",
    "#specializations\n",
    "res_specializations = resume_json['specialization']\n",
    "spec_features = [0] * len(spec_ids)\n",
    "for res_spec in res_specializations:\n",
    "    spec_features[spec_ids.index(res_spec['id'])] = 1        \n",
    "feature = feature + spec_features\n",
    "\n",
    "#salary\n",
    "salary = 0\n",
    "if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "    salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "max_salary = 500000.0\n",
    "if salary >= max_salary:\n",
    "    salary = max_salary\n",
    "salary = salary/max_salary\n",
    "feature.append(salary)\n",
    "\n",
    "#keyskills\n",
    "skill_features = [0] * len(key_skills)\n",
    "res_skills = resume_json['skill_set']\n",
    "for skill in res_skills:\n",
    "    words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        if word in key_skills:\n",
    "            skill_features[key_skills.index(word)] = 1 \n",
    "feature = feature + skill_features\n",
    "\n",
    "#title\n",
    "title_features = [0] * len(title_words)\n",
    "title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', title.strip())\n",
    "for title_word in words:\n",
    "    title_word = stemmer.stemWord(title_word)\n",
    "    if title_word in title_words:\n",
    "        title_features[title_words.index(title_word)] = 1\n",
    "feature = feature + title_features\n",
    "\n",
    "#description\n",
    "doc = resume_json['skills']\n",
    "if resume_json['experience'] != None and len(resume_json['experience']) > 0:\n",
    "    doc =  doc + \" \" + resume_json['experience'][0]['description']\n",
    "p_doc = \"\"\n",
    "doc = re.sub('<[^>]*>', '', doc.lower())\n",
    "doc = re.sub('&quot;', '', doc)\n",
    "doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', doc.strip())\n",
    "for word in words:\n",
    "    word = stemmer.stemWord(word)\n",
    "    p_doc = p_doc + \" \" + word\n",
    "p_doc_vec = vectorizer.transform([p_doc])\n",
    "p_doc_vec2 = count_vectorizer.transform(p_doc_vec)\n",
    "p_doc_vec_tfidf = tfidf_transformer.transform(p_doc_vec2)\n",
    "feature = feature + list(p_doc_vec_tfidf.toarray()[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vacancies done\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      " опытн разработчик программн обеспечен им широк спектр навык знан технолог разрабатыва быстр программ работа больш объем дан многопотоков сред уделя больш вниман качеств код удобств дальн поддержк аналитическ подхож решен задач. им хорош знан алгоритм структур данных. хобби, увлечен интернет-технологии, путешествия, фотография, кулинария. организова вед местн школ кружок по программирован для детей. последн врем интерес машин обучен opencv профил на habrahabr: http://habrahabr.ru/users/shurik2533/  java developer  java java ee oracle pl/sql python data analysis big data recommender systems machine learning\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs processed\n",
      "9924\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "#из каждой профобласти извлекаем ключевые слова, релевантные для нее.\n",
    "#есть и общие слова. каждая профобласть должна иметь свой вектор множителей весов\n",
    "#верная зп должна добавлять какой-то вес (сортировать результат)\n",
    "#для каждой профобласти считать свой tfidf? и вектор кол-ва найденных слов умножать на него?\n",
    "#не забыть учитывать тип занятости\n",
    "\n",
    "#собираем вектор из таких фич, чтобы определенная часть в большей степени соответствовала определенной специялизации\n",
    "#тогда потом, когда будем считать косинусное расстояние будет больше созпадать с нужной профобластью\n",
    "\n",
    "\n",
    "#!считал не верно. считаем tfidf для каждой специализации. на основе этого выбираем самые значимые из специализации\n",
    "#слова из которых затем строитятся вектора. Попробовать использовать vocabulary\n",
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "db = TinyDB('vacancies.json')\n",
    "db2 = TinyDB('vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "# db = TinyDB('vacancies.json')\n",
    "# vacancies = db.all()\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "vac_by_spec = {}\n",
    "i = 0\n",
    "#keyskills, title, description\n",
    "for vacancy in vacancies:\n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "            \n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "    \n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "    \n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    \n",
    "    for spec in vacancy['specializations']:\n",
    "        if spec['id'] not in vac_by_spec:\n",
    "            vac_by_spec[spec['id']] = []\n",
    "        vac_by_spec[spec['id']].append(p_doc)\n",
    "    \n",
    "#     i = i+1\n",
    "#     if i > 100:\n",
    "#         break\n",
    "print 'docs processed'\n",
    "words = set()\n",
    "for key in vac_by_spec:\n",
    "    corpus = vac_by_spec[key]\n",
    "    vectorizer = CountVectorizer(min_df=1)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    transformer = TfidfTransformer()\n",
    "    X_tfidf = transformer.fit_transform(X)\n",
    "    spec_means = X_tfidf.mean(axis=0)\n",
    "    spec_means_arr = numpy.squeeze(numpy.asarray(spec_means))\n",
    "    res = heapq.nlargest(350, range(len(spec_means_arr)), spec_means_arr.take)\n",
    "    for i in res:\n",
    "        words.add(vectorizer.get_feature_names()[i])\n",
    "        \n",
    "print len(words)\n",
    "pickle.dump( words, open( \"spec_dict2.p\", \"wb\" ) )\n",
    "print 'finish'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.6316672   0.44943642  0.6316672   0.        ]\n",
      " [ 0.          0.81818021  0.          0.57496187]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "corpus = ['aa bb cc', 'bb bb dd']\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X)\n",
    "print X_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish collect docs\n",
      "finish\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "db = TinyDB('vacancies.json')\n",
    "db2 = TinyDB('vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "# db = TinyDB('vacancies.json')\n",
    "# vacancies = db.all()\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "i = 0\n",
    "p_docs = []\n",
    "#keyskills, title, description\n",
    "for vacancy in vacancies:\n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "            \n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "    \n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "    \n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    p_docs.append(p_doc)\n",
    "    \n",
    "print 'finish collect docs'\n",
    "\n",
    "voc = pickle.load( open( \"spec_dict2.p\", \"rb\" ) )\n",
    "vectorizer = CountVectorizer(min_df=1, vocabulary=voc)\n",
    "X = vectorizer.fit_transform(p_docs)\n",
    "transformer = TfidfTransformer()\n",
    "X_tfidf = transformer.fit_transform(X)\n",
    "pickle.dump( vectorizer, open( \"count_vectorizer.p\", \"wb\" ) )\n",
    "pickle.dump( transformer, open( \"tfidf_transformer.p\", \"wb\" ) )\n",
    "\n",
    "print 'finish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish 113289.0 - 35969.0 - 0.317497727052\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tinydb import TinyDB\n",
    "import pickle\n",
    "import Stemmer\n",
    "import re \n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "\n",
    "db = TinyDB('vacancies.json')\n",
    "db2 = TinyDB('vacancies2.json')\n",
    "vacancies1 = db.all()\n",
    "vacancies2 = db2.all()\n",
    "vacancies = vacancies1+vacancies2\n",
    "\n",
    "# db = TinyDB('vacancies.json')\n",
    "# vacancies = db.all()\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "\n",
    "i = 0.0\n",
    "p_docs = []\n",
    "none = 0.0\n",
    "#keyskills, title, description\n",
    "for vacancy in vacancies:\n",
    "    if vacancy['salary'] == None:\n",
    "        none = none+1\n",
    "    i = i+1\n",
    "#     if i > 10:\n",
    "#         break\n",
    "    \n",
    "print 'finish {} - {} - {}'.format(i, none, none/i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      "vacancies done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3c680237e6be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeatures_by_type\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresume_type\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mheapq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlargest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m    879\u001b[0m     \u001b[1;31m# to avoid recursive import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 881\u001b[1;33m     \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/metrics/pairwise.pyc\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed)\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m         \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/shurik2533/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.pyc\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    413\u001b[0m                              \u001b[1;34m\" a minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m                              % (n_features, shape_repr, ensure_min_features,\n\u001b[1;32m--> 415\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mwarn_on_dtype\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdtype_orig\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype_orig\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 feature(s) (shape=(1, 0)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "features_by_type = {}\n",
    "for employment_type in dictionaries_json['employment']:\n",
    "    features_by_type[employment_type['id']] = []\n",
    "\n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "\n",
    "cursor = db.cursor()\n",
    "cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT 10\"\"\")\n",
    "data = []\n",
    "for item in cursor:\n",
    "    feature = []\n",
    "    vacancy = json.loads(item[0])\n",
    "    data.append(vacancy['name'])\n",
    "    \n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "            \n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "    \n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    vac_skills = vacancy['key_skills']\n",
    "    for skill in vac_skills:\n",
    "        words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "    \n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    \n",
    "    feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "    tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "    \n",
    "    if features_by_type[vacancy['employment']['id']] == None:\n",
    "        features_by_type[vacancy['employment']['id']] = []\n",
    "    features_by_type[vacancy['employment']['id']].append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "cursor.close()\n",
    "\n",
    "db.close()\n",
    "\n",
    "print('vacancies done')\n",
    "\n",
    "\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\", \"Authorization\" : \"Bearer T5MIT6GVV85LSVR75CB7U768TR3PFGS990I3QJNFV6A4CBQJF6M30G0MOT8U2V8I\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/mine\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "me = r1.read()\n",
    "me_json = json.loads(me)\n",
    "\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(me_json['items'][0]['id']), headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "resume = r1.read()\n",
    "resume_json = json.loads(resume)\n",
    "\n",
    "feature = []\n",
    "resume_type = 'full'\n",
    "\n",
    "resume_type = resume_json['employment']['id']\n",
    "\n",
    "#description\n",
    "p_doc = ''\n",
    "doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "doc = re.sub('&quot;', '', doc)\n",
    "doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', doc.strip())\n",
    "for word in words:\n",
    "    word = stemmer.stemWord(word.strip())\n",
    "    if len(word.strip()) > 1:\n",
    "        p_doc = p_doc + \" \" + word\n",
    "\n",
    "#title\n",
    "p_title = ''\n",
    "title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "words = re.split(r'\\s{1,}', title.strip())\n",
    "for title_word in words:\n",
    "    title_word = stemmer.stemWord(title_word)\n",
    "    if len(title_word.strip()) > 1:\n",
    "        p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "#keyskills\n",
    "p_skills = ''\n",
    "res_skills = resume_json['skill_set']\n",
    "for skill in res_skills:\n",
    "    words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word)\n",
    "        if len(word.strip()) > 1:\n",
    "            p_skills = p_skills + \" \" + word.strip()\n",
    "\n",
    "p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "\n",
    "feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "feature = tfidf_transformer.transform(feature_p_doc)\n",
    "\n",
    "features = features_by_type[resume_type]\n",
    "\n",
    "result = cosine_similarity(feature, features)\n",
    "res = heapq.nlargest(10, range(len(result[0])), result[0].take)\n",
    "for i in res:\n",
    "    print result[0][i]\n",
    "    print data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9924\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "9924\n",
      "0.583040144288\n",
      "0.518259144701\n",
      "0.448771156518\n",
      "0.435977176192\n"
     ]
    }
   ],
   "source": [
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "doc = \"java разработчик программист api rrreree\"\n",
    "X = count_vectorizer.transform([doc])\n",
    "X1 = tfidf_transformer.transform(X)\n",
    "print len(X.toarray()[0])\n",
    "for v in X.toarray()[0]:\n",
    "    if v != 0:\n",
    "        print v\n",
    "\n",
    "print len(X1.toarray()[0])\n",
    "for v in X1.toarray()[0]:\n",
    "    if v != 0:\n",
    "        print v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " любл готов вредн привычек не им аккуратн  повар  поварск дел\n",
      "processed 1000 vаcancies\n",
      "processed 2000 vаcancies\n",
      "processed 3000 vаcancies\n",
      "processed 4000 vаcancies\n",
      "processed 5000 vаcancies\n",
      "processed 6000 vаcancies\n",
      "processed 7000 vаcancies\n",
      "processed 8000 vаcancies\n",
      "processed 9000 vаcancies\n",
      "processed 10000 vаcancies\n",
      "0.369745464628\n",
      "17136190\n",
      "0.352538100623\n",
      "16764555\n",
      "0.28401864007\n",
      "17135847\n",
      "0.28179223153\n",
      "17135823\n",
      "0.2696830206\n",
      "17137164\n",
      "0.265194214145\n",
      "17134022\n",
      "0.239828494588\n",
      "17135165\n",
      "0.229601478081\n",
      "17136834\n",
      "0.220165661334\n",
      "17136978\n",
      "0.204926291534\n",
      "17135840\n",
      "0.181030884969\n",
      "16827698\n",
      "0.174920378348\n",
      "16026656\n",
      "0.166019536197\n",
      "16710006\n",
      "0.159519643985\n",
      "17134738\n",
      "0.157873346428\n",
      "16722702\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "def get_my_resume():\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    headers = {\"User-Agent\": \"hh-recommender\", \"Authorization\" : \"Bearer T5MIT6GVV85LSVR75CB7U768TR3PFGS990I3QJNFV6A4CBQJF6M30G0MOT8U2V8I\"}\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/resumes/mine\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "    me = r1.read()\n",
    "    me_json = json.loads(me)\n",
    "\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "#     conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(me_json['items'][0]['id']), headers=headers)\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/resumes/{}\".format(\"5bdc936300031347c20039ed1f644868457044\"), headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "    resume = r1.read()\n",
    "    resume_json = json.loads(resume)\n",
    "\n",
    "    feature = []\n",
    "    resume_type = 'full'\n",
    "\n",
    "    resume_type = resume_json['employment']['id']\n",
    "\n",
    "    #description\n",
    "    p_doc = ''\n",
    "    doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "    doc = re.sub('&quot;', '', doc)\n",
    "    doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', doc.strip())\n",
    "    for word in words:\n",
    "        word = stemmer.stemWord(word.strip())\n",
    "        if len(word.strip()) > 1:\n",
    "            p_doc = p_doc + \" \" + word\n",
    "\n",
    "    #title\n",
    "    p_title = ''\n",
    "    title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "    words = re.split(r'\\s{1,}', title.strip())\n",
    "    for title_word in words:\n",
    "        title_word = stemmer.stemWord(title_word)\n",
    "        if len(title_word.strip()) > 1:\n",
    "            p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "    #keyskills\n",
    "    p_skills = ''\n",
    "    res_skills = resume_json['skill_set']\n",
    "    for skill in res_skills:\n",
    "        words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word)\n",
    "            if len(word.strip()) > 1:\n",
    "                p_skills = p_skills + \" \" + word.strip()\n",
    "                \n",
    "    #salary\n",
    "    salary = None\n",
    "    if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "        salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "    max_salary = 500000.0\n",
    "    if salary >= max_salary:\n",
    "        salary = max_salary\n",
    "\n",
    "    p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "    print p_doc\n",
    "    feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "    feature = tfidf_transformer.transform(feature_p_doc)\n",
    "    return feature.toarray(), salary\n",
    "\n",
    "\n",
    "def get_vacancies(offset, rows):\n",
    "    features = []\n",
    "\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    #будет задвоение, когда во время выборки в несколько запросом добавляются новые данные\n",
    "    cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 5 DAY) LIMIT {}, {}\"\"\".format(offset, rows))\n",
    "    vacancy_ids = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    for item in cursor:\n",
    "        feature = []\n",
    "        vacancy = json.loads(item[0])\n",
    "        vacancy_ids.append(vacancy['id'])\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        salaries.append(salary)\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "        \n",
    "        cnt = 0\n",
    "        for f in tfidf_feature_p_doc.toarray()[0]:\n",
    "            if f > 0:\n",
    "                cnt = cnt + 1\n",
    "            \n",
    "        if cnt > 1:\n",
    "            features.append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "    cursor.close()\n",
    "    return features, vacancy_ids, salaries\n",
    "\n",
    "def get_recommended(resume_feature, vacancy_features, resume_salary, vacancy_salaries, vacancy_ids):\n",
    "    new_vacancy_features = []\n",
    "    new_vacancy_ids = []\n",
    "    if resume_salary == None:\n",
    "        new_vacancy_features = vacancy_features\n",
    "        new_vacancy_ids = vacancy_ids\n",
    "    else:\n",
    "        i = 0\n",
    "        for vac_salary in vacancy_salaries:\n",
    "            if vac_salary == None:\n",
    "                new_vacancy_features.append(vacancy_features[i])\n",
    "                new_vacancy_ids.append(vacancy_ids[i])\n",
    "            else:\n",
    "                min_resume_salary = resume_salary - (resume_salary * 0.2)\n",
    "                max_resume_salary = resume_salary + (resume_salary * 0.8)\n",
    "                if vac_salary >= min_resume_salary and vac_salary <= max_resume_salary:\n",
    "                    new_vacancy_features.append(vacancy_features[i])\n",
    "                    new_vacancy_ids.append(vacancy_ids[i])\n",
    "                \n",
    "            i = i+1    \n",
    "    \n",
    "    if len(new_vacancy_features) > 0:\n",
    "        c_result = cosine_similarity(resume_feature, new_vacancy_features)\n",
    "        res = heapq.nlargest(15, range(len(c_result[0])), c_result[0].take)\n",
    "\n",
    "        similarities = []\n",
    "        ids = []\n",
    "        for j in res:\n",
    "            similarities.append(c_result[0][j])\n",
    "            ids.append(new_vacancy_ids[j])\n",
    "    return similarities, ids\n",
    "\n",
    "feature, salary = get_my_resume()\n",
    "\n",
    "count = 1000\n",
    "similarities = []\n",
    "ids = []\n",
    "features = get_vacancies(0, count)\n",
    "features, vacancy_ids, salaries = get_vacancies(0, count)\n",
    "\n",
    "f_len = len(features)\n",
    "\n",
    "r_similarities, r_ids = get_recommended(feature, features, salary, salaries, vacancy_ids)\n",
    "similarities = similarities + r_similarities\n",
    "ids = ids + r_ids\n",
    "\n",
    "i = 0\n",
    "while f_len > 0:\n",
    "    features, vacancy_ids, salaries = get_vacancies(i*count, count)\n",
    "    f_len = len(features)\n",
    "    if f_len > 0:\n",
    "        r_similarities, r_ids = get_recommended(feature, features, salary, salaries, vacancy_ids)\n",
    "        similarities = similarities + r_similarities\n",
    "        ids = ids + r_ids\n",
    "    \n",
    "    i = i+1\n",
    "    print 'processed {} vаcancies'.format(i*count)\n",
    "        \n",
    "    if i == 10:\n",
    "        break\n",
    "        \n",
    "max_similarities = heapq.nlargest(15, range(len(numpy.asarray(similarities))), numpy.asarray(similarities).take)\n",
    "for ind in max_similarities:\n",
    "    print similarities[ind]\n",
    "    print ids[ind]\n",
    "    \n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-01 17:13:25.445153\n",
      "processed 1000 vаcancies\n",
      "processed 2000 vаcancies\n",
      "processed 3000 vаcancies\n",
      "processed 4000 vаcancies\n",
      "processed 5000 vаcancies\n",
      "processed 6000 vаcancies\n",
      "processed 7000 vаcancies\n",
      "processed 8000 vаcancies\n",
      "processed 9000 vаcancies\n",
      "processed 10000 vаcancies\n",
      "processed 11000 vаcancies\n",
      "processed 12000 vаcancies\n",
      "processed 13000 vаcancies\n",
      "processed 14000 vаcancies\n",
      "processed 15000 vаcancies\n",
      "processed 16000 vаcancies\n",
      "processed 17000 vаcancies\n",
      "processed 18000 vаcancies\n",
      "processed 19000 vаcancies\n",
      "processed 20000 vаcancies\n",
      "c13f3c94ff023cf7d20039ed1f30766857326a\n",
      "for 17185733 similarity is 0.241420428222\n",
      "for 16858436 similarity is 0.231747031736\n",
      "for 15685222 similarity is 0.217491396879\n",
      "for 17098616 similarity is 0.213119123972\n",
      "for 16825405 similarity is 0.209957690914\n",
      "for 17197588 similarity is 0.201732166924\n",
      "for 17193832 similarity is 0.197981865485\n",
      "for 16165915 similarity is 0.195952985719\n",
      "for 16716632 similarity is 0.193102134053\n",
      "for 17190074 similarity is 0.190184213484\n",
      "for 17188217 similarity is 0.189807166359\n",
      "for 17190141 similarity is 0.183846894776\n",
      "for 15045209 similarity is 0.182566398696\n",
      "for 17124148 similarity is 0.179818133491\n",
      "for 15045200 similarity is 0.177550143234\n",
      "for 17182223 similarity is 0.17651011305\n",
      "for 17182223 similarity is 0.17651011305\n",
      "for 15431158 similarity is 0.170411555353\n",
      "for 16679291 similarity is 0.161486448948\n",
      "for 16711861 similarity is 0.151652867442\n",
      "4527975dff02f59d5f0039ed1f534356744c6e\n",
      "for 17190592 similarity is 0.32853290184\n",
      "for 16803389 similarity is 0.308989905585\n",
      "for 15685222 similarity is 0.297546103535\n",
      "for 17182094 similarity is 0.288522303422\n",
      "for 17182094 similarity is 0.288522303422\n",
      "for 17014281 similarity is 0.277741825232\n",
      "for 17185733 similarity is 0.27079151884\n",
      "for 17190074 similarity is 0.244443850617\n",
      "for 17182193 similarity is 0.243885821073\n",
      "for 17182193 similarity is 0.243885821073\n",
      "for 16825405 similarity is 0.243197307423\n",
      "for 16585330 similarity is 0.239942405205\n",
      "for 17182830 similarity is 0.225159026085\n",
      "for 17182830 similarity is 0.225159026085\n",
      "for 17183677 similarity is 0.224284520785\n",
      "for 16858436 similarity is 0.218734527172\n",
      "for 16683078 similarity is 0.216280094867\n",
      "for 17183052 similarity is 0.207465712192\n",
      "for 16165915 similarity is 0.198576857039\n",
      "for 17187549 similarity is 0.1932426447\n",
      "848f3de0ff000c40da0039ed1f736563726574\n",
      "for 17185733 similarity is 0.261828800225\n",
      "for 16858436 similarity is 0.242504778706\n",
      "for 17098616 similarity is 0.228706829185\n",
      "for 15685222 similarity is 0.227540897071\n",
      "for 16165915 similarity is 0.222043158323\n",
      "for 16825405 similarity is 0.22128041777\n",
      "for 17190141 similarity is 0.200561978596\n",
      "for 17190074 similarity is 0.198996694012\n",
      "for 17197588 similarity is 0.198967574545\n",
      "for 17188217 similarity is 0.197002169865\n",
      "for 17193832 similarity is 0.195268668255\n",
      "for 16716632 similarity is 0.194427885552\n",
      "for 17124148 similarity is 0.191434033214\n",
      "for 15431158 similarity is 0.183046982862\n",
      "for 15045209 similarity is 0.180064459207\n",
      "for 16711861 similarity is 0.178968946636\n",
      "for 15045200 similarity is 0.175116947872\n",
      "for 17182223 similarity is 0.174091170544\n",
      "for 17182223 similarity is 0.174091170544\n",
      "for 17193643 similarity is 0.171526618719\n",
      "04589be6ff01f2fef60039ed1f56534e424e42\n",
      "for 17196267 similarity is 0.347377411128\n",
      "for 17183846 similarity is 0.31507728119\n",
      "for 17194918 similarity is 0.259246490004\n",
      "for 17198776 similarity is 0.249693840602\n",
      "for 17196960 similarity is 0.193201172172\n",
      "for 16476271 similarity is 0.171531032001\n",
      "for 17052776 similarity is 0.168706725035\n",
      "for 17188895 similarity is 0.15081751129\n",
      "for 17188748 similarity is 0.14584607867\n",
      "for 17184427 similarity is 0.141031629954\n",
      "for 17192323 similarity is 0.140576082179\n",
      "for 17182928 similarity is 0.125456917786\n",
      "for 17193953 similarity is 0.124317609046\n",
      "for 17198486 similarity is 0.122805150635\n",
      "for 17182648 similarity is 0.121051896874\n",
      "for 17182648 similarity is 0.121051896874\n",
      "for 17182688 similarity is 0.117621283548\n",
      "for 17182688 similarity is 0.117621283548\n",
      "for 17192040 similarity is 0.112092600228\n",
      "for 16583323 similarity is 0.110703442662\n",
      "a588f755ff02418ca00039ed1f5a75387a3955\n",
      "for 17181878 similarity is 0.0\n",
      "for 17181879 similarity is 0.0\n",
      "for 17181881 similarity is 0.0\n",
      "for 17181892 similarity is 0.0\n",
      "for 17181894 similarity is 0.0\n",
      "for 17181908 similarity is 0.0\n",
      "for 17181911 similarity is 0.0\n",
      "for 16941968 similarity is 0.0\n",
      "for 17181916 similarity is 0.0\n",
      "for 17181924 similarity is 0.0\n",
      "for 17181925 similarity is 0.0\n",
      "for 17181938 similarity is 0.0\n",
      "for 17181944 similarity is 0.0\n",
      "for 17181949 similarity is 0.0\n",
      "for 17181961 similarity is 0.0\n",
      "for 17181967 similarity is 0.0\n",
      "for 17181972 similarity is 0.0\n",
      "for 14690439 similarity is 0.0\n",
      "for 17181974 similarity is 0.0\n",
      "for 17181980 similarity is 0.0\n",
      "d7c90e2eff02418ca50039ed1f356b4e504642\n",
      "for 16977650 similarity is 0.328960023028\n",
      "for 17191425 similarity is 0.303407733652\n",
      "for 16895056 similarity is 0.295275719024\n",
      "for 17187260 similarity is 0.252307966964\n",
      "for 17182323 similarity is 0.241233208258\n",
      "for 17182323 similarity is 0.241233208258\n",
      "for 17192228 similarity is 0.239448984579\n",
      "for 16873362 similarity is 0.232595859938\n",
      "for 17192930 similarity is 0.218048261794\n",
      "for 16898603 similarity is 0.209261470205\n",
      "for 17190491 similarity is 0.194549025145\n",
      "for 17183024 similarity is 0.184081680497\n",
      "for 17187244 similarity is 0.162514360595\n",
      "for 17066755 similarity is 0.160708895232\n",
      "for 15065374 similarity is 0.155506579317\n",
      "for 15065375 similarity is 0.155506579317\n",
      "for 16432831 similarity is 0.154820607105\n",
      "for 17190667 similarity is 0.154806479736\n",
      "for 16976331 similarity is 0.153476546131\n",
      "for 17189731 similarity is 0.15340405254\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574\n",
      "for 15431158 similarity is 0.362254145256\n",
      "for 17124148 similarity is 0.29410149051\n",
      "for 15685222 similarity is 0.289767363695\n",
      "for 17182094 similarity is 0.271701788903\n",
      "for 17182094 similarity is 0.271701788903\n",
      "for 17182830 similarity is 0.258590119731\n",
      "for 17182830 similarity is 0.258590119731\n",
      "for 16825405 similarity is 0.255158467325\n",
      "for 16165915 similarity is 0.246060346584\n",
      "for 17193561 similarity is 0.244257352046\n",
      "for 17185733 similarity is 0.233767330032\n",
      "for 16803389 similarity is 0.224634473624\n",
      "for 16679291 similarity is 0.218911853448\n",
      "for 17182193 similarity is 0.211796680403\n",
      "for 17182193 similarity is 0.211796680403\n",
      "for 14121053 similarity is 0.210809709834\n",
      "for 17182643 similarity is 0.210268995969\n",
      "for 17182643 similarity is 0.210268995969\n",
      "for 17190592 similarity is 0.209984133237\n",
      "for 17183052 similarity is 0.197427628001\n",
      "total time 117.014307022 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "start_time = time.time()\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "    \n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "def get_resumes():\n",
    "    salaries = []\n",
    "    features = []\n",
    "    ids = []\n",
    "    areas = []\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(\"\"\"SELECT item FROM resumes WHERE is_active=1\"\"\")\n",
    "    for item in cursor:\n",
    "        resume_json = json.loads(item[0])\n",
    "        feature = []\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        if resume_json['skills'] != None:\n",
    "            doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "            doc = re.sub('&quot;', '', doc)\n",
    "            doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', doc.strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word.strip())\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        if resume_json['title'] != None:\n",
    "            title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', title.strip())\n",
    "            for title_word in words:\n",
    "                title_word = stemmer.stemWord(title_word)\n",
    "                if len(title_word.strip()) > 1:\n",
    "                    p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        res_skills = resume_json['skill_set']\n",
    "        for skill in res_skills:\n",
    "            words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "\n",
    "        #salary\n",
    "        salary = None\n",
    "        if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "            salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        \n",
    "        \n",
    "        res_areas = []\n",
    "        if resume_json['area'] == None:\n",
    "            res_areas.append(areas_map[\"1\"])\n",
    "        else :\n",
    "            res_areas.append(areas_map[resume_json['area']['id']])\n",
    "        for area in resume_json['relocation']['area']:\n",
    "            res_areas.append(areas_map[area['id']])\n",
    "        areas.append(res_areas)\n",
    "        \n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        feature = tfidf_transformer.transform(feature_p_doc)\n",
    "        features.append(feature.toarray())\n",
    "        salaries.append(salary)\n",
    "        ids.append(resume_json['id'])\n",
    "    cursor.close()\n",
    "    return features, salaries, ids, areas\n",
    "\n",
    "\n",
    "def get_vacancies(offset, rows):\n",
    "    features = []\n",
    "\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    #будет задвоение, когда во время выборки в несколько запросом добавляются новые данные\n",
    "    cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT {}, {}\"\"\".format(offset, rows))\n",
    "    vacancy_ids = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    titles = []\n",
    "    areas = []\n",
    "    for item in cursor:\n",
    "        feature = []\n",
    "        vacancy = json.loads(item[0])\n",
    "        vacancy_ids.append(vacancy['id'])\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "                \n",
    "        titles.append(vacancy['name'])\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        salaries.append(salary)\n",
    "        try:\n",
    "            areas.append(areas_map[vacancy['area']['id']])\n",
    "        except KeyError:\n",
    "            print 'missed area id {} for vacancy {}'.format(vacancy['area']['id'], vacancy['id'])\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area_vacancy = r1.read()\n",
    "            missed_area_vacancy_json = json.loads(missed_area_vacancy)\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/areas/{}\"\n",
    "                         .format(missed_area_vacancy_json['area']['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area = r1.read()\n",
    "            missed_area_json = json.loads(missed_area)\n",
    "            areas_map[vacancy['area']['id']] = missed_area_json['parent_id']\n",
    "            areas.append(missed_area_json['parent_id'])\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "            \n",
    "        features.append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "    cursor.close()\n",
    "    return features, vacancy_ids, salaries, titles, areas\n",
    "\n",
    "def get_recommended(resume_feature, vacancy_features, resume_salary, vacancy_salaries, vacancy_ids, vac_titles, resume_areas, vacancies_area):\n",
    "    pre_vacancy_features = []\n",
    "    pre_vacancy_ids = []\n",
    "    pre_vac_titles = []\n",
    "    pre_vacancy_salaries = []\n",
    "    j = 0\n",
    "    for vac_area in vacancies_area:\n",
    "        if vac_area in resume_areas:\n",
    "            pre_vacancy_features.append(vacancy_features[j])\n",
    "            pre_vacancy_ids.append(vacancy_ids[j])\n",
    "            pre_vac_titles.append(vac_titles[j])\n",
    "            pre_vacancy_salaries.append(vacancy_salaries[j])\n",
    "        j = j+1\n",
    "        \n",
    "    new_vacancy_features = []\n",
    "    new_vacancy_ids = []\n",
    "    new_vac_titles = []\n",
    "    if resume_salary == None:\n",
    "        new_vacancy_features = pre_vacancy_features\n",
    "        new_vacancy_ids = pre_vacancy_ids\n",
    "        new_vac_titles = pre_vac_titles\n",
    "    else:\n",
    "        i = 0\n",
    "        for vac_salary in pre_vacancy_salaries:\n",
    "            if vac_salary == None:\n",
    "                new_vacancy_features.append(pre_vacancy_features[i])\n",
    "                new_vacancy_ids.append(pre_vacancy_ids[i])\n",
    "                new_vac_titles.append(pre_vac_titles[i])\n",
    "            else:\n",
    "                min_resume_salary = resume_salary - (resume_salary * 0.2)\n",
    "                max_resume_salary = resume_salary + (resume_salary * 0.8)\n",
    "                if vac_salary >= min_resume_salary and vac_salary <= max_resume_salary:\n",
    "                    new_vacancy_features.append(pre_vacancy_features[i])\n",
    "                    new_vacancy_ids.append(pre_vacancy_ids[i])\n",
    "                    new_vac_titles.append(pre_vac_titles[i])\n",
    "                \n",
    "            i = i+1    \n",
    "    \n",
    "    similarities = []\n",
    "    ids = []\n",
    "    titles = []\n",
    "    if len(new_vacancy_features) > 0:\n",
    "        c_result = cosine_similarity(resume_feature, new_vacancy_features)\n",
    "        res = heapq.nlargest(20, range(len(c_result[0])), c_result[0].take)\n",
    "        \n",
    "        for j in res:\n",
    "            similarities.append(c_result[0][j])\n",
    "            ids.append(new_vacancy_ids[j])\n",
    "            titles.append(new_vac_titles[j])\n",
    "    return similarities, ids, titles\n",
    "\n",
    "resume_features, resume_salaries, resume_ids, resume_areas = get_resumes()\n",
    "\n",
    "count = 1000\n",
    "features = get_vacancies(0, count)\n",
    "features, vacancy_ids, salaries, titles, vacancy_areas = get_vacancies(0, count)\n",
    "\n",
    "f_len = len(features)\n",
    "\n",
    "res_similarities = {}\n",
    "res_recommended_ids = {}\n",
    "res_recommended_titles = {}\n",
    "for idx, val in enumerate(resume_features):  \n",
    "    r_similarities, r_ids, r_titles = get_recommended(resume_features[idx], features, resume_salaries[idx], salaries, vacancy_ids, \n",
    "                                                      titles, resume_areas[idx], vacancy_areas)\n",
    "    res_similarities[resume_ids[idx]] = r_similarities\n",
    "    res_recommended_ids[resume_ids[idx]] = r_ids\n",
    "    res_recommended_titles[resume_ids[idx]] = r_titles\n",
    "\n",
    "i = 0\n",
    "while f_len > 0:\n",
    "    features, vacancy_ids, salaries, titles, vacancy_areas = get_vacancies(i*count, count)\n",
    "    f_len = len(features)\n",
    "    if f_len > 0:\n",
    "        for idx, val in enumerate(resume_features):\n",
    "            r_similarities, r_ids, r_titles = get_recommended(resume_features[idx], features, resume_salaries[idx], salaries, vacancy_ids, \n",
    "                                                              titles, resume_areas[idx], vacancy_areas)\n",
    "            res_similarities[resume_ids[idx]] = res_similarities[resume_ids[idx]] + r_similarities\n",
    "            res_recommended_ids[resume_ids[idx]] = res_recommended_ids[resume_ids[idx]] + r_ids\n",
    "            res_recommended_titles[resume_ids[idx]] = res_recommended_titles[resume_ids[idx]] + r_titles\n",
    "            \n",
    "    i = i+1\n",
    "    print 'processed {} vаcancies'.format(i*count)\n",
    "        \n",
    "    if i == 20:\n",
    "        break\n",
    "\n",
    "for resume_id in res_similarities.keys():\n",
    "    print resume_id\n",
    "    similarities = res_similarities[resume_id]\n",
    "    ids = res_recommended_ids[resume_id]\n",
    "    titles = res_recommended_titles[resume_id]\n",
    "    max_similarities = heapq.nlargest(20, range(len(numpy.asarray(similarities))), numpy.asarray(similarities).take)\n",
    "    cursor = db.cursor()\n",
    "    try:\n",
    "        cursor.execute(\"\"\"UPDATE recommendations SET is_active=0 WHERE resume_id='{}'\"\"\".format(resume_id))\n",
    "    except BaseException:\n",
    "        db.rollback()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "    for ind in max_similarities:\n",
    "        cursor = db.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"INSERT INTO recommendations (resume_id, vacancy_id, updated, is_active, similarity, vacancy_title) VALUES ('{}', {}, now(), 1, {}, '{}')\"\"\".format(resume_id, ids[ind], similarities[ind], titles[ind].encode('utf-8').strip()))\n",
    "        except BaseException:\n",
    "            db.rollback()\n",
    "        finally:\n",
    "            cursor.close()\n",
    "        print 'for {} similarity is {}'.format(ids[ind], similarities[ind])\n",
    "    db.commit()\n",
    "        \n",
    "db.commit()\n",
    "db.close()\n",
    "\n",
    "print 'total time {} sec\\n'.format(time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bar'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import redis\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "r.set('foo', 'bar')\n",
    "r.get('foo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-06 16:25:07.609814\n",
      "loaded 2000\n",
      "loaded 4000\n",
      "loaded 6000\n",
      "loaded 8000\n",
      "loaded 10000\n",
      "missed area id 3309 for vacancy 15891681\n",
      "loaded 12000\n",
      "loaded 14000\n",
      "loaded 16000\n",
      "loaded 18000\n",
      "loaded 20000\n",
      "loaded 22000\n",
      "loaded 24000\n",
      "loaded 26000\n",
      "loaded 28000\n",
      "loaded 30000\n",
      "loaded 32000\n",
      "loaded 34000\n",
      "loaded 36000\n",
      "loaded 38000\n",
      "loaded 40000\n",
      "loaded 42000\n",
      "loaded 44000\n",
      "loaded 46000\n",
      "loaded 48000\n",
      "loaded 50000\n",
      "loaded 52000\n",
      "loaded 54000\n",
      "loaded 56000\n",
      "loaded 58000\n",
      "loaded 60000\n",
      "loaded 62000\n",
      "loaded 64000\n",
      "loaded 66000\n",
      "loaded 68000\n",
      "loaded 70000\n",
      "loaded 72000\n",
      "loaded 74000\n",
      "loaded 76000\n",
      "loaded 78000\n",
      "loaded 80000\n",
      "loaded 82000\n",
      "loaded 84000\n",
      "loaded 86000\n",
      "loaded 88000\n",
      "loaded 90000\n",
      "loaded 92000\n",
      "loaded 94000\n",
      "loaded 96000\n",
      "loaded 98000\n",
      "loaded 100000\n",
      "loaded 102000\n",
      "loaded 104000\n",
      "loaded 106000\n",
      "loaded 108000\n",
      "loaded 110000\n",
      "loaded 112000\n",
      "loaded 114000\n",
      "loaded 116000\n",
      "loaded 118000\n",
      "loaded 120000\n",
      "loaded 122000\n",
      "loaded 124000\n",
      "loaded 126000\n",
      "loaded 128000\n",
      "loaded 130000\n",
      "loaded 132000\n",
      "loaded 134000\n",
      "loaded 136000\n",
      "loaded 138000\n",
      "loaded 140000\n",
      "loaded 142000\n",
      "loaded 144000\n",
      "loaded 146000\n",
      "loaded 148000\n",
      "loaded 150000\n",
      "loaded 152000\n",
      "finish  at 34.5748804132 min\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# save all vacancies into redis\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "from tinydb import TinyDB\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy import spatial\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import heapq\n",
    "import numpy\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "import time\n",
    "import datetime\n",
    "import redis\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "start_time = time.time()\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "    \n",
    "spec_ids = pickle.load( open( \"spec_ids.p\", \"rb\" ) )\n",
    "key_skills = pickle.load( open( \"key_skills.p\", \"rb\" ) )\n",
    "title_words = pickle.load( open( \"title_words.p\", \"rb\" ) )\n",
    "\n",
    "count_vectorizer = pickle.load( open( \"count_vectorizer.p\", \"rb\" ) )\n",
    "tfidf_transformer = pickle.load( open( \"tfidf_transformer.p\", \"rb\" ) )\n",
    "\n",
    "def get_vacancies(offset, rows):\n",
    "    features = []\n",
    "\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    #будет задвоение, когда во время выборки в несколько запросом добавляются новые данные\n",
    "    cursor.execute(\"\"\"SELECT item, id FROM vacancies WHERE updated >= (NOW() - INTERVAL 7 DAY) LIMIT {}, {}\"\"\".format(offset, rows))\n",
    "    vacancy_ids = []\n",
    "    salaries = []\n",
    "    cities = []\n",
    "    titles = []\n",
    "    areas = []\n",
    "    for item in cursor:\n",
    "        feature = []\n",
    "        vacancy = json.loads(item[0])\n",
    "        vacancy_ids.append(vacancy['id'])\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "                \n",
    "        titles.append(vacancy['name'])\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        salaries.append(salary)\n",
    "        try:\n",
    "            areas.append(areas_map[vacancy['area']['id']])\n",
    "        except KeyError:\n",
    "            print 'missed area id {} for vacancy {}'.format(vacancy['area']['id'], vacancy['id'])\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area_vacancy = r1.read()\n",
    "            missed_area_vacancy_json = json.loads(missed_area_vacancy)\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/areas/{}\"\n",
    "                         .format(missed_area_vacancy_json['area']['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area = r1.read()\n",
    "            missed_area_json = json.loads(missed_area)\n",
    "            areas_map[vacancy['area']['id']] = missed_area_json['parent_id']\n",
    "            areas.append(missed_area_json['parent_id'])\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "            \n",
    "        features.append(tfidf_feature_p_doc.toarray()[0])\n",
    "\n",
    "    cursor.close()\n",
    "    return features, vacancy_ids, salaries, titles, areas\n",
    "\n",
    "timeout = 6*24*60*60\n",
    "vac_cnt = 2000\n",
    "\n",
    "features, vacancy_ids, salaries, titles, areas = get_vacancies(0, vac_cnt)\n",
    "cnt = len(features)\n",
    "for idx, val in enumerate(features): \n",
    "    data = {}\n",
    "    data['features'] = json.dumps(features[idx].tolist()).encode(\"zlib\")\n",
    "    data['salary'] = salaries[idx]\n",
    "    data['area'] = areas[idx]\n",
    "    r.hmset(vacancy_ids[idx], data)\n",
    "    r.expire(vacancy_ids[idx], timeout)\n",
    "    \n",
    "i = 0\n",
    "while cnt > 0:\n",
    "    features, vacancy_ids, salaries, titles, areas = get_vacancies(i*vac_cnt, vac_cnt)\n",
    "    cnt = len(features)\n",
    "    for idx, val in enumerate(features): \n",
    "        data = {}\n",
    "        data['features'] = json.dumps(features[idx].tolist()).encode(\"zlib\")\n",
    "        data['salary'] = salaries[idx]\n",
    "        data['area'] = areas[idx]\n",
    "        r.hmset(vacancy_ids[idx], data)\n",
    "        r.expire(vacancy_ids[idx], timeout)\n",
    "    print 'loaded {}'.format(i*vac_cnt+vac_cnt)\n",
    "    i = i+1\n",
    "\n",
    "    \n",
    "print \"finish  at {} min\".format((time.time()-start_time)/60.0)\n",
    "\n",
    "#print data['features'].decode('zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-14 19:47:37.178057\n",
      "/vacancies?per_page=200&date_from=2016-06-14T19:42:38&date_to=2016-06-14T19:47:38&page=0\n",
      "81\n",
      "starting t1\n",
      "25.5510351658 sec\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#vacancy downloader 2\n",
    "import httplib\n",
    "import json\n",
    "import MySQLdb\n",
    "import ConfigParser\n",
    "import time\n",
    "import threading\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import heapq\n",
    "import numpy\n",
    "import re \n",
    "import Stemmer\n",
    "import datetime\n",
    "import redis\n",
    "\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "start_time = time.time()\n",
    "timeout = 5*24*60*60\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "conn.close()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "    \n",
    "with open( \"count_vectorizer.p\", \"rb\" ) as f:\n",
    "    count_vectorizer = pickle.load(f)\n",
    "    \n",
    "with open( \"tfidf_transformer.p\", \"rb\" ) as f:\n",
    "    tfidf_transformer = pickle.load(f)\n",
    "    \n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "    \n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "cursor.close()\n",
    "\n",
    "def get_vacancy_ids():\n",
    "    vacancy_ids = []\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    per_page = 200\n",
    "    page = 0\n",
    "    count = per_page\n",
    "    date_from = (datetime.datetime.now() - datetime.timedelta(minutes=5)).strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    date_to = datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    while count == per_page:\n",
    "        path = (\"/vacancies?per_page={}&date_from={}&date_to={}&page={}\"\n",
    "                .format(per_page, date_from, date_to, page))\n",
    "        print path\n",
    "\n",
    "        conn.request(\"GET\", path, headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        vacancies = r1.read()\n",
    "        conn.close()\n",
    "\n",
    "        count = len(json.loads(vacancies)['items'])\n",
    "        page = page+1\n",
    "        for item in json.loads(vacancies)['items']:\n",
    "            vacancy_ids.append(item['id'])\n",
    "    return vacancy_ids\n",
    "        \n",
    "\n",
    "def process_vacancies(vacancy_ids):\n",
    "    headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "    for vac_id in vacancy_ids:\n",
    "        conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        if r1 != 200:\n",
    "            conn.close()\n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"/vacancies/{}\".format(vac_id), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "        vacancy_txt = r1.read()\n",
    "        conn.close()\n",
    "        vacancy = json.loads(vacancy_txt)\n",
    "        \n",
    "        feature = []\n",
    "\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        doc = re.sub('<[^>]*>', '', vacancy['description'].lower())\n",
    "        doc = re.sub('&quot;', '', doc)\n",
    "        doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', doc.strip())\n",
    "        for word in words:\n",
    "            word = stemmer.stemWord(word.strip())\n",
    "            if len(word.strip()) > 1:\n",
    "                p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        title = re.sub(ur'[^a-zа-я]+', ' ', vacancy['name'].lower(), re.UNICODE)\n",
    "        words = re.split(r'\\s{1,}', title.strip())\n",
    "        for title_word in words:\n",
    "            title_word = stemmer.stemWord(title_word)\n",
    "            if len(title_word.strip()) > 1:\n",
    "                p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        vac_skills = vacancy['key_skills']\n",
    "        for skill in vac_skills:\n",
    "            words = re.split(r'\\s{1,}', skill['name'].lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "                    \n",
    "        #salary\n",
    "        salary = None\n",
    "        if vacancy['salary'] != None:\n",
    "            if vacancy['salary']['from'] == None and vacancy['salary']['to'] != None:\n",
    "                salary = vacancy['salary']['to']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] == None and vacancy['salary']['from'] != None:\n",
    "                salary = vacancy['salary']['from']/currency_rates[vacancy['salary']['currency']]\n",
    "            elif vacancy['salary']['to'] != None and vacancy['salary']['from'] != None:\n",
    "                salary = ((vacancy['salary']['from'] + vacancy['salary']['to'])/2)/currency_rates[vacancy['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "\n",
    "        try:\n",
    "            area_id = areas_map[vacancy['area']['id']]\n",
    "        except KeyError:\n",
    "            print 'missed area id {} for vacancy {}'.format(vacancy['area']['id'], vacancy['id'])\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            if r1 != 200:\n",
    "                conn.close()\n",
    "                conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "                conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(vacancy['id']), headers=headers)\n",
    "                r1 = conn.getresponse()\n",
    "            missed_area_vacancy = r1.read()\n",
    "            conn.close()\n",
    "            missed_area_vacancy_json = json.loads(missed_area_vacancy)\n",
    "            \n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/areas/{}\"\n",
    "                         .format(missed_area_vacancy_json['area']['id']), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "            missed_area = r1.read()\n",
    "            conn.close()\n",
    "            missed_area_json = json.loads(missed_area)\n",
    "            areas_map[vacancy['area']['id']] = missed_area_json['parent_id']\n",
    "            area_id = missed_area_json['parent_id']\n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        \n",
    "\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        tfidf_feature_p_doc = tfidf_transformer.transform(feature_p_doc)\n",
    "            \n",
    "        data = {}\n",
    "        data['features'] = json.dumps(tfidf_feature_p_doc.toarray()[0].tolist()).encode(\"zlib\")\n",
    "        data['salary'] = salary\n",
    "        data['area'] = area_id\n",
    "        r.hmset(vacancy['id'], data)\n",
    "        r.expire(vacancy['id'], timeout)\n",
    "\n",
    "\n",
    "ids = get_vacancy_ids()\n",
    "print len(ids)\n",
    "vac_id_chunks=[ids[x:x+100] for x in xrange(0, len(ids), 100)]\n",
    "t_num = 1;\n",
    "threads = []\n",
    "for vac_id_chunk in vac_id_chunks:\n",
    "    print 'starting t{}'.format(t_num)\n",
    "    t_num = t_num + 1\n",
    "    t = threading.Thread(target=process_vacancies, kwargs={'vacancy_ids': vac_id_chunk})\n",
    "    threads.append(t)\n",
    "    t.start()\n",
    "    \n",
    "for t in threads:\n",
    "    t.join()\n",
    "    \n",
    "\n",
    "db.close()\n",
    "print \"{} sec\".format(time.time() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at 2016-06-14 20:38:34.781637\n",
      "processed 168\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 15085407 similarity is 0.10639463313\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403666 similarity is 0.0867117762745\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17215363 similarity is 0.0778524129871\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17215463 similarity is 0.0636312702493\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403833 similarity is 0.0486753668977\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 13771336 similarity is 0.0473248742695\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 13771335 similarity is 0.0473248742695\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 13771333 similarity is 0.0473248742695\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403648 similarity is 0.0472854235832\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403649 similarity is 0.0472854235832\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403650 similarity is 0.0472854235832\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 16565774 similarity is 0.0437845019753\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 13782569 similarity is 0.0369401306925\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 13782570 similarity is 0.0369401306925\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 13782567 similarity is 0.0369401306925\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 16050262 similarity is 0.0365782626874\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403852 similarity is 0.033104389423\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403670 similarity is 0.0272363458173\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403761 similarity is 0.021365607586\n",
      "27cf52c1ff00b6dc6d0039ed1f736563726574. for 17403652 similarity is 0.02105481539\n",
      "total time 9.01302719116 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#recommender2\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import heapq\n",
    "import numpy\n",
    "import ConfigParser\n",
    "import MySQLdb\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import httplib\n",
    "import re \n",
    "import Stemmer\n",
    "import time\n",
    "import datetime\n",
    "import redis\n",
    "import threading\n",
    "from multiprocessing import Pool\n",
    "\n",
    "print 'Start at {}'.format(datetime.datetime.now())\n",
    "start_time = time.time()\n",
    "r = redis.StrictRedis(host='localhost', port=6379, db=0)\n",
    "config = ConfigParser.ConfigParser()\n",
    "config.readfp(open('my.cfg'))\n",
    "db = MySQLdb.connect(host=\"127.0.0.1\", \n",
    "                     port=config.getint('mysqld', 'port'), \n",
    "                     user=config.get('mysqld', 'user'), \n",
    "                     passwd=config.get('mysqld', 'password'), \n",
    "                     db=config.get('mysqld', 'database') )\n",
    "db.autocommit(True)\n",
    "db.set_character_set('utf8')\n",
    "cursor = db.cursor()\n",
    "cursor.execute('SET NAMES utf8;')\n",
    "cursor.execute('SET CHARACTER SET utf8;')\n",
    "cursor.execute('SET character_set_connection=utf8;')\n",
    "cursor.close()\n",
    "\n",
    "headers = {\"User-Agent\": \"hh-recommender\"}\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/dictionaries\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "dictionaries = r1.read()\n",
    "conn.close()\n",
    "dictionaries_json = json.loads(dictionaries)\n",
    "\n",
    "currencies = dictionaries_json['currency']\n",
    "currency_rates = {}\n",
    "for currency in currencies:\n",
    "    currency_rates[currency['code']] = currency['rate']\n",
    "    \n",
    "#areas\n",
    "conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "r1 = conn.getresponse()\n",
    "if r1.status != 200:\n",
    "    conn.close()\n",
    "    conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "    conn.request(\"GET\", \"https://api.hh.ru/areas\", headers=headers)\n",
    "    r1 = conn.getresponse()\n",
    "areas = r1.read()\n",
    "conn.close()\n",
    "areas_json = json.loads(areas)\n",
    "areas_map = {}\n",
    "def build_areas_map(areas, areas_map):\n",
    "    for area in areas:\n",
    "        if area['id'] == '1':#msk\n",
    "            parent_id = '2019'\n",
    "        elif area['id'] == '2':#spb\n",
    "            parent_id = '145'\n",
    "        elif area['id'] == '115':#kiev\n",
    "            parent_id = '2164'\n",
    "        elif area['id'] == '1002':#minsk\n",
    "            parent_id = '2237'\n",
    "        else:\n",
    "            parent_id = area['parent_id']\n",
    "        areas_map[area['id']] = parent_id\n",
    "        build_areas_map(area['areas'], areas_map)\n",
    "        \n",
    "build_areas_map(areas_json, areas_map)\n",
    "  \n",
    "with open( \"count_vectorizer.p\", \"rb\" ) as f:\n",
    "    count_vectorizer = pickle.load(f)\n",
    "    \n",
    "with open( \"tfidf_transformer.p\", \"rb\" ) as f:\n",
    "    tfidf_transformer = pickle.load(f)\n",
    "\n",
    "def get_resumes():\n",
    "    salaries = []\n",
    "    features = []\n",
    "    ids = []\n",
    "    areas = []\n",
    "    stemmer = Stemmer.Stemmer('russian')\n",
    "    cursor = db.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT r.item \n",
    "        FROM resumes r \n",
    "        WHERE r.is_active=1 AND NOT EXISTS (\n",
    "            SELECT * \n",
    "            FROM recommendations rec\n",
    "            WHERE rec.resume_id=r.item_id and rec.is_active=1\n",
    "        ) \"\"\")\n",
    "    for item in cursor:\n",
    "        resume_json = json.loads(item[0])\n",
    "        feature = []\n",
    "        #description\n",
    "        p_doc = ''\n",
    "        if resume_json['skills'] != None:\n",
    "            doc = re.sub('<[^>]*>', '', resume_json['skills'].lower())\n",
    "            doc = re.sub('&quot;', '', doc)\n",
    "            doc = re.sub(ur'[^a-zа-я]+', ' ', doc, re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', doc.strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word.strip())\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_doc = p_doc + \" \" + word\n",
    "\n",
    "        #title\n",
    "        p_title = ''\n",
    "        if resume_json['title'] != None:\n",
    "            title = re.sub(ur'[^a-zа-я]+', ' ', resume_json['title'].lower(), re.UNICODE)\n",
    "            words = re.split(r'\\s{1,}', title.strip())\n",
    "            for title_word in words:\n",
    "                title_word = stemmer.stemWord(title_word)\n",
    "                if len(title_word.strip()) > 1:\n",
    "                    p_title = p_title + \" \" + title_word.strip()\n",
    "\n",
    "        #keyskills\n",
    "        p_skills = ''\n",
    "        res_skills = resume_json['skill_set']\n",
    "        for skill in res_skills:\n",
    "            words = re.split(r'\\s{1,}', skill.lower().strip())\n",
    "            for word in words:\n",
    "                word = stemmer.stemWord(word)\n",
    "                if len(word.strip()) > 1:\n",
    "                    p_skills = p_skills + \" \" + word.strip()\n",
    "\n",
    "        #salary\n",
    "        salary = None\n",
    "        if resume_json['salary'] != None and resume_json['salary']['amount'] != None:\n",
    "            salary = resume_json['salary']['amount']/currency_rates[resume_json['salary']['currency']]\n",
    "        max_salary = 500000.0\n",
    "        if salary >= max_salary:\n",
    "            salary = max_salary\n",
    "        \n",
    "        \n",
    "        res_areas = []\n",
    "        if resume_json['area'] == None:\n",
    "            res_areas.append(areas_map[\"1\"])\n",
    "        else :\n",
    "            res_areas.append(areas_map[resume_json['area']['id']])\n",
    "        for area in resume_json['relocation']['area']:\n",
    "            res_areas.append(areas_map[area['id']])\n",
    "        areas.append(res_areas)\n",
    "        \n",
    "\n",
    "        p_doc = p_doc + \" \" + p_title + \" \" + p_skills\n",
    "        feature_p_doc = count_vectorizer.transform([p_doc])\n",
    "        feature = tfidf_transformer.transform(feature_p_doc)\n",
    "        features.append(feature.toarray())\n",
    "        salaries.append(salary)\n",
    "        ids.append(resume_json['id'])\n",
    "    cursor.close()\n",
    "    return features, salaries, ids, areas\n",
    "\n",
    "resume_features, resume_salaries, resume_ids, resume_areas = get_resumes()\n",
    "lock = threading.Lock()\n",
    "\n",
    "def process_vacancy_ids(vacancies):\n",
    "    pre_vacancy_similarities = {}\n",
    "    pre_vacancy_ids = {}\n",
    "\n",
    "    for idx, val in enumerate(resume_features):\n",
    "        new_vacancy_features = []\n",
    "        new_vacancy_ids = []\n",
    "        for vac_id, vac_data in vacancies.iteritems():\n",
    "            if resume_areas[idx][0] == vac_data['area'] and (resume_salaries[idx] == None or vac_data['salary'] == 'None'):\n",
    "                new_vacancy_features.append(json.loads(vac_data['features'].decode('zlib')))\n",
    "                new_vacancy_ids.append(vac_id)\n",
    "            elif resume_areas[idx][0] == vac_data['area']:\n",
    "                min_resume_salary = resume_salaries[idx] - (resume_salaries[idx] * 0.2)\n",
    "                max_resume_salary = resume_salaries[idx] + (resume_salaries[idx] * 0.8)\n",
    "                vac_salary = float(vac_data['salary'])\n",
    "                if vac_salary >= min_resume_salary and vac_salary <= max_resume_salary:\n",
    "                    new_vacancy_features.append(json.loads(vac_data['features'].decode('zlib')))\n",
    "                    new_vacancy_ids.append(vac_id)\n",
    "                    \n",
    "        similarities = []\n",
    "        ids = []\n",
    "        if len(new_vacancy_features) > 0:\n",
    "            c_result = cosine_similarity(resume_features[idx], new_vacancy_features)\n",
    "            res = heapq.nlargest(20, range(len(c_result[0])), c_result[0].take)\n",
    "\n",
    "            for j in res:\n",
    "                similarities.append(c_result[0][j])\n",
    "                ids.append(new_vacancy_ids[j])\n",
    "        \n",
    "        lock.acquire()\n",
    "        try:\n",
    "            if resume_ids[idx] not in pre_vacancy_similarities:\n",
    "                pre_vacancy_similarities[resume_ids[idx]] = similarities\n",
    "                pre_vacancy_ids[resume_ids[idx]] = ids\n",
    "            else:\n",
    "                pre_vacancy_similarities[resume_ids[idx]] = pre_vacancy_similarities[resume_ids[idx]] + similarities\n",
    "                pre_vacancy_ids[resume_ids[idx]] = pre_vacancy_ids[resume_ids[idx]] + ids\n",
    "        finally:\n",
    "            lock.release()\n",
    "            \n",
    "    return len(vacancies), pre_vacancy_similarities, pre_vacancy_ids\n",
    "\n",
    "tp_res = [] \n",
    "tpool = Pool(3) \n",
    "def iterate_ids(start):\n",
    "    cnt = 500\n",
    "    rcursor = r.scan(cursor=start, count=cnt)\n",
    "    vacancies = {}\n",
    "    for vac_id in rcursor[1]:\n",
    "        vacancies[vac_id] = r.hgetall(vac_id)\n",
    "    tres = tpool.apply_async(process_vacancy_ids, (vacancies,))\n",
    "    tp_res.append(tres)\n",
    "    while (rcursor[0] != 0):\n",
    "        rcursor = r.scan(cursor=rcursor[0], count=cnt)\n",
    "        vacancies = {}\n",
    "        for vac_id in rcursor[1]:\n",
    "            vacancies[vac_id] = r.hgetall(vac_id)\n",
    "        tres = tpool.apply_async(process_vacancy_ids, (vacancies,))\n",
    "        tp_res.append(tres)\n",
    "\n",
    "iterate_ids(0)\n",
    "\n",
    "c = 0\n",
    "pre_vacancy_similarities = {}\n",
    "pre_vacancy_ids = {}\n",
    "for tr in tp_res:\n",
    "    cnt, p_vacancy_similarities, p_vacancy_ids = tr.get()\n",
    "    for resume_id in p_vacancy_similarities.keys():\n",
    "        if resume_id not in pre_vacancy_similarities:\n",
    "            pre_vacancy_similarities[resume_id] = p_vacancy_similarities[resume_id]\n",
    "            pre_vacancy_ids[resume_id] = p_vacancy_ids[resume_id]\n",
    "        else:\n",
    "            pre_vacancy_similarities[resume_id] = pre_vacancy_similarities[resume_id]+p_vacancy_similarities[resume_id]\n",
    "            pre_vacancy_ids[resume_id] = pre_vacancy_ids[resume_id]+p_vacancy_ids[resume_id]\n",
    "    \n",
    "    c = c+cnt\n",
    "    print 'processed {}'.format(c)\n",
    "\n",
    "def finalize_recommendations(resume_id):\n",
    "    result = []\n",
    "    similarities = pre_vacancy_similarities[resume_id]\n",
    "    ids = pre_vacancy_ids[resume_id]\n",
    "    max_similarities = heapq.nlargest(20, range(len(numpy.asarray(similarities))), numpy.asarray(similarities).take)\n",
    "    lock.acquire()\n",
    "    try:\n",
    "        cursor = db.cursor()\n",
    "        try:\n",
    "            cursor.execute(\"\"\"UPDATE recommendations SET is_active=0 WHERE resume_id='{}'\"\"\".format(resume_id))\n",
    "        except BaseException as ex:\n",
    "            print ex\n",
    "        finally:\n",
    "            cursor.close()\n",
    "    finally:\n",
    "        lock.release()\n",
    "        \n",
    "    for ind in max_similarities:\n",
    "        conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "        conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(ids[ind]), headers=headers)\n",
    "        r1 = conn.getresponse()\n",
    "        if r1.status != 200:\n",
    "            conn.close()\n",
    "            conn = httplib.HTTPSConnection(\"api.hh.ru\")\n",
    "            conn.request(\"GET\", \"https://api.hh.ru/vacancies/{}\".format(ids[ind]), headers=headers)\n",
    "            r1 = conn.getresponse()\n",
    "        t_vacancy = r1.read()\n",
    "        conn.close()\n",
    "        t_vacancy_json = json.loads(t_vacancy)\n",
    "        try:\n",
    "            title = t_vacancy_json['name'].encode('utf-8').strip()\n",
    "        except KeyError as ex:\n",
    "            print ex\n",
    "            title = 'Title temporary not found'\n",
    "\n",
    "        lock.acquire()\n",
    "        try:\n",
    "            cursor = db.cursor()\n",
    "            try:\n",
    "                cursor.execute(\"\"\"\n",
    "                    INSERT INTO recommendations (resume_id, vacancy_id, updated, is_active, similarity, vacancy_title) \n",
    "                    VALUES ('{}', {}, now(), 1, {}, '{}')\n",
    "                \"\"\".format(resume_id, ids[ind], similarities[ind], title))\n",
    "            except BaseException as err:\n",
    "                print err\n",
    "            finally:\n",
    "                cursor.close()\n",
    "        finally:\n",
    "            lock.release()\n",
    "        result.append('{}. for {} similarity is {}'.format(resume_id, ids[ind], similarities[ind]))\n",
    "\n",
    "    return result\n",
    "\n",
    "p_res = [] \n",
    "pool = Pool(7) \n",
    "for resume_id in pre_vacancy_similarities.keys():\n",
    "    res = pool.apply_async(finalize_recommendations, (resume_id,))\n",
    "    p_res.append(res)\n",
    "    \n",
    "for t in p_res:\n",
    "    res = t.get()\n",
    "    for s in res:\n",
    "        print s\n",
    "        \n",
    "db.close()\n",
    "\n",
    "print 'total time {} sec\\n'.format(time.time()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "хабр\n",
      "хабр\n"
     ]
    }
   ],
   "source": [
    "import Stemmer\n",
    "stemmer = Stemmer.Stemmer('russian')\n",
    "print stemmer.stemWord('хабром')\n",
    "print stemmer.stemWord('хабру')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
